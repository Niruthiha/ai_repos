[
  {
    "issue_number": 124,
    "title": "mac启动报错",
    "author": "sontianye",
    "state": "open",
    "created_at": "2025-03-31T09:40:37Z",
    "updated_at": "2025-06-12T04:04:07Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\npython main.py\nzsh: bus error  python main.py\n\n\n```\n\n## Description\n\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "能展示下所有问题吗？这个信息太少了"
      },
      {
        "user": "sontianye",
        "body": "import keyboard\n还是这个的问题"
      },
      {
        "user": "yuruotong1",
        "body": "这个问题已经有人在修复了，应该是mac下需要管理员权限"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 128,
    "title": "运行python main py打不开软件",
    "author": "luffuy69",
    "state": "open",
    "created_at": "2025-04-08T14:35:53Z",
    "updated_at": "2025-06-06T03:01:14Z",
    "labels": [
      "bug"
    ],
    "body": "(base) PS C:\\Users\\路飞> python main.py\nD:\\miniconda\\python.exe: can't open file 'C:\\\\Users\\\\路飞\\\\main.py': [Errno 2] No such file or directory",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "这个应该不是代码问题吧🤔"
      },
      {
        "user": "qingyuan0o0",
        "body": "![Image](https://github.com/user-attachments/assets/a89536ea-f0b4-4800-ab42-22f4c5db61b2)"
      },
      {
        "user": "Livermorest",
        "body": "\n\n> (base) PS C:\\Users\\路飞> python main.py D:\\miniconda\\python.exe: can't open file 'C:\\Users\\路飞\\main.py': [Errno 2] No such file or directory\n\n看看你的C:\\Users\\路飞\\  目录下有没有 main.py"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 133,
    "title": "支持DeepSeek模型吗？为啥必须是Gpt4o",
    "author": "fxcfxc",
    "state": "closed",
    "created_at": "2025-04-17T07:26:44Z",
    "updated_at": "2025-06-06T03:00:26Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n\n```\n\n## Description\n\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "因为自动化控制电脑，强依赖于多模态能力，DeepSeek不是多模态模型，所以无法完成这个工作。另外，GPT4o 提供了多模态能力、结构化输出的完整能力，这个能力在项目前期能够帮我省下大量的工作量，我还没有在国内找到同时支持这两者的模型。"
      },
      {
        "user": "yuruotong1",
        "body": "现在最重要的问题是，目前市面上的多模态模型对于 computer-use 的支持都不好，需要自已训练一个专用场景的“小模型”，我已经有了大概的思路，但是对工程、算法能力上有较强的要求，我需要小的团队或者资金支持才能搞下去，现在一方面在找资源，另一方面在海外找业务 。"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 134,
    "title": "无法打开localhost:7888",
    "author": "printfInc",
    "state": "open",
    "created_at": "2025-04-26T10:18:03Z",
    "updated_at": "2025-06-06T03:00:15Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n\n```\n\n![Image](https://github.com/user-attachments/assets/d2fea85f-d2e4-4b33-89de-42184aaa6954)\n\n## Description\n\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "新版本已经不用 localhost:7888 了，运行后就是可操作的界面"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 136,
    "title": "是否考虑加入一下Qwen2.5-VL",
    "author": "whisper-yev",
    "state": "open",
    "created_at": "2025-05-08T08:08:05Z",
    "updated_at": "2025-06-06T02:59:39Z",
    "labels": [
      "bug"
    ],
    "body": "调用国外的api还是会担心数据安全的问题，希望能接入国内的模型吧\n",
    "comments": [
      {
        "user": "greenflute",
        "body": "being able to call local model would be the final solution."
      },
      {
        "user": "zz6zz666",
        "body": "可以啊，我试过了，请求是没任何问题，毕竟千问是兼容openAI的API的\n\n![Image](https://github.com/user-attachments/assets/2638cbf5-d8c6-4b9b-9113-d6c12bc625cf)"
      },
      {
        "user": "yuruotong1",
        "body": "最近我在考虑兼容一下"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 137,
    "title": "TypeError: the JSON object must be str, bytes or bytearray, not list",
    "author": "zz6zz666",
    "state": "open",
    "created_at": "2025-05-30T04:37:13Z",
    "updated_at": "2025-06-06T02:59:07Z",
    "labels": [
      "bug"
    ],
    "body": "![Image](https://github.com/user-attachments/assets/f9c4070a-bb83-40df-9e4f-7e12db4426fd)\n为什么在这一步之后出错了\n\n![Image](https://github.com/user-attachments/assets/2cd64137-71b4-46ad-bbfe-ed394674821e)\n\n![Image](https://github.com/user-attachments/assets/097c6646-6d66-45d0-8131-399c8e2f6934)\n\n定位到是这一行",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "在模型执行的中间，用户不能主动提问"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 138,
    "title": "输入格式错误",
    "author": "zz6zz666",
    "state": "open",
    "created_at": "2025-05-30T04:46:36Z",
    "updated_at": "2025-06-06T02:58:27Z",
    "labels": [
      "bug"
    ],
    "body": "![Image](https://github.com/user-attachments/assets/f856ce3d-6289-4277-a2b7-21436f2c7d86)",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "感觉像是模型地址不太对"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 135,
    "title": "是否考虑增加一下gemma3-tools:27b",
    "author": "doutianye",
    "state": "open",
    "created_at": "2025-04-28T05:44:44Z",
    "updated_at": "2025-04-28T05:44:44Z",
    "labels": [
      "bug"
    ],
    "body": "ollama部署了gemma3-tools:27b，它是多模态也支持tools，是否考虑这个大模型选项。",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 131,
    "title": "可以使用本地大模型吗？",
    "author": "Dou-Z",
    "state": "closed",
    "created_at": "2025-04-14T07:34:33Z",
    "updated_at": "2025-04-17T10:03:20Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n\n```\n\n## Description\n\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "暂时还不能用，只能用 openai-gpt4o"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 132,
    "title": "mac m4启动失败",
    "author": "AmarisWhite",
    "state": "closed",
    "created_at": "2025-04-14T08:45:24Z",
    "updated_at": "2025-04-17T02:39:03Z",
    "labels": [
      "bug"
    ],
    "body": "python main.py\nzsh: bus error  python main.py\n\n",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 126,
    "title": "在高分辨率下，屏幕元素识别不准确",
    "author": "freedomxie",
    "state": "open",
    "created_at": "2025-04-04T00:14:59Z",
    "updated_at": "2025-04-15T13:28:11Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n \"QHD\": Resolution(width=2560, height=1440),   # 16:9     \n \"3K\": Resolution(width=2880, height=1620),   # 16:9     \n \"4K\": Resolution(width=3840, height=2160),    # 16:9\n```\n\n## Description\n能否适以上分辨率\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "我感觉可能需要对图片进行缩放！"
      },
      {
        "user": "yuruotong1",
        "body": "我手头确实没有这个分辨率的电脑，这个问题可能稍晚才能解决。"
      },
      {
        "user": "greenflute",
        "body": "1. image scaling, in higher resolution, mac just need the origin image not the 2x, so i don’t think it will cause scaling issue \n2. tested under 2014x1280, prompt is „open terminal then cd to root and ls -lha“, with previous executed ls in terminal, automate can correctly recognise the results thus considering the task is already finished. discard the logic issue here, automate can actually correctly recognise icons and text in terminal window. so i don’t think higher resolution will cause problem. except when some old app was forced under higher resolution to maintain proper size using „zoom“, which will definitely cause „blurred“ images, and that may interfere automate’s recognising."
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 129,
    "title": "希望得到多显示器的支持",
    "author": "xaviershl",
    "state": "open",
    "created_at": "2025-04-11T08:45:43Z",
    "updated_at": "2025-04-15T13:27:21Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n你好!\n我通过最新版main.exe运行后, 选择场景目前只能选择主屏幕, 无法选择副屏幕的内容, 可能因为主屏幕分辨率是2560*1440的原因, 自动操作也不准确\n感谢\n```\n\n## Description\n\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "奈何我只有一台小笔记本，还没有双屏(~_~💧)"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 123,
    "title": "Must be Administrator?",
    "author": "greenflute",
    "state": "closed",
    "created_at": "2025-03-28T00:44:51Z",
    "updated_at": "2025-04-07T01:31:56Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: 0f9b9954b71d51123189de2c9c03705981868882\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n% python main.py\nCreating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/Users/~/Library/Application Support/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nDownloading Model from https://www.modelscope.cn to directory: weights/AI-ModelScope/OmniParser-v2.0\n2025-03-28 01:40:40,613 - modelscope - INFO - Target directory already exists, skipping creation.\nException in thread Thread-1 (listen):\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\nRegistered stop hotkey: alt+f3\n\n\n🚀 PyQt6 application launched\n    self.run()\n  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py\", line 1012, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/~/Sources/github.com_greenflute/yuruotong-autoMate/.venv/lib/python3.12/site-packages/keyboard/__init__.py\", line 294, in listen\n    _os_keyboard.listen(self.direct_callback)\n  File \"/Users/~/Sources/github.com_greenflute/yuruotong-autoMate/.venv/lib/python3.12/site-packages/keyboard/_darwinkeyboard.py\", line 430, in listen\n    raise OSError(\"Error 13 - Must be run as administrator\")\nOSError: Error 13 - Must be run as administrator\n```\n\n## Description\n\n\n### Current Behavior\nRunning „python main.py“ under mac, as typical local main user (which does have admin right)\n\n### Expected Behavior\ntypical local main users are in group 80 (admin), this should not be an error\n",
    "comments": [
      {
        "user": "greenflute",
        "body": "so are we listening to the keyboard globally or just in our gui window? if not globally, we can switch to pynput or PyQt6’s keyPressEvent"
      },
      {
        "user": "yuruotong1",
        "body": "> so are we listening to the keyboard globally or just in our gui window? if not globally, we can switch to pynput or PyQt6’s\n\n I don't have a Mac computer at hand. Do I need administrator privileges to use pynput?\n\n"
      },
      {
        "user": "greenflute",
        "body": "> > so are we listening to the keyboard globally or just in our gui window? if not globally, we can switch to pynput or PyQt6’s\n> \n> I don't have a Mac computer at hand. Do I need administrator privileges to use pynput?\n\nwell the keyboard needs it, but pynput seems not, i’ll give it a try."
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 119,
    "title": "[BUG]",
    "author": "Xavier-777",
    "state": "closed",
    "created_at": "2025-03-24T08:01:13Z",
    "updated_at": "2025-04-07T01:31:41Z",
    "labels": [
      "bug"
    ],
    "body": "使用mac启动项目，点击 `Select Screen Region` 按钮后，系统会报错，报错信息如下，然后系统自动退出。请问是什么原因呢，如解决。\n\n\n*** Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: 'NSWindow should only be instantiated on the main thread!'\n*** First throw call stack:\n(\n\t0   CoreFoundation                      0x00000001903ec300 __exceptionPreprocess + 176\n\t1   libobjc.A.dylib                     0x000000018fed2cd8 objc_exception_throw + 88\n\t2   CoreFoundation                      0x0000000190410994 _CFBundleGetValueForInfoKey + 0\n\t3   AppKit                              0x0000000193eb556c -[NSWindow _initContent:styleMask:backing:defer:contentView:] + 260\n\t4   AppKit                              0x0000000193eb545c -[NSWindow initWithContentRect:styleMask:backing:defer:] + 48\n\t5   libtk8.6.dylib                      0x000000030cf71ba4 TkMacOSXMakeRealWindowExist + 564\n\t6   libtk8.6.dylib                      0x000000030cf717f0 TkWmMapWindow + 76\n\t7   libtk8.6.dylib                      0x000000030cece588 MapFrame + 76\n\t8   libtcl8.6.dylib                     0x000000030d29b1a0 TclServiceIdle + 88\n\t9   libtcl8.6.dylib                     0x000000030d27b09c Tcl_DoOneEvent + 120\n\t10  libtk8.6.dylib                      0x000000030cf629c8 TkpInit + 800\n\t11  libtk8.6.dylib                      0x000000030cec6f20 Initialize + 2500\n\t12  _tkinter.cpython-312-darwin.so      0x000000016fdfe34c Tcl_AppInit + 92\n\t13  _tkinter.cpython-312-darwin.so      0x000000016fdfdfe0 Tkapp_New + 548\n\t14  _tkinter.cpython-312-darwin.so      0x000000016fdfddb8 _tkinter_create_impl + 268\n\t15  _tkinter.cpython-312-darwin.so      0x000000016fdfda38 _tkinter_create + 276\n\t16  python3.12                          0x0000000100acf3c0 cfunction_vectorcall_FASTCALL + 96\n\t17  python3.12                          0x0000000100bc1380 _PyEval_EvalFrameDefault + 189840\n\t18  python3.12                          0x0000000100b01654 slot_tp_init + 328\n\t19  python3.12                          0x0000000100af55e4 type_call + 148\n\t20  python3.12                          0x0000000100bc171c _PyEval_EvalFrameDefault + 190764\n\t21  python3.12                          0x0000000100b01654 slot_tp_init + 328\n\t22  python3.12                          0x0000000100af55e4 type_call + 148\n\t23  python3.12                          0x0000000100bc171c _PyEval_EvalFrameDefault + 190764\n\t24  python3.12                          0x0000000100be96a4 _PyObject_VectorcallTstate.4867 + 88\n\t25  python3.12                          0x0000000100be955c context_run + 104\n\t26  python3.12                          0x0000000100acf30c cfunction_vectorcall_FASTCALL_KEYWORDS + 92\n\t27  python3.12                          0x0000000100bc561c _PyEval_EvalFrameDefault + 206892\n\t28  python3.12                          0x0000000100a6c078 method_vectorcall + 368\n\t29  python3.12                          0x0000000100ccc1a0 thread_run + 80\n\t30  python3.12                          0x0000000100c4bc50 pythread_wrapper + 48\n\t31  libsystem_pthread.dylib             0x00000001902932e4 _pthread_start + 136\n\t32  libsystem_pthread.dylib             0x000000019028e0fc thread_start + 8\n)\nlibc++abi: terminating due to uncaught exception of type NSException\nzsh: abort      python main.py\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "这块代码正在修改"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 125,
    "title": "OSWorld Benchmark?",
    "author": "BradKML",
    "state": "closed",
    "created_at": "2025-04-03T03:44:10Z",
    "updated_at": "2025-04-07T01:31:23Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n\n```\n\n## Description\nWould like to see this to be compared against Agent S2 and the others, since they are the FOSS SOTA https://github.com/simular-ai/Agent-S https://os-world.github.io/\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "Your suggestion is very good!"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 127,
    "title": "on Mac M1 chip",
    "author": "Awesomepieman69",
    "state": "closed",
    "created_at": "2025-04-06T13:12:10Z",
    "updated_at": "2025-04-07T01:31:13Z",
    "labels": [
      "bug"
    ],
    "body": "\n## Error Message\nFatal Python error: Bus error\n\nCurrent thread 0x00000001f8ecb240 (most recent call first):\n  File \"/Users/pieman/enter/envs/automate/lib/python3.12/site-packages/keyboard/_darwinkeyboard.py\", line 134 in __init__\n  File \"/Users/pieman/enter/envs/automate/lib/python3.12/site-packages/keyboard/_darwinkeyboard.py\", line 204 in __init__\n  File \"/Users/pieman/enter/envs/automate/lib/python3.12/site-packages/keyboard/_darwinkeyboard.py\", line 404 in <module>\n  …  \n  File \"/Users/pieman/autoMate/main.py\", line 1 in <module>\nBus error: 10\n## Description\n\n\n### Current Behavior\nWhen running python3 main.py in the automate conda environment, the application immediately crashes with a Bus error: 10. The stack trace shows the fault occurs inside the macOS-specific portion of the keyboard package (_darwinkeyboard.py).\n### Expected Behavior\n\n",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 121,
    "title": "这个软件是不是必须在线？",
    "author": "xiaoxiongwn",
    "state": "closed",
    "created_at": "2025-03-27T14:32:47Z",
    "updated_at": "2025-03-31T23:07:04Z",
    "labels": [
      "bug"
    ],
    "body": "这个软件运行时，是不是必须有互联网？在局域网中是不可以运行的吧。",
    "comments": [
      {
        "user": "greenflute",
        "body": "如果有本地部署大模型，应该就可以不用"
      },
      {
        "user": "yuruotong1",
        "body": "目前是需要联网的，现在这个版本只支持 openai 模型"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 122,
    "title": "Packaging Version?",
    "author": "greenflute",
    "state": "closed",
    "created_at": "2025-03-28T00:31:51Z",
    "updated_at": "2025-03-28T12:11:40Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: 0f9b9954b71d51123189de2c9c03705981868882\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-core 0.1.23 requires packaging<24.0,>=23.2, but you have packaging 24.1 which is incompatible.\n```\n\n## Description\n\n\n### Current Behavior\nreport this error while running \"python install.py\"\n\n\n### Expected Behavior\nshould report no error\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "我们没有用到 langChain，你可以用 conda 创建一个虚拟环境"
      },
      {
        "user": "greenflute",
        "body": "great thanks, maybe it comes from the previous revision, i’ll clean the venv and test again"
      },
      {
        "user": "greenflute",
        "body": "after cleaning up .venv, now there is no error messages any more, issue closed :)"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 82,
    "title": "库包问题有谁遇到吗",
    "author": "OLpaye",
    "state": "closed",
    "created_at": "2025-03-12T09:57:35Z",
    "updated_at": "2025-03-27T07:01:05Z",
    "labels": [],
    "body": "![Image](https://github.com/user-attachments/assets/eeb0d9cd-246f-48f4-b58b-80634628b971)",
    "comments": [
      {
        "user": "OLpaye",
        "body": "(automate) D:\\Zhaowh\\Work Space\\3.Technical\\automateai>python main.py\nTraceback (most recent call last):\n  File \"D:\\Zhaowh\\Work Space\\3.Technical\\automateai\\main.py\", line 5, in <module>\n    from gradio_ui import app\n  File \"D:\\Zhaowh\\Work Space\\3.Technical\\automateai\\gradio_ui\\app.py\", line 17, in <module>\n    from gradio_ui.loop import (\n  File \"D:\\Zhaowh\\Work Space\\3.Technical\\automateai\\gradio_ui\\loop.py\", line 16, in <module>\n    from gradio_ui.tools import ToolResult\n  File \"D:\\Zhaowh\\Work Space\\3.Technical\\automateai\\gradio_ui\\tools\\__init__.py\", line 3, in <module>\n    from .computer import ComputerTool\n  File \"D:\\Zhaowh\\Work Space\\3.Technical\\automateai\\gradio_ui\\tools\\computer.py\", line 6, in <module>\n    from util import tool\n  File \"D:\\Zhaowh\\Work Space\\3.Technical\\automateai\\util\\tool.py\", line 6, in <module>\n    import pyautogui\n  File \"C:\\Users\\Zhaowh\\.conda\\envs\\automate\\Lib\\site-packages\\pyautogui\\__init__.py\", line 542, in <module>\n    from . import _pyautogui_win as platformModule\nImportError: cannot import name '_pyautogui_win' from partially initialized module 'pyautogui' (most likely due to a circular import) (C:\\Users\\Zhaowh\\.conda\\envs\\automate\\Lib\\site-packages\\pyautogui\\__init__.py)\n\n(automate) D:\\Zhaowh\\Work Space\\3.Technical\\automateai>python\nPython 3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:20:38) [MSC v.1916 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pyautogui\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Users\\Zhaowh\\.conda\\envs\\automate\\Lib\\site-packages\\pyautogui\\__init__.py\", line 542, in <module>\n    from . import _pyautogui_win as platformModule\nImportError: cannot import name '_pyautogui_win' from partially initialized module 'pyautogui' (most likely due to a circular import) (C:\\Users\\Zhaowh\\.conda\\envs\\automate\\Lib\\site-packages\\pyautogui\\__init__.py)\n>>>"
      },
      {
        "user": "yuruotong1",
        "body": "稍等我精简一下 requirements"
      },
      {
        "user": "yuruotong1",
        "body": "@OLpaye 已经把重构后的代码推上去了！可以更新下看看"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 120,
    "title": "[BUG]",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2025-03-24T15:33:11Z",
    "updated_at": "2025-03-24T15:33:21Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n\n```\n\n## Description\n\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 114,
    "title": "[BUG] 我看OmniParser定位似乎是没问题的，不知道是鼠标移动有问题导致点错了地方？",
    "author": "Rosejacka",
    "state": "closed",
    "created_at": "2025-03-19T03:11:28Z",
    "updated_at": "2025-03-24T03:14:14Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n\n```\n\n## Description\n\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "Rosejacka",
        "body": "并且感觉耗时很长这是正常的吗"
      },
      {
        "user": "yuruotong1",
        "body": "耗时长，有两部分原因：1）omniparser 打标时间长，这个可能显卡不适配 2）大模型返回时间长，这个没特别好的解决思路，除非我们自己训练一个大模型"
      },
      {
        "user": "yuruotong1",
        "body": "你看下Gradio界面AI的输出内容，它会告诉你点哪个BoxId，这个BoxId在图片中的位置是否正确？"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 118,
    "title": "[BUG]",
    "author": "pigq",
    "state": "closed",
    "created_at": "2025-03-21T03:47:20Z",
    "updated_at": "2025-03-23T22:56:33Z",
    "labels": [
      "bug"
    ],
    "body": "![Image](https://github.com/user-attachments/assets/f6b20755-0216-4fe9-85a0-a528d5ccd0b8)\n这使用main.exe文件，无法使用，是否是vpn的问题",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "是的，请关闭代理工具！"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 117,
    "title": "这个最后一步的报错快把我搞崩溃了",
    "author": "hainiumoumou",
    "state": "closed",
    "created_at": "2025-03-20T17:39:38Z",
    "updated_at": "2025-03-22T04:20:28Z",
    "labels": [
      "bug"
    ],
    "body": "![Image](https://github.com/user-attachments/assets/8536cbd0-1d5b-4481-9c3f-2a325f6133eb)",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "看下命令行？"
      },
      {
        "user": "hainiumoumou",
        "body": "> 看下命令行？\n*** Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: 'NSWindow should only be instantiated on the main thread!'\n*** First throw call stack:\n(\n 0   CoreFoundation                      0x00007ff80c346b8e __exceptionPreprocess + 242\n 1   libobjc.A.dylib                     0x00007ff80be2cf12 objc_exception_throw + 62\n 2   CoreFoundation                      0x00007ff80c3699f4 -[NSException raise] + 9\n 3   AppKit                              0x00007ff80fbb6126 -[NSWindow _initContent:styleMask:backing:defer:contentView:] + 1653\n 4   AppKit                              0x00007ff80fbb5aa9 -[NSWindow initWithContentRect:styleMask:backing:defer:] + 42\n 5   libtk8.6.dylib                      0x000000015305b040 TkMacOSXMakeRealWindowExist + 736\n 6   libtk8.6.dylib                      0x000000015305ab91 TkWmMapWindow + 81\n 7   libtk8.6.dylib                      0x0000000152fa8491 MapFrame + 65\n 8   libtcl8.6.dylib                     0x0000000153410446 TclServiceIdle + 134\n 9   libtcl8.6.dylib                     0x00000001533edfb1 Tcl_DoOneEvent + 385\n 10  libtk8.6.dylib                      0x000000015304aafe TkpInit + 766\n 11  libtk8.6.dylib                      0x0000000152fa0362 Initialize + 2610\n 12  _tkinter.cpython-312-darwin.so      0x000000015010271b Tcl_AppInit + 91\n 13  _tkinter.cpython-312-darwin.so      0x00000001501023f9 Tkapp_New + 585\n 14  _tkinter.cpython-312-darwin.so      0x000000015010219e _tkinter_create_impl + 222\n 15  _tkinter.cpython-312-darwin.so      0x0000000150101dfa _tkinter_create + 186\n 16  python3.12                          0x0000000100a6847a cfunction_vectorcall_FASTCALL + 106\n 17  python3.12                          0x0000000100b71aff _PyEval_EvalFrameDefault + 229071\n 18  python3.12                          0x0000000100a9d5c9 slot_tp_init + 313\n 19  python3.12                          0x0000000100a90c37 type_call + 135\n 20  python3.12                          0x0000000100b722af _PyEval_EvalFrameDefault + 231039\n 21  python3.12                          0x0000000100a9d5c9 slot_tp_init + 313\n 22  python3.12                          0x0000000100a90c37 type_call + 135\n 23  python3.12                          0x0000000100b722af _PyEval_EvalFrameDefault + 231039\n 24  python3.12                          0x0000000100b9f7c9 _PyObject_VectorcallTstate.4867 + 73\n 25  python3.12                          0x0000000100b9f6b6 context_run + 86\n 26  python3.12                          0x0000000100a683e2 cfunction_vectorcall_FASTCALL_KEYWORDS + 98\n 27  python3.12                          0x0000000100b76dab _PyEval_EvalFrameDefault + 250235\n 28  python3.12                          0x0000000100a013a0 method_vectorcall + 464\n 29  python3.12                          0x0000000100c95b01 thread_run + 81\n 30  python3.12                          0x0000000100c0fe34 pythread_wrapper + 36\n 31  libsystem_pthread.dylib             0x00007ff80c1f0253 _pthread_start + 99\n 32  libsystem_pthread.dylib             0x00007ff80c1ebbef thread_start + 15\n)\nlibc++abi: terminating due to uncaught exception of type NSException\nzsh: abort      python main.py\n(automate) a123123@123deMacBook-Air autoMate %"
      },
      {
        "user": "yuruotong1",
        "body": "好像存在 mac 适配问题 @Gushroom "
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 116,
    "title": "Mac MPS (Metal) Support?",
    "author": "GaleiqTesting",
    "state": "closed",
    "created_at": "2025-03-19T20:48:26Z",
    "updated_at": "2025-03-21T23:54:43Z",
    "labels": [
      "bug"
    ],
    "body": "\nHi I saw an earlier message about Mac.\n\nCould you tell me what LLM Models this project uses and reference the files I can edit to enable MPS support?\n\nIf Omniparser, here is a fix which worked for me to enable MPS support:\nhttps://github.com/microsoft/OmniParser/issues/224\n\nAnd any specific libraries that wouldn't work on Mac?\n\nI'm a Doctor and don't know much about coding. This software will help me significantly with document handling tasks.\n\nThanks.\n\n\n\n\n\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "hi, you could try  it now!  i'd update the logic, so it can run on Mac."
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 103,
    "title": "可以麻烦支持一下Azure的openAI吗",
    "author": "Rosejacka",
    "state": "closed",
    "created_at": "2025-03-17T04:07:56Z",
    "updated_at": "2025-03-21T02:50:45Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "yuruotong1",
        "body": "理论上已经适配了，你是遇到什么错误了吗"
      },
      {
        "user": "Rosejacka",
        "body": "如何在页面配置呢?我已经正确填写了我的url 以及key以及gpt4o"
      },
      {
        "user": "yuruotong1",
        "body": "报什么错呢？"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 89,
    "title": "Is it only supporting GPT-4 for now?",
    "author": "gavinju",
    "state": "closed",
    "created_at": "2025-03-13T21:01:44Z",
    "updated_at": "2025-03-21T02:41:55Z",
    "labels": [],
    "body": "Is it only supporting GPT-4 for now?\nWe hope to support more models, especially those that can be accessed through SiliconFlow.\n\ntks\n\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "We need the ability to multimodal and structured output, especially openai, which supports pydantic structured output, It could save us a lot of effort if other models with both capabilities are currently available. automate is still at a very early stage, and we're probably going to focus on multi-agent collaboration, Solve real user scenarios, and then wait until the project really solves everyone's problems before you think about compatibility adaptations, Otherwise only do toys do not do practical tools, then the project may not survive this year."
      },
      {
        "user": "Rosejacka",
        "body": "支持azure的openai嘛"
      },
      {
        "user": "yuruotong1",
        "body": "> 支持azure的openai嘛\n\n这个还没测试，理论上是支持的"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 109,
    "title": "[Feature Request]",
    "author": "devops724",
    "state": "closed",
    "created_at": "2025-03-18T06:45:17Z",
    "updated_at": "2025-03-20T18:05:04Z",
    "labels": [
      "bug"
    ],
    "body": " i serve local running VLM that provide openai api capability \nhow i can set local network endpoint , port , access key , model name...",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "Local large models are not yet supported, so it is recommended to look at the list of large models listed in readme."
      },
      {
        "user": "devops724",
        "body": "but it can be as simply as add custom url token to openai capability urls \nmost engine like vllm support text/completion and same as openai api "
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 107,
    "title": "autoMate Liinux Mint 22.1 Cinnamon - API ratelimit",
    "author": "Luci-Manus",
    "state": "closed",
    "created_at": "2025-03-18T02:35:51Z",
    "updated_at": "2025-03-19T17:52:08Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: Current\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\nFile \"/home/name/Downloads/opt/anaconda3/envs/automate/lib/python3.12/site-packages/openai/_base_client.py\", line 1061, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-1EdAY23eggNgX4Ej0UPb3d8c on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n\n```\n\n## Description\ngetting an api rate limit, maybe add a field that lets us delay the script or add api rate limit throttling so it continues to complete the task it just performs 1 entry per 20 seconds as the api entry mentions.\n\n### Current Behavior\nBroken\n\n### Expected Behavior\nThe thing to work without needing openai pro for chatgpt access\n\nOther:\n\nCan I just get this to use a local copy of ollama using a multimodel output model like gemma3 or llava or something? I tried sending the default URL to my ollama URL but couldn't get it working. \n\nMy goal is to get AutoMate and use ollama to be the eyes and hands of AutoMate which I will then use to operate LocalAI for me and it will pass the generated contents back to me via autoMate.",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "First issue: I believe the current execution speed is already very slow, which is likely caused by your API being throttled due to speed limits from other usage scenarios. Many users have been widely reporting that autoMate's performance is significantly degraded.\n\nSecond issue: This concern has been raised by numerous users. Our team will prioritize considering the integration of Ollama in future updates to enable local deployment capabilities for autoMate."
      },
      {
        "user": "Luci-Manus",
        "body": "I'm trying to get openManus or autoMate working so i can use it to troubleshoot eachother and other ai models idk if that's dumb but i'm dumber than these ai lol i cannot get anything working beyond ollama interminal and localai (which i can't figure out how to shut off)\n"
      },
      {
        "user": "yuruotong1",
        "body": "Your idea is exceptional and has unlocked a fresh direction for our product’s application! We deeply value innovative efforts like yours—please don’t hesitate to reach out anytime challenges arise. We’re here to support you every step of the way!\n"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 105,
    "title": "Does it support Apple Silicon M series?",
    "author": "fylm",
    "state": "closed",
    "created_at": "2025-03-17T09:15:06Z",
    "updated_at": "2025-03-19T14:11:28Z",
    "labels": [
      "bug"
    ],
    "body": "Does it support Apple Silicon M series?",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "Sorry, i doesn't have an M-series coMputer, but we use pure visual technology, which in theory can run on any device."
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 113,
    "title": "调用openai 返回结果 json解析失败",
    "author": "ljtth",
    "state": "closed",
    "created_at": "2025-03-19T02:46:23Z",
    "updated_at": "2025-03-19T11:17:58Z",
    "labels": [
      "bug"
    ],
    "body": "感谢开源~\n遇到一个问题，项目成功启动，api也通了。但是通了之后的返回值会报json解析失败，请问是不是和哪个的版本导致的问题。感谢回复\n\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\gradio\\queueing.py\", line 712, in process_events\n    response = await route_utils.call_process_api(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n    output = await app.get_blocks().process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 2015, in process_api\n    result = await self.call_function(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 1574, in call_function\n    prediction = await utils.async_iteration(iterator)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\gradio\\utils.py\", line 710, in async_iteration\n    return await anext(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\gradio\\utils.py\", line 704, in __anext__\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2177, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 859, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\gradio\\utils.py\", line 687, in run_sync_iterator_async\n    return next(iterator)\n           ^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\gradio\\utils.py\", line 848, in gen_wrapper\n    response = next(iterator)\n               ^^^^^^^^^^^^^^\n  File \"D:\\Demo\\autoMate\\autoMate\\gradio_ui\\app.py\", line 137, in process_input\n    for _ in sampling_loop_sync(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Demo\\autoMate\\autoMate\\gradio_ui\\loop.py\", line 33, in sampling_loop_sync\n    task_plan_agent(messages=messages, parsed_screen_result=parsed_screen_result)\n  File \"D:\\Demo\\autoMate\\autoMate\\gradio_ui\\agent\\task_plan_agent.py\", line 20, in __call__\n    response = run(messages, user_prompt=system_prompt.format(screen_info=screen_info, action_list=str(Action)), response_format=TaskPlanResponse)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\xbrain\\core\\chat.py\", line 30, in run\n    chat_response = prepare_openai_tools(messages, user_prompt, response_format)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\xbrain\\core\\chat.py\", line 8, in prepare_openai_tools\n    chat_response = chat(messages, tools=[i[\"model\"] for i in xbrain_tool.tools], user_prompt=user_prompt, response_format=response_format)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\xbrain\\utils\\openai_utils.py\", line 16, in chat\n    response = client.beta.chat.completions.parse(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 156, in parse\n    return self._post(\n           ^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1280, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 957, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _request\n    return self._process_response(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1162, in _process_response\n    return api_response.parse()\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\_response.py\", line 319, in parse\n    parsed = self._options.post_parser(parsed)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 150, in parser\n    return _parse_chat_completion(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py\", line 110, in parse_chat_completion\n    \"parsed\": maybe_parse_content(\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py\", line 161, in maybe_parse_content\n    return _parse_content(response_format, message.content)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py\", line 221, in _parse_content\n    return cast(ResponseFormatT, model_parse_json(response_format, content))\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\openai\\_compat.py\", line 169, in model_parse_json\n    return model.model_validate_json(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Tools\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py\", line 597, in model_validate_json\n    return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for TaskPlanResponse\n  Input should be an object [type=model_type, input_value=[{'reasoning': \"用户希...输入'horldworld'\"]}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.8/v/model_type",
    "comments": [
      {
        "user": "ljtth",
        "body": "pydantic                  2.8.2\npydantic_core             2.20.1\nopenai                    1.55.3\n\napi：https://api.openai-next.com/v1/  \nmodel: gpt-4o"
      },
      {
        "user": "yuruotong1",
        "body": "可能 openai-next 有问题，可以尝试用一下野卡"
      },
      {
        "user": "ljtth",
        "body": "确实是 openai-next 是有问题了，换了另外一个就好了。openai-next白充值了，没法用。您能联系那边给退款不。他那网站里面连个客服都没有，感谢"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 115,
    "title": "找不到openai的包",
    "author": "A97139012",
    "state": "closed",
    "created_at": "2025-03-19T06:28:29Z",
    "updated_at": "2025-03-19T08:28:24Z",
    "labels": [
      "bug"
    ],
    "body": "\n## Error Message\n启动Omniserver服务中，因为加载模型真的超级慢，请耐心等待！\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"E:\\secert_data\\automate\\autoMate\\omniserver.py\", line 14, in <module>\n[SERVER-ERR]     from util.omniparser import Omniparser\n[SERVER-ERR]   File \"E:\\secert_data\\automate\\autoMate\\util\\omniparser.py\", line 1, in <module>\n[SERVER-ERR]     from util.utils import get_som_labeled_img, get_caption_model_processor, get_yolo_model, check_ocr_box\n[SERVER-ERR]   File \"E:\\secert_data\\automate\\autoMate\\util\\utils.py\", line 11, in <module>\n[SERVER-ERR]     from openai import AzureOpenAI\n[SERVER-ERR] ModuleNotFoundError: No module named 'openai'\nTraceback (most recent call last):\n  File \"E:\\secert_data\\automate\\autoMate\\main.py\", line 78, in <module>\n    run()\n  File \"E:\\secert_data\\automate\\autoMate\\main.py\", line 55, in run\n    raise RuntimeError(f\"服务器进程报错退出：{server_process.returncode}\")\nRuntimeError: 服务器进程报错退出：1\n\n\n\n## Description\n我已经在虚拟环境中安装了openai，但是还是无法找到这个包，但是我单独utils.py是可以运行的，但是如果我运行main.py就会报错\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "你这个版本的代码非常旧了，可以拉一下最新代码！"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 108,
    "title": "[BUG] vlm_agent.py报了个错",
    "author": "skydragron",
    "state": "closed",
    "created_at": "2025-03-18T05:50:47Z",
    "updated_at": "2025-03-19T07:12:16Z",
    "labels": [
      "bug"
    ],
    "body": "\\gradio_ui\\agent\\vlm_agent.py\", line 76, in __call__     vlm_response, token_usage = run_oai_interleaved(，运行后在网页中输入apikey再输入问题，出现如上报错",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "这个是很早的代码了，辛苦更新一下"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 102,
    "title": "测试不是很成功！",
    "author": "Roxywusu",
    "state": "closed",
    "created_at": "2025-03-17T01:59:13Z",
    "updated_at": "2025-03-19T04:25:46Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "yuruotong1",
        "body": "可以详细描述一下你的问题，用一下我指定的API"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 110,
    "title": "[BUG]",
    "author": "skydragron",
    "state": "closed",
    "created_at": "2025-03-18T10:37:19Z",
    "updated_at": "2025-03-19T04:17:56Z",
    "labels": [
      "bug"
    ],
    "body": "运行python main后报错\nModuleNotFoundError: No module named 'xbrain'，但是我安装了该模块，\n执行pip show xbrain后：\nName: xbrain\nVersion: 0.0.1\nSummary: Python library of standardized neural data analysis pipelines\nHome-page: https://github.com/doerlbh/xbrain\nAuthor: Baihan Lin\nAuthor-email: doerlbh@gmail.com\nLicense: GPLv3\nLocation: ...\\lib\\site-packages\\xbrain-0.0.1-py3.12.egg\nRequires: numpy, pandas\nRequired-by:\n\n\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "建议按照 readme 安装 requirements，是 pyxbrain 而不是 xbrain"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 111,
    "title": "[BUG] 硅基流动的api是用不了吗",
    "author": "webwww123",
    "state": "closed",
    "created_at": "2025-03-18T12:53:55Z",
    "updated_at": "2025-03-19T04:17:50Z",
    "labels": [
      "bug"
    ],
    "body": "如题",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "支持的大模型已经在 readme 中介绍了"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 112,
    "title": "--",
    "author": "skydragron",
    "state": "closed",
    "created_at": "2025-03-19T01:59:33Z",
    "updated_at": "2025-03-19T03:20:48Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\n\n```\n\n## Description\n\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 106,
    "title": "运行之后不报错，localhost7888也无法访问",
    "author": "qzltianxing",
    "state": "closed",
    "created_at": "2025-03-17T16:48:46Z",
    "updated_at": "2025-03-19T01:37:21Z",
    "labels": [
      "bug"
    ],
    "body": "## Version Information\n- Commit Hash: <!-- e.g. abc123def456 -->\n\n## Error Message\n<!-- Paste your error message in markdown -->\n```\nPS G:\\autoMate> python main.py\ncuda is_available:  True\nMPS is_available:  False\ncuda device_count 1\ncuda device_name NVIDIA GeForce RTX 2070 SUPER\nModel files already detected!\n```\n\n## Description\n运行到这里就没反应了，http://localhost:7888/ 也无法访问\n\n### Current Behavior\n<!-- Describe what actually happened -->\n\n### Expected Behavior\n<!-- Describe what you expected to happen -->\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "稍等我来看看！"
      },
      {
        "user": "yuruotong1",
        "body": "可以拉一下最新的代码，试试还有问题吗？"
      },
      {
        "user": "SkalaFrost",
        "body": "i got the same problem with lastest code base\n"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 104,
    "title": "OSError: We couldn't connect to 'https://huggingface.co' to load this file",
    "author": "damaomaom",
    "state": "closed",
    "created_at": "2025-03-17T06:40:20Z",
    "updated_at": "2025-03-18T02:17:46Z",
    "labels": [
      "bug"
    ],
    "body": "已经是最新的程序包，还是出现这个。\n(automate) D:\\autoMate\\autoMate>python main.py\ncuda is_available:  False\nMPS is_available:  False\ncuda device_count 0\nGPU driver is not compatible, please install the appropriate version of torch according to the readme!\nModel files already detected!\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n    raise err\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n    raise new_e\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n    self._validate_conn(conn)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n    conn.connect()\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n    self.sock = sock = self._new_conn()\n                       ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 207, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001CC9089DC40>, 'Connection to huggingface.co timed out. (connect timeout=10)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001CC9089DC40>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1376, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1296, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 280, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 303, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 688, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001CC9089DC40>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: e11b73d6-9a22-41bb-aa19-2f12053c7470)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 342, in cached_file\n    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1489, in _raise_on_head_call_error\n    raise LocalEntryNotFoundError(\nhuggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\autoMate\\autoMate\\main.py\", line 19, in <module>\n    run()\n  File \"D:\\autoMate\\autoMate\\main.py\", line 15, in run\n    app.run()\n  File \"D:\\autoMate\\autoMate\\gradio_ui\\app.py\", line 320, in run\n    vision_agent = VisionAgent(yolo_model_path=os.path.join(MODEL_DIR, \"icon_detect\", \"model.pt\"),\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\autoMate\\autoMate\\gradio_ui\\agent\\vision_agent.py\", line 35, in __init__\n    self.caption_processor = AutoProcessor.from_pretrained(\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py\", line 305, in from_pretrained\n    config = AutoConfig.from_pretrained(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1075, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 594, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 653, in _get_config_dict\n    resolved_config_file = cached_file(\n                           ^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n    raise EnvironmentError(\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like microsoft/Florence-2-base is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "我们已经找到原因，了正在修改代码"
      },
      {
        "user": "yuruotong1",
        "body": "这个问题已经解决了，可以看最新代码"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 97,
    "title": "无法连接到Hugging Face的服务器",
    "author": "damaomaom",
    "state": "closed",
    "created_at": "2025-03-15T02:09:15Z",
    "updated_at": "2025-03-17T04:51:20Z",
    "labels": [],
    "body": "(automate) D:\\autoMate\\autoMate-master>python main.py\ncuda is_available:  False\nMPS is_available:  False\ncuda device_count 0\nGPU driver is not compatible, please install the appropriate version of torch according to the readme!\nModel files already detected!\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n    raise err\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n    raise new_e\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n    self._validate_conn(conn)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n    conn.connect()\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n    self.sock = sock = self._new_conn()\n                       ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 207, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x00000133D476DC70>, 'Connection to huggingface.co timed out. (connect timeout=10)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000133D476DC70>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1376, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1296, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 280, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 303, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 688, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000133D476DC70>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 2615a1f1-15ce-4520-aa35-74db35af4321)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 342, in cached_file\n    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1489, in _raise_on_head_call_error\n    raise LocalEntryNotFoundError(\nhuggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\autoMate\\autoMate-master\\main.py\", line 20, in <module>\n    run()\n  File \"D:\\autoMate\\autoMate-master\\main.py\", line 16, in run\n    app.run()\n  File \"D:\\autoMate\\autoMate-master\\gradio_ui\\app.py\", line 283, in run\n    vision_agent = VisionAgent(yolo_model_path=os.path.join(MODEL_DIR, \"icon_detect\", \"model.pt\"),\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\autoMate\\autoMate-master\\gradio_ui\\agent\\vision_agent.py\", line 35, in __init__\n    self.caption_processor = AutoProcessor.from_pretrained(\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py\", line 305, in from_pretrained\n    config = AutoConfig.from_pretrained(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1075, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 594, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 653, in _get_config_dict\n    resolved_config_file = cached_file(\n                           ^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n    raise EnvironmentError(\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like microsoft/Florence-2-base is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "是最新版本代码吗？最新版本代码已经不从 huggingface 拉取镜像了"
      },
      {
        "user": "damaomaom",
        "body": "今天刚下的代码，最新的代码。\n\n(automate) D:\\autoMate\\autoMate-master>python main.py\nOMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\nOMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/."
      },
      {
        "user": "shao-ssq",
        "body": "> 是最新版本代码吗？最新版本代码已经不从 huggingface 拉取镜像了\n\n我也遇到了这个问题，似乎是https://huggingface.co/ 今天没办法访问，我早上还可以，后面拉取了最新代码，就出现了。"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 100,
    "title": "不支持WSL上的Ubuntu20.04么？",
    "author": "linuaries",
    "state": "closed",
    "created_at": "2025-03-15T14:17:20Z",
    "updated_at": "2025-03-17T04:51:11Z",
    "labels": [],
    "body": "python main.py 之后下载完所有模型，最后跳出这个提示。\n\n[SERVER-OUT] \n[SERVER-OUT] ----------------------\n[SERVER-OUT] Error Message Summary:\n[SERVER-OUT] ----------------------\n[SERVER-OUT] FatalError: `Illegal instruction` is detected by the operating system.\n[SERVER-OUT]   [TimeInfo: *** Aborted at 1742047831 (unix time) try \"date -d @1742047831\" if you are using GNU date ***]\n[SERVER-OUT]   [SignalInfo: *** SIGILL (@0x7f506957260a) received by PID 247198 (TID 0x7f5290822740) from PID 1767319050 ***]\n[SERVER-OUT] \n等待服务启动...\nTraceback (most recent call last):\n  File \"/home/aistudio/CVs/autoMate/main.py\", line 78, in <module>\n    run()\n  File \"/home/aistudio/CVs/autoMate/main.py\", line 55, in run\n    raise RuntimeError(f\"服务器进程报错退出：{server_process.returncode}\")\nRuntimeError: 服务器进程报错退出：-4",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "直接在 windows 上就能跑，还真没试过 WSL..."
      },
      {
        "user": "Gushroom",
        "body": "请检查下是否已经更新到最新版本了，最近更新频率比较高，请持续关注最新修复！"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 101,
    "title": "License?",
    "author": "limcheekin",
    "state": "closed",
    "created_at": "2025-03-16T12:47:30Z",
    "updated_at": "2025-03-17T00:44:10Z",
    "labels": [],
    "body": "Thanks for sharing the code.\n\nAppreciate if you could add a LICENSE file to the repo.",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "Thank you very much for reminding me. I will add a license！"
      },
      {
        "user": "yuruotong1",
        "body": "> Thanks for sharing the code.\n> \n> Appreciate if you could add a LICENSE file to the repo.\n\nI have add an MIT liecense."
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 98,
    "title": "我在运行的时候出现了报错：TaskPlanResponse 解析 JSON 时出现格式错误",
    "author": "shao-ssq",
    "state": "closed",
    "created_at": "2025-03-15T05:57:49Z",
    "updated_at": "2025-03-15T14:50:08Z",
    "labels": [],
    "body": "具体错误是 Invalid JSON: expected value at line 2 column 1。也就是说，返回的数据不是有效的 JSON 格式。\n\n我使用了最新的代码并且重新申请了OpenAI next的key。\n\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n    response = await route_utils.call_process_api(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\gradio\\blocks.py\", line 2103, in process_api\n    result = await self.call_function(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n    prediction = await utils.async_iteration(iterator)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\gradio\\utils.py\", line 735, in async_iteration\n    return await anext(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\gradio\\utils.py\", line 729, in __anext__\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\gradio\\utils.py\", line 712, in run_sync_iterator_async\n    return next(iterator)\n           ^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\gradio\\utils.py\", line 873, in gen_wrapper\n    response = next(iterator)\n               ^^^^^^^^^^^^^^\n  File \"D:\\autoMate\\gradio_ui\\app.py\", line 113, in process_input\n    for _ in sampling_loop_sync(\n  File \"D:\\autoMate\\gradio_ui\\loop.py\", line 35, in sampling_loop_sync\n    plan_list = task_plan_agent(messages=messages, parsed_screen_result=parsed_screen_result)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\autoMate\\gradio_ui\\agent\\task_plan_agent.py\", line 18, in __call__\n    response = run(messages, user_prompt=system_prompt.format(screen_info=screen_info), response_format=TaskPlanResponse)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\xbrain\\core\\chat.py\", line 30, in run\n    chat_response = prepare_openai_tools(messages, user_prompt, response_format)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\xbrain\\core\\chat.py\", line 8, in prepare_openai_tools\n    chat_response = chat(messages, tools=[i[\"model\"] for i in xbrain_tool.tools], user_prompt=user_prompt, response_format=response_format)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\xbrain\\utils\\openai_utils.py\", line 16, in chat\n    response = client.beta.chat.completions.parse(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 156, in parse\n    return self._post(\n           ^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_base_client.py\", line 1280, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_base_client.py\", line 957, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_base_client.py\", line 1063, in _request\n    return self._process_response(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_base_client.py\", line 1162, in _process_response\n    return api_response.parse()\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_response.py\", line 319, in parse\n    parsed = self._options.post_parser(parsed)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 150, in parser\n    return _parse_chat_completion(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py\", line 110, in parse_chat_completion\n    \"parsed\": maybe_parse_content(\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py\", line 161, in maybe_parse_content\n    return _parse_content(response_format, message.content)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py\", line 221, in _parse_content\n    return cast(ResponseFormatT, model_parse_json(response_format, content))\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_compat.py\", line 169, in model_parse_json\n    return model.model_validate_json(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\pydantic\\main.py\", line 656, in model_validate_json\n    return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for TaskPlanResponse\n  Invalid JSON: expected value at line 2 column 1 [type=json_invalid, input_value='\\n> search(\"武汉天气...做好出行安排。 ', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid",
    "comments": [
      {
        "user": "shao-ssq",
        "body": "api key 显示已经成功消费，但是返回的JSON似乎是有问题的。"
      },
      {
        "user": "shao-ssq",
        "body": "当我尝试使用最新的模型：gpt-4.5-preview-2025-02-27\n报错：\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_base_client.py\", line 1095, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"D:\\Miniconda3\\envs\\AutoMate\\Lib\\site-packages\\openai\\_base_client.py\", line 1061, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 500 - {'error': {'message': 'bad response status code 500 (request id: 2025031514080023726644529196622)', 'localized_message': 'Unknown error', 'type': 'upstream_error', 'param': '500', 'code': 'bad_response_status_code'}}"
      },
      {
        "user": "shao-ssq",
        "body": "我猜测可能是最近一次更新导致的，昨天我用你的key可以实现的，今天你的key失效了，然后我重新购买了key，然后报错了。\n\n新模型 gpt-4.5-preview-2025-02-27 是没有请求到，没有消费。\n\n我使用python SDK 的访问 gpt-4.5-preview-2025-02-27 任然是报错：openai.InternalServerError: Error code: 500 - {'error': {'message': 'bad response status code 500 (request id: 2025031514564547055486425958814)', 'localized_message': 'Unknown error', 'type': 'upstream_error', 'param': '500', 'code': 'bad_response_status_code'}}\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"key\", base_url=\"https://api.openai-next.com/v1\")\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4.5-preview-2025-02-27\",\n    stream=False,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n\nprint(completion.choices[0].message)\n"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 93,
    "title": "urllib3\\connection连接不上？模型的运行支持环境吗？",
    "author": "fishergithub",
    "state": "closed",
    "created_at": "2025-03-14T08:56:30Z",
    "updated_at": "2025-03-15T14:49:45Z",
    "labels": [],
    "body": "梯子我有的，能够科学上网。但我鼓捣了半天我还是没进展，小白求大佬赐教该怎么破~\n\n(D:\\MyProjects\\conda_envs\\automate) D:\\MyProjects\\automate>python main.py\ncuda is_available:  True\nMPS is_available:  False\ncuda device_count 1\ncuda device_name NVIDIA GeForce RTX 4060 Laptop GPU\nModel files already detected!\nD:\\MyProjects\\automate\\gradio_ui\\app.py:296: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  chatbot = gr.Chatbot(\nTraceback (most recent call last):\n  File \"D:\\MyProjects\\conda_envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\MyProjects\\conda_envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n    raise err\n  File \"D:\\MyProjects\\conda_envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: timed out",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "首先，建议更新为最新的代码，看代码报错目前还是上一个版本的内容。其次，建议优先使用我们验证过的模型，readme 有一个列表，里面会不断迭代兼容的厂商！"
      },
      {
        "user": "fishergithub",
        "body": "> 首先，建议更新为最新的代码，看代码报错目前还是上一个版本的内容。其次，建议优先使用我们验证过的模型，readme 有一个列表，里面会不断迭代兼容的厂商！\n\n好的，我已按照最新版的更新了。现在似乎还是卡在connection的问题耶？\n(D:\\MyProjects\\conda_envs\\automate) D:\\MyProjects\\automate>python main.py\ncuda is_available:  True\nMPS is_available:  False\ncuda device_count 1\ncuda device_name NVIDIA GeForce RTX 4060 Laptop GPU\nModel files already detected!\nTraceback (most recent call last):\n  File \"D:\\MyProjects\\conda_envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\MyProjects\\conda_envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n    raise err\n  File \"D:\\MyProjects\\conda_envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: timed out\n\n另外，兼容厂商的模型我明白了，目前支持的是openai~感谢回答~！"
      },
      {
        "user": "yuruotong1",
        "body": "这个报错不全， connection 是要连哪里呢？另外建议用 markdown 包裹一下你的报错信息，这样更容易让人看懂!"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 99,
    "title": "关于system prompt 的提问",
    "author": "LeeOrange-is-me",
    "state": "closed",
    "created_at": "2025-03-15T06:42:09Z",
    "updated_at": "2025-03-15T14:23:30Z",
    "labels": [],
    "body": "作者之前的prompt我记得是英文，为什么换成中文了？作者是基于什么样子的考量写中文的system prompt?\n作者是对system prompt 做了一些探究吗？\n\n作者目前有没有在当前system prompt 上面的成功的例子呢？",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "> 作者之前的prompt我记得是英文，为什么换成中文了？作者是基于什么样子的考量写中文的system prompt?\n\n之前的代码是omniparser的提示词，但总体比较简单，由于新版Agent改动较大，前期准备先用中文所示词过渡，后面待稳定后再转为英文提示词。案例的话我最近录几个发出来。"
      },
      {
        "user": "yuruotong1",
        "body": "> 作者之前的prompt我记得是英文，为什么换成中文了？作者是基于什么样子的考量写中文的system prompt? 作者是对system prompt 做了一些探究吗？\n> \n> 作者目前有没有在当前system prompt 上面的成功的例子呢？\n\n我刚才更新了一个视频，关于自动化通过微信好友的例子"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 96,
    "title": "我成功部署了，但是我在执行的时候遇到的了：无效的令牌 , 'type': 'shell_api_error'}}",
    "author": "shao-ssq",
    "state": "closed",
    "created_at": "2025-03-14T11:58:38Z",
    "updated_at": "2025-03-14T12:58:02Z",
    "labels": [],
    "body": "我的环境：\ngpt-4o-2024-11-20\nhttps://api.openai-next.com/v1\n我的 key 确定是没错的，并且是可以访问 GPT 4o，余额充足。\nwindows CMD & powershell\n\n\n我发送 send 后，控制台输出了：openai.AuthenticationError: Error code: 401 - {'error': {'message': '[proj]无效的令牌 (request id: 2025031419475264762428894645609)', 'type': 'shell_api_error'}}\n\n\n全部日志：\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n    response = await route_utils.call_process_api(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\blocks.py\", line 2103, in process_api\n    result = await self.call_function(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n    prediction = await utils.async_iteration(iterator)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\utils.py\", line 735, in async_iteration\n    return await anext(iterator)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\utils.py\", line 729, in __anext__\n    return await anyio.to_thread.run_sync(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n    return await future\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n    result = context.run(func, *args)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\utils.py\", line 712, in run_sync_iterator_async\n    return next(iterator)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\utils.py\", line 873, in gen_wrapper\n    response = next(iterator)\n  File \"D:\\autoMate\\gradio_ui\\app.py\", line 113, in process_input\n    for _ in sampling_loop_sync(\n  File \"D:\\autoMate\\gradio_ui\\loop.py\", line 35, in sampling_loop_sync\n    plan_list = task_plan_agent(messages=messages, parsed_screen_result=parsed_screen_result)\n  File \"D:\\autoMate\\gradio_ui\\agent\\task_plan_agent.py\", line 18, in __call__\n    response = run(messages, user_prompt=system_prompt.format(screen_info=screen_info), response_format=TaskPlanResponse)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\xbrain\\core\\chat.py\", line 30, in run\n    chat_response = prepare_openai_tools(messages, user_prompt, response_format)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\xbrain\\core\\chat.py\", line 8, in prepare_openai_tools\n    chat_response = chat(messages, tools=[i[\"model\"] for i in xbrain_tool.tools], user_prompt=user_prompt, response_format=response_format)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\xbrain\\utils\\openai_utils.py\", line 16, in chat\n    response = client.beta.chat.completions.parse(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 156, in parse\n    return self._post(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\_base_client.py\", line 1280, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\_base_client.py\", line 957, in request\n    return self._request(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\_base_client.py\", line 1061, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.AuthenticationError: Error code: 401 - {'error': {'message': '[proj]无效的令牌 (request id: 2025031419512898553700535924396)', 'type': 'shell_api_error'}}",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "感觉像是浏览器过期了，你刷新一下浏览器界面，然后再复制一下令牌到浏览器中重新输入一下。"
      },
      {
        "user": "shao-ssq",
        "body": "> 感觉像是浏览器过期了，你刷新一下浏览器界面，然后再复制一下令牌到浏览器中重新输入一下。\n\n我使用无痕方式，但是失败了。我的key是128 位的。"
      },
      {
        "user": "shao-ssq",
        "body": "> 感觉像是浏览器过期了，你刷新一下浏览器界面，然后再复制一下令牌到浏览器中重新输入一下。\n\n另外，我才用中转的方式，报错：AttributeError: 'str' object has no attribute 'choices'\n\n````\nTraceback (most recent call last):\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n    response = await route_utils.call_process_api(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\blocks.py\", line 2103, in process_api\n    result = await self.call_function(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n    prediction = await utils.async_iteration(iterator)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\utils.py\", line 735, in async_iteration\n    return await anext(iterator)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\utils.py\", line 729, in __anext__\n    return await anyio.to_thread.run_sync(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n    return await future\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n    result = context.run(func, *args)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\utils.py\", line 712, in run_sync_iterator_async\n    return next(iterator)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\gradio\\utils.py\", line 873, in gen_wrapper\n    response = next(iterator)\n  File \"D:\\autoMate\\gradio_ui\\app.py\", line 113, in process_input\n    for _ in sampling_loop_sync(\n  File \"D:\\autoMate\\gradio_ui\\loop.py\", line 35, in sampling_loop_sync\n    plan_list = task_plan_agent(messages=messages, parsed_screen_result=parsed_screen_result)\n  File \"D:\\autoMate\\gradio_ui\\agent\\task_plan_agent.py\", line 18, in __call__\n    response = run(messages, user_prompt=system_prompt.format(screen_info=screen_info), response_format=TaskPlanResponse)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\xbrain\\core\\chat.py\", line 30, in run\n    chat_response = prepare_openai_tools(messages, user_prompt, response_format)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\xbrain\\core\\chat.py\", line 8, in prepare_openai_tools\n    chat_response = chat(messages, tools=[i[\"model\"] for i in xbrain_tool.tools], user_prompt=user_prompt, response_format=response_format)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\xbrain\\utils\\openai_utils.py\", line 16, in chat\n    response = client.beta.chat.completions.parse(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 156, in parse\n    return self._post(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\_base_client.py\", line 1280, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\_base_client.py\", line 957, in request\n    return self._request(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\_base_client.py\", line 1063, in _request\n    return self._process_response(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\_base_client.py\", line 1162, in _process_response\n    return api_response.parse()\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\_response.py\", line 319, in parse\n    parsed = self._options.post_parser(parsed)\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 150, in parser\n    return _parse_chat_completion(\n  File \"D:\\Miniconda3\\envs\\AutoMate\\lib\\site-packages\\openai\\lib\\_parsing\\_completions.py\", line 70, in parse_chat_completion\n    for choice in chat_completion.choices:\nAttributeError: 'str' object has no attribute 'choices'\nin sampling_loop_sync, model: gpt-4o-2024-11-20\n```"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 94,
    "title": "支持mac嘛",
    "author": "wxl1779766474",
    "state": "closed",
    "created_at": "2025-03-14T10:48:08Z",
    "updated_at": "2025-03-14T12:14:06Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "yuruotong1",
        "body": "支持的"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 88,
    "title": "hmmm..... this model's url doesn't appear to be valid?",
    "author": "weisk",
    "state": "closed",
    "created_at": "2025-03-13T18:45:26Z",
    "updated_at": "2025-03-14T12:13:24Z",
    "labels": [],
    "body": "in [this commit](https://github.com/yuruotong1/autoMate/commit/23331aaa67c75e66ddccbc096eb0bacc38815bc7#diff-4303b0bda0cedc61d2f19bc7148d039b7c82738f78e52df01b1ccafa2b80b11bR3),\n\nthis url was introduced for the openainext model api, which appears to be a phising site...\n\ncloudflare prevents me from accessing it, also this is reported by virustotal:\n\n![Image](https://github.com/user-attachments/assets/4061201e-106c-4da7-b4de-b5c1862c5643)",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "You can also use openai's official gpt 4o interface directly!"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 87,
    "title": "时常sentitive words detected",
    "author": "YXuan66",
    "state": "closed",
    "created_at": "2025-03-13T13:00:09Z",
    "updated_at": "2025-03-14T12:13:11Z",
    "labels": [],
    "body": "我更换了https://api.moleapi.com/v1作为baseurl调用gpt4o，但是经常openai.BadRequestError: Error code: 400 - {'error': {'message': 'sensitive words: 8964,8964 (request id: 20250313205909537876276OGDih6VW)', 'type': 'mole_api_error', 'param': '', 'code': 'sensitive_words_detected'}}",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "这个明显是这个网站额外加的限制，它限制了一些敏感词提问，也可以换下我们推荐的代理站"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 90,
    "title": "功能执行如何提高准确度",
    "author": "llk2014",
    "state": "closed",
    "created_at": "2025-03-14T03:46:44Z",
    "updated_at": "2025-03-14T12:13:04Z",
    "labels": [],
    "body": "看了一圈issue发现没有人问相关问题，不知道大家的运行情况如何\n\n我给出的prompt是：在当前浏览器打开douyin.com\n\n试了百炼上所有模型，都没有成功打开抖音\n走得最远的是deepseek-v3，他在识别浏览器、找到地址栏这一步卡住了，反复的打开浏览器，看起来他不认识浏览器长什么样\n尝试了示例的删txt的prompt，也没有成功，停在了右键菜单，点击删除这一步\n\n一个最常见的问题是：\npydantic_core._pydantic_core.ValidationError: 1 validation error for TaskRunAgentResponse\n  Input should be an object [type=model_type, input_value=1.0, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n偶发性的，不知道是不是国内大模型的问题，gpt-4o是否不存在这个问题\n\n我的问题是：如何提高功能准确度\n是否与屏幕分辨率，图标大小，显示器元素数量有关系。。。",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "问题一：跑下来感觉omniparser的打标估计只有30%左右的准确率，如果要实现精准识别目前是不可能的，这应该是computer use类工具的通病，分析下来有两个解决方案（1）通过优化提示词增加多agent规划能力（2）接入更好的多模态大模态。但感觉目前两种方法都 不会解决根本问题，我们也在积极的寻找更优的方案；\n问题二：`1 validation error ` 这个问题应该是输出的内容不支持结构化输出，readme有提供一份已经测试通过的大模型厂商的清单，这块还在逐渐适配中。"
      },
      {
        "user": "Rosejacka",
        "body": "我的建议是你们检查omniparser的ocr设置是否加上了中文，他的参数默认设置是英文，导致中文识别率很差"
      },
      {
        "user": "yuruotong1",
        "body": "> 我的建议是你们检查omniparser的ocr设置是否加上了中文，他的参数默认设置是英文，导致中文识别率很差\n\nautoMate已经加上了中文识别"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 81,
    "title": "这个可以调用自己的本地部署的模型吗？",
    "author": "djfch",
    "state": "closed",
    "created_at": "2025-03-12T09:16:26Z",
    "updated_at": "2025-03-14T09:59:27Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "yuruotong1",
        "body": "是想接ollama吗？"
      },
      {
        "user": "Zephyr69",
        "body": "同问，建议接入xinference的推理api"
      },
      {
        "user": "yuruotong1",
        "body": "目前暂时只测试了 `https://api.openai-next.com` 的接口，因为代码中用了结构化输出和多模态能力，本地模型可能不兼容的情况比较多，现阶段主要是修复bug和完善使用场景，不过兼容本地模型我们后面会逐步优化的。"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 92,
    "title": "底座用的是OmniParser？",
    "author": "Rosejacka",
    "state": "closed",
    "created_at": "2025-03-14T07:36:11Z",
    "updated_at": "2025-03-14T09:41:46Z",
    "labels": [],
    "body": "底座用的是OmniParser？",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "是的，打标模型用的是OmniParser的模型。"
      },
      {
        "user": "Rosejacka",
        "body": "支不支持azure的gpt呀？"
      },
      {
        "user": "Rosejacka",
        "body": "我有个问题哈，OmniParser框架默认的需要这样改reader = easyocr.Reader(【'en'】)改成reader = easyocr.Reader(【'ch_sim','en'】)，才会支持中文的识别，在你这里能改么？"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 61,
    "title": "MacBook测试了下，分辨率不对什么情况",
    "author": "qauzy",
    "state": "closed",
    "created_at": "2025-03-07T02:55:00Z",
    "updated_at": "2025-03-13T23:28:16Z",
    "labels": [],
    "body": "MacBook测试了下，分辨率不对什么情况",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "感谢反馈，请问是定位不准确吗"
      },
      {
        "user": "qauzy",
        "body": "> 感谢反馈，请问是定位不准确吗\n是的，比如屏幕分辨率是2k，然后截图大小时4k，然后他定位的时候就超出屏幕了"
      },
      {
        "user": "Gushroom",
        "body": "> > 感谢反馈，请问是定位不准确吗\n> > 是的，比如屏幕分辨率是2k，然后截图大小时4k，然后他定位的时候就超出屏幕了\n\n没能复现这个问题，能详细说下配置吗？定位超出屏幕是怎么发现的呢？"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 86,
    "title": "APIkey从哪里获取",
    "author": "skydragron",
    "state": "closed",
    "created_at": "2025-03-13T10:03:27Z",
    "updated_at": "2025-03-13T23:28:05Z",
    "labels": [],
    "body": "APIkey从哪里获取",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "我们公布了一个已经测试过的大模型列表，可以参考这个列表：\nhttps://github.com/yuruotong1/autoMate/blob/master/SUPPORT_MODEL.md"
      },
      {
        "user": "zhangxiaokai007",
        "body": "不能用qwen2.5vl么？？？"
      },
      {
        "user": "yuruotong1",
        "body": "qwen2.5vl不支持结构化输出，之前几个版本，大模型返回的很多次操作都卡在了返回内容非结构化上！这块我们用了很多的时间去修修BUG，现在多模态+结构化只有 openai 的 gpt4o、o3mini。不过我们会时刻关注国内的模型厂商，尽快适配。"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 80,
    "title": "服务器进程报错退出",
    "author": "damaomaom",
    "state": "closed",
    "created_at": "2025-03-12T08:24:24Z",
    "updated_at": "2025-03-13T23:27:52Z",
    "labels": [],
    "body": "(automate) D:\\autoMate\\autoMate-master>python main.py\ncuda is_available: False\nMPS is_available: False\ncuda device_count 0\n显卡驱动不适配，请根据readme安装合适版本的 torch！\n已经检测到模型文件！\n启动Omniserver服务中，因为加载模型真的超级慢，请耐心等待！\n等待服务启动...\n[SERVER-ERR] [2025-03-12 15:16:04,245] [ WARNING] easyocr.py:80 - Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n等待服务启动...\n等待服务启动...\n等待服务启动...\n等待服务启动...\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n[SERVER-ERR] sock = connection.create_connection(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n[SERVER-ERR] raise err\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n[SERVER-ERR] sock.connect(sa)\n[SERVER-ERR] TimeoutError: timed out\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n[SERVER-ERR] response = self._make_request(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n[SERVER-ERR] raise new_e\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n[SERVER-ERR] self._validate_conn(conn)\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n[SERVER-ERR] conn.connect()\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n[SERVER-ERR] self.sock = sock = self._new_conn()\n[SERVER-ERR] ^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 207, in _new_conn\n[SERVER-ERR] raise ConnectTimeoutError(\n[SERVER-ERR] urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)')\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n[SERVER-ERR] resp = conn.urlopen(\n[SERVER-ERR] ^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n[SERVER-ERR] retries = retries.increment(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n[SERVER-ERR] raise MaxRetryError(_pool, url, reason) from reason # type: ignore[arg-type]\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\n[SERVER-ERR]\n[SERVER-ERR] During handling of the above exception, another exception occurred:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1376, in _get_metadata_or_catch_error\n[SERVER-ERR] metadata = get_hf_file_metadata(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils_validators.py\", line 114, in _inner_fn\n[SERVER-ERR] return fn(*args, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1296, in get_hf_file_metadata\n[SERVER-ERR] r = _request_wrapper(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 280, in _request_wrapper\n[SERVER-ERR] response = _request_wrapper(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 303, in _request_wrapper\n[SERVER-ERR] response = get_session().request(method=method, url=url, **params)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n[SERVER-ERR] resp = self.send(prep, **send_kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n[SERVER-ERR] r = adapter.send(request, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils_http.py\", line 96, in send\n[SERVER-ERR] return super().send(request, *args, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 688, in send\n[SERVER-ERR] raise ConnectTimeout(e, request=request)\n[SERVER-ERR] requests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 60ba7c86-ef24-4a9f-9382-58851c6f21b3)')\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 342, in cached_file\n[SERVER-ERR] resolved_file = hf_hub_download(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils_validators.py\", line 114, in _inner_fn\n[SERVER-ERR] return fn(*args, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 862, in hf_hub_download\n[SERVER-ERR] return _hf_hub_download_to_cache_dir(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n[SERVER-ERR] _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1489, in _raise_on_head_call_error\n[SERVER-ERR] raise LocalEntryNotFoundError(\n[SERVER-ERR] huggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\autoMate\\autoMate-master\\omniserver.py\", line 32, in\n[SERVER-ERR] omniparser = Omniparser(config)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\autoMate\\autoMate-master\\util\\omniparser.py\", line 12, in init\n[SERVER-ERR] self.caption_model_processor = get_caption_model_processor(model_name=config['caption_model_name'], model_name_or_path=config['caption_model_path'], device=device)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\autoMate\\autoMate-master\\util\\utils.py\", line 63, in get_caption_model_processor\n[SERVER-ERR] processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py\", line 305, in from_pretrained\n[SERVER-ERR] config = AutoConfig.from_pretrained(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1075, in from_pretrained\n[SERVER-ERR] config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 594, in get_config_dict\n[SERVER-ERR] config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 653, in _get_config_dict\n[SERVER-ERR] resolved_config_file = cached_file(\n[SERVER-ERR] ^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n[SERVER-ERR] raise EnvironmentError(\n[SERVER-ERR] OSError: We couldn't connect to 'https://huggingface.co/' to load this file, couldn't find it in the cached files and it looks like microsoft/Florence-2-base is not the path to a directory containing a file named config.json.\n[SERVER-ERR] Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\nTraceback (most recent call last):\nFile \"D:\\autoMate\\autoMate-master\\main.py\", line 78, in\nrun()\nFile \"D:\\autoMate\\autoMate-master\\main.py\", line 55, in run\nraise RuntimeError(f\"服务器进程报错退出：{server_process.returncode}\")\nRuntimeError: 服务器进程报错退出：1",
    "comments": [
      {
        "user": "ansemz",
        "body": "你这个是因为在国内环境，没法下载hf上的模型。给shell配置一个科学上网的代理应该就可以了。"
      },
      {
        "user": "damaomaom",
        "body": "暂时没有部署到 服务器上面，没有固定IP地址。这个Shell如何配置代理？"
      },
      {
        "user": "yuruotong1",
        "body": "我这里加一个针对国内的镜像地址吧，这样你就不用动了"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 79,
    "title": "非conda环境运行报No module named 'fastapi'的解决办法",
    "author": "ansemz",
    "state": "closed",
    "created_at": "2025-03-12T06:23:35Z",
    "updated_at": "2025-03-13T23:27:41Z",
    "labels": [],
    "body": "我本人没有用conda，使用uv和venv都无法执行成功。报错如下：\n```\ncuda is_available:  False\nMPS is_available:  False\ncuda device_count 0\n显卡驱动不适配，请根据readme安装合适版本的 torch！\n已经检测到模型文件！\n启动Omniserver服务中，因为加载模型真的超级慢，请耐心等待！\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\uv\\autoMate1\\omniserver.py\", line 8, in <module>\n[SERVER-ERR]     from fastapi import FastAPI\n[SERVER-ERR] ModuleNotFoundError: No module named 'fastapi'\nTraceback (most recent call last):\n  File \"D:\\uv\\autoMate1\\main.py\", line 78, in <module>\n    run()\n  File \"D:\\uv\\autoMate1\\main.py\", line 55, in run\n    raise RuntimeError(f\"服务器进程报错退出：{server_process.returncode}\")\nRuntimeError: 服务器进程报错退出：1\n```\n应该是由于subprocess启动的python其实不是venv中的python，所以找不到fastapi。把main.py修改一下就可以了。\n```\n1   + import sys\n22 -         [\"python\", \"./omniserver.py\"],\n22 +        [sys.executable, \"./omniserver.py\"],\n```\n\n供和我一样的新手参考\n\n\nupdate 1：\n又发现一个文件gradio_ui/tools/computer.py直接调用的python。同样修改一下\n```\n1   + import sys\n224 -                 [\"python\", \"-c\", \"import pyautogui; print(pyautogui.size())\"]\n224 +                 [sys.executable, \"-c\", \"import pyautogui; print(pyautogui.size())\"]\n```\n这样就可以了。\n\nbtw：cpu跑确实真的慢\n",
    "comments": [
      {
        "user": "damaomaom",
        "body": "老师，我是conda环境  也报上面的错误。按照您的修改了 main.py文件 但是还是不行。\n\n(automate) D:\\autoMate\\autoMate-master>python main.py\ncuda is_available: False\nMPS is_available: False\ncuda device_count 0\n显卡驱动不适配，请根据readme安装合适版本的 torch！\n已经检测到模型文件！\n启动Omniserver服务中，因为加载模型真的超级慢，请耐心等待！\n等待服务启动...\n[SERVER-ERR] [2025-03-12 15:16:04,245] [ WARNING] easyocr.py:80 - Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n等待服务启动...\n等待服务启动...\n等待服务启动...\n等待服务启动...\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n[SERVER-ERR] sock = connection.create_connection(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n[SERVER-ERR] raise err\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n[SERVER-ERR] sock.connect(sa)\n[SERVER-ERR] TimeoutError: timed out\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n[SERVER-ERR] response = self._make_request(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n[SERVER-ERR] raise new_e\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n[SERVER-ERR] self._validate_conn(conn)\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n[SERVER-ERR] conn.connect()\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n[SERVER-ERR] self.sock = sock = self._new_conn()\n[SERVER-ERR] ^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 207, in _new_conn\n[SERVER-ERR] raise ConnectTimeoutError(\n[SERVER-ERR] urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)')\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n[SERVER-ERR] resp = conn.urlopen(\n[SERVER-ERR] ^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n[SERVER-ERR] retries = retries.increment(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n[SERVER-ERR] raise MaxRetryError(_pool, url, reason) from reason # type: ignore[arg-type]\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\n[SERVER-ERR]\n[SERVER-ERR] During handling of the above exception, another exception occurred:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1376, in _get_metadata_or_catch_error\n[SERVER-ERR] metadata = get_hf_file_metadata(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils_validators.py\", line 114, in _inner_fn\n[SERVER-ERR] return fn(*args, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1296, in get_hf_file_metadata\n[SERVER-ERR] r = _request_wrapper(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 280, in _request_wrapper\n[SERVER-ERR] response = _request_wrapper(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 303, in _request_wrapper\n[SERVER-ERR] response = get_session().request(method=method, url=url, **params)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n[SERVER-ERR] resp = self.send(prep, **send_kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n[SERVER-ERR] r = adapter.send(request, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils_http.py\", line 96, in send\n[SERVER-ERR] return super().send(request, *args, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 688, in send\n[SERVER-ERR] raise ConnectTimeout(e, request=request)\n[SERVER-ERR] requests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 60ba7c86-ef24-4a9f-9382-58851c6f21b3)')\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 342, in cached_file\n[SERVER-ERR] resolved_file = hf_hub_download(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils_validators.py\", line 114, in _inner_fn\n[SERVER-ERR] return fn(*args, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 862, in hf_hub_download\n[SERVER-ERR] return _hf_hub_download_to_cache_dir(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n[SERVER-ERR] _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1489, in _raise_on_head_call_error\n[SERVER-ERR] raise LocalEntryNotFoundError(\n[SERVER-ERR] huggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR] File \"D:\\autoMate\\autoMate-master\\omniserver.py\", line 32, in\n[SERVER-ERR] omniparser = Omniparser(config)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\autoMate\\autoMate-master\\util\\omniparser.py\", line 12, in init\n[SERVER-ERR] self.caption_model_processor = get_caption_model_processor(model_name=config['caption_model_name'], model_name_or_path=config['caption_model_path'], device=device)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\autoMate\\autoMate-master\\util\\utils.py\", line 63, in get_caption_model_processor\n[SERVER-ERR] processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py\", line 305, in from_pretrained\n[SERVER-ERR] config = AutoConfig.from_pretrained(\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1075, in from_pretrained\n[SERVER-ERR] config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 594, in get_config_dict\n[SERVER-ERR] config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n[SERVER-ERR] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 653, in _get_config_dict\n[SERVER-ERR] resolved_config_file = cached_file(\n[SERVER-ERR] ^^^^^^^^^^^^\n[SERVER-ERR] File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n[SERVER-ERR] raise EnvironmentError(\n[SERVER-ERR] OSError: We couldn't connect to 'https://huggingface.co/' to load this file, couldn't find it in the cached files and it looks like microsoft/Florence-2-base is not the path to a directory containing a file named config.json.\n[SERVER-ERR] Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\nTraceback (most recent call last):\nFile \"D:\\autoMate\\autoMate-master\\main.py\", line 78, in\nrun()\nFile \"D:\\autoMate\\autoMate-master\\main.py\", line 55, in run\nraise RuntimeError(f\"服务器进程报错退出：{server_process.returncode}\")\nRuntimeError: 服务器进程报错退出：1"
      },
      {
        "user": "yuruotong1",
        "body": "这个应该是网络问题，模型没下载下来，稍等我下个版本更新一个国内镜像！"
      },
      {
        "user": "yuruotong1",
        "body": "代码已经更新完了，可以再试一下 @ansemz  @damaomaom "
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 77,
    "title": "信息: 用提供的模式无法找到文件。这是缺少哪个文件呢",
    "author": "54clz",
    "state": "closed",
    "created_at": "2025-03-12T02:26:40Z",
    "updated_at": "2025-03-13T23:27:29Z",
    "labels": [],
    "body": "(automate) D:\\autoMate>python main.py\ncuda is_available:  True\nMPS is_available:  False\ncuda device_count 1\ncuda device_name NVIDIA GeForce RTX 4070 Ti SUPER\n已经检测到模型文件！\n启动Omniserver服务中，因为加载模型真的超级慢，请耐心等待！\nOmniparser服务启动成功...\nC:\\ProgramData\\miniconda3\\envs\\automate\\Lib\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  warnings.warn(\n信息: 用提供的模式无法找到文件。\n* Running on local URL:  http://0.0.0.0:7888\nTraceback (most recent call last):\n  File \"D:\\autoMate\\main.py\", line 78, in <module>\n    run()\n  File \"D:\\autoMate\\main.py\", line 59, in run\n    app.run()\n  File \"D:\\autoMate\\gradio_ui\\app.py\", line 318, in run\n    demo.launch(server_name=\"0.0.0.0\", server_port=7888)\n  File \"C:\\ProgramData\\miniconda3\\envs\\automate\\Lib\\site-packages\\gradio\\blocks.py\", line 2620, in launch\n    raise ValueError(\nValueError: When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "可以尝试在启动的代码加上这个：\n`demo.launch(server_name=\"0.0.0.0\", server_port=7888, share=True)`"
      },
      {
        "user": "yuruotong1",
        "body": "代码已经更新了，可以再试一下"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 72,
    "title": "Error in `Computer.py`",
    "author": "LeeOrange-is-me",
    "state": "closed",
    "created_at": "2025-03-11T08:09:08Z",
    "updated_at": "2025-03-13T23:27:18Z",
    "labels": [],
    "body": "``` APL\n  File \"[][][]\\autoMate\\gradio_ui\\tools\\computer.py\", line 202, in screenshot\n    screenshot = self.padding_image(screenshot)\n                                    ^^^^^^^^^^\nUnboundLocalError: cannot access local variable 'screenshot' where it is not associated with a value\n\n```\n\nWe can find that in `\\autoMate\\gradio_ui\\tools\\computer.py`. In line 3, we see the `screenshot` firstly, but WHERE is it first defined?\n\n``` Python\nasync def screenshot(self):\n        if not hasattr(self, 'target_dimension'):\n            screenshot = self.padding_image(screenshot)\n            self.target_dimension = MAX_SCALING_TARGETS[\"WXGA\"]\n        width, height = self.target_dimension[\"width\"], self.target_dimension[\"height\"]\n        screenshot, path = get_screenshot(resize=True, target_width=width, target_height=height)\n        time.sleep(0.7) # avoid async error as actions take time to complete\n        return ToolResult(base64_image=base64.b64encode(path.read_bytes()).decode())\n```\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "I think this is a bug, and I'm refactoring this part of the code, thanks for the issue."
      },
      {
        "user": "yuruotong1",
        "body": "The code has been updated, you can try it again. @LeeOrange-is-me "
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 70,
    "title": "执行报错 一直超时",
    "author": "damaomaom",
    "state": "closed",
    "created_at": "2025-03-11T06:28:41Z",
    "updated_at": "2025-03-13T11:43:11Z",
    "labels": [],
    "body": "cuda is_available:  False\nMPS is_available:  False\ncuda device_count 0\n显卡驱动不适配，请根据readme安装合适版本的 torch！\n未检测到模型文件，需要下载 6 个文件\n正在下载: icon_detect/train_args.yaml (尝试 1/3)\n[SERVER-ERR] [2025-03-11 14:13:13,527] [ WARNING] easyocr.py:80 - Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n[SERVER-ERR] [2025-03-11 14:13:13,529] [ WARNING] easyocr.py:251 - Downloading detection model, please wait. This may take several minutes depending upon your network connection.\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n    raise err\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n    raise new_e\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n    self._validate_conn(conn)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n    conn.connect()\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n    self.sock = sock = self._new_conn()\n                       ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 207, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x0000022555C13590>, 'Connection to huggingface.co timed out. (connect timeout=10)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/OmniParser-v2.0/resolve/main/icon_detect/train_args.yaml (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022555C13590>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1376, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1296, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 280, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 303, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 688, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/OmniParser-v2.0/resolve/main/icon_detect/train_args.yaml (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022555C13590>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 9123285a-634f-4da2-b8df-dca0c2d2c582)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Scripts\\huggingface-cli.exe\\__main__.py\", line 7, in <module>\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\huggingface_cli.py\", line 57, in main\n    service.run()\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\download.py\", line 153, in run\n    print(self._download())  # Print path to downloaded files\n          ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\download.py\", line 166, in _download\n    return hf_hub_download(\n           ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 842, in hf_hub_download\n    return _hf_hub_download_to_local_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1091, in _hf_hub_download_to_local_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1489, in _raise_on_head_call_error\n    raise LocalEntryNotFoundError(\nhuggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n下载失败: icon_detect/train_args.yaml，正在重试...\n正在下载: icon_detect/train_args.yaml (尝试 2/3)\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n    raise err\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n    raise new_e\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n    self._validate_conn(conn)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n    conn.connect()\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n    self.sock = sock = self._new_conn()\n                       ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 207, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001F637553770>, 'Connection to huggingface.co timed out. (connect timeout=10)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/OmniParser-v2.0/resolve/main/icon_detect/train_args.yaml (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001F637553770>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1376, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1296, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 280, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 303, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 688, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/OmniParser-v2.0/resolve/main/icon_detect/train_args.yaml (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001F637553770>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: e4d13bb2-e913-42ba-9de3-809927ee782b)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Scripts\\huggingface-cli.exe\\__main__.py\", line 7, in <module>\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\huggingface_cli.py\", line 57, in main\n    service.run()\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\download.py\", line 153, in run\n    print(self._download())  # Print path to downloaded files\n          ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\download.py\", line 166, in _download\n    return hf_hub_download(\n           ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 842, in hf_hub_download\n    return _hf_hub_download_to_local_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1091, in _hf_hub_download_to_local_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1489, in _raise_on_head_call_error\n    raise LocalEntryNotFoundError(\nhuggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n下载失败: icon_detect/train_args.yaml，正在重试...\n正在下载: icon_detect/train_args.yaml (尝试 3/3)\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\urllib\\request.py\", line 1344, in do_open\n[SERVER-ERR]     h.request(req.get_method(), req.selector, req.data, headers,\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\http\\client.py\", line 1319, in request\n[SERVER-ERR]     self._send_request(method, url, body, headers, encode_chunked)\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\http\\client.py\", line 1365, in _send_request\n[SERVER-ERR]     self.endheaders(body, encode_chunked=encode_chunked)\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\http\\client.py\", line 1314, in endheaders\n[SERVER-ERR]     self._send_output(message_body, encode_chunked=encode_chunked)\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\http\\client.py\", line 1074, in _send_output\n[SERVER-ERR]     self.send(msg)\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\http\\client.py\", line 1018, in send\n[SERVER-ERR]     self.connect()\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\http\\client.py\", line 1453, in connect\n[SERVER-ERR]     super().connect()\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\http\\client.py\", line 984, in connect\n[SERVER-ERR]     self.sock = self._create_connection(\n[SERVER-ERR]                 ^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\socket.py\", line 852, in create_connection\n[SERVER-ERR]     raise exceptions[0]\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\socket.py\", line 837, in create_connection\n[SERVER-ERR]     sock.connect(sa)\n[SERVER-ERR] TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。\n[SERVER-ERR]\n[SERVER-ERR] During handling of the above exception, another exception occurred:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\autoMate\\autoMate-master\\omniserver.py\", line 14, in <module>\n[SERVER-ERR]     from util.omniparser import Omniparser\n[SERVER-ERR]   File \"D:\\autoMate\\autoMate-master\\util\\omniparser.py\", line 1, in <module>\n[SERVER-ERR]     from util.utils import get_som_labeled_img, get_caption_model_processor, get_yolo_model, check_ocr_box\n[SERVER-ERR]   File \"D:\\autoMate\\autoMate-master\\util\\utils.py\", line 22, in <module>\n[SERVER-ERR]     reader = easyocr.Reader(['en', 'ch_sim'])\n[SERVER-ERR]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\easyocr\\easyocr.py\", line 92, in __init__\n[SERVER-ERR]     detector_path = self.getDetectorPath(detect_network)\n[SERVER-ERR]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\easyocr\\easyocr.py\", line 253, in getDetectorPath\n[SERVER-ERR]     download_and_unzip(self.detection_models[self.detect_network]['url'], self.detection_models[self.detect_network]['filename'], self.model_storage_directory, self.verbose)\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\easyocr\\utils.py\", line 628, in download_and_unzip\n[SERVER-ERR]     urlretrieve(url, zip_path, reporthook=reporthook)\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\urllib\\request.py\", line 240, in urlretrieve\n[SERVER-ERR]     with contextlib.closing(urlopen(url, data)) as fp:\n[SERVER-ERR]                             ^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\urllib\\request.py\", line 215, in urlopen\n[SERVER-ERR]     return opener.open(url, data, timeout)\n[SERVER-ERR]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\urllib\\request.py\", line 515, in open\n[SERVER-ERR]     response = self._open(req, data)\n[SERVER-ERR]                ^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\urllib\\request.py\", line 532, in _open\n[SERVER-ERR]     result = self._call_chain(self.handle_open, protocol, protocol +\n[SERVER-ERR]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\urllib\\request.py\", line 492, in _call_chain\n[SERVER-ERR]     result = func(*args)\n[SERVER-ERR]              ^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\urllib\\request.py\", line 1392, in https_open\n[SERVER-ERR]     return self.do_open(http.client.HTTPSConnection, req,\n[SERVER-ERR]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\urllib\\request.py\", line 1347, in do_open\n[SERVER-ERR]     raise URLError(err)\n[SERVER-ERR] urllib.error.URLError: <urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n    raise err\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n    raise new_e\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n    self._validate_conn(conn)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n    conn.connect()\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n    self.sock = sock = self._new_conn()\n                       ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 207, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000002A4901A2CC0>, 'Connection to huggingface.co timed out. (connect timeout=10)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/OmniParser-v2.0/resolve/main/icon_detect/train_args.yaml (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002A4901A2CC0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1376, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1296, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 280, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 303, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 688, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/OmniParser-v2.0/resolve/main/icon_detect/train_args.yaml (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002A4901A2CC0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 8d67034e-37bd-43f0-9918-44c16caff6c7)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Scripts\\huggingface-cli.exe\\__main__.py\", line 7, in <module>\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\huggingface_cli.py\", line 57, in main\n    service.run()\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\download.py\", line 153, in run\n    print(self._download())  # Print path to downloaded files\n          ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\commands\\download.py\", line 166, in _download\n    return hf_hub_download(\n           ^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 842, in hf_hub_download\n    return _hf_hub_download_to_local_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1091, in _hf_hub_download_to_local_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1489, in _raise_on_head_call_error\n    raise LocalEntryNotFoundError(\nhuggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n下载失败: icon_detect/train_args.yaml，已达到最大重试次数\nTraceback (most recent call last):\n  File \"D:\\autoMate\\autoMate-master\\main.py\", line 78, in <module>\n    run()\n  File \"D:\\autoMate\\autoMate-master\\main.py\", line 44, in run\n    download_weights.download()\n  File \"D:\\autoMate\\autoMate-master\\util\\download_weights.py\", line 45, in download\n    subprocess.run(cmd, check=True)\n  File \"D:\\Users\\admin\\anaconda3\\envs\\automate\\Lib\\subprocess.py\", line 571, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['huggingface-cli', 'download', 'microsoft/OmniParser-v2.0', 'icon_detect/train_args.yaml', '--local-dir', 'weights']' returned non-zero exit status 1.",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "这是模型文件下载失败了，我把模型文件放百度网盘了，可以直接下载：\n\n链接: https://pan.baidu.com/s/1Tj8sZZK9_QI7whZV93vb0w?pwd=dyeu 提取码: dyeu"
      },
      {
        "user": "damaomaom",
        "body": "(automate) D:\\autoMate\\autoMate-master>python main.py\ncuda is_available:  False\nMPS is_available:  False\ncuda device_count 0\n显卡驱动不适配，请根据readme安装合适版本的 torch！\n已经检测到模型文件！\n启动Omniserver服务中，因为加载模型真的超级慢，请耐心等待！\n等待服务启动...\n[SERVER-ERR] [2025-03-12 15:16:04,245] [ WARNING] easyocr.py:80 - Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n等待服务启动...\n等待服务启动...\n等待服务启动...\n等待服务启动...\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n[SERVER-ERR]     sock = connection.create_connection(\n[SERVER-ERR]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n[SERVER-ERR]     raise err\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n[SERVER-ERR]     sock.connect(sa)\n[SERVER-ERR] TimeoutError: timed out\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n[SERVER-ERR]     response = self._make_request(\n[SERVER-ERR]                ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n[SERVER-ERR]     raise new_e\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n[SERVER-ERR]     self._validate_conn(conn)\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n[SERVER-ERR]     conn.connect()\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n[SERVER-ERR]     self.sock = sock = self._new_conn()\n[SERVER-ERR]                        ^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connection.py\", line 207, in _new_conn\n[SERVER-ERR]     raise ConnectTimeoutError(\n[SERVER-ERR] urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)')\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n[SERVER-ERR]     resp = conn.urlopen(\n[SERVER-ERR]            ^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n[SERVER-ERR]     retries = retries.increment(\n[SERVER-ERR]               ^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n[SERVER-ERR]     raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n[SERVER-ERR]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR] urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\n[SERVER-ERR]\n[SERVER-ERR] During handling of the above exception, another exception occurred:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1376, in _get_metadata_or_catch_error\n[SERVER-ERR]     metadata = get_hf_file_metadata(\n[SERVER-ERR]                ^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n[SERVER-ERR]     return fn(*args, **kwargs)\n[SERVER-ERR]            ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1296, in get_hf_file_metadata\n[SERVER-ERR]     r = _request_wrapper(\n[SERVER-ERR]         ^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 280, in _request_wrapper\n[SERVER-ERR]     response = _request_wrapper(\n[SERVER-ERR]                ^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 303, in _request_wrapper\n[SERVER-ERR]     response = get_session().request(method=method, url=url, **params)\n[SERVER-ERR]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n[SERVER-ERR]     resp = self.send(prep, **send_kwargs)\n[SERVER-ERR]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n[SERVER-ERR]     r = adapter.send(request, **kwargs)\n[SERVER-ERR]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n[SERVER-ERR]     return super().send(request, *args, **kwargs)\n[SERVER-ERR]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 688, in send\n[SERVER-ERR]     raise ConnectTimeout(e, request=request)\n[SERVER-ERR] requests.exceptions.ConnectTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/Florence-2-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D549783770>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 60ba7c86-ef24-4a9f-9382-58851c6f21b3)')\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 342, in cached_file\n[SERVER-ERR]     resolved_file = hf_hub_download(\n[SERVER-ERR]                     ^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n[SERVER-ERR]     return fn(*args, **kwargs)\n[SERVER-ERR]            ^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 862, in hf_hub_download\n[SERVER-ERR]     return _hf_hub_download_to_cache_dir(\n[SERVER-ERR]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n[SERVER-ERR]     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1489, in _raise_on_head_call_error\n[SERVER-ERR]     raise LocalEntryNotFoundError(\n[SERVER-ERR] huggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n[SERVER-ERR]\n[SERVER-ERR] The above exception was the direct cause of the following exception:\n[SERVER-ERR]\n[SERVER-ERR] Traceback (most recent call last):\n[SERVER-ERR]   File \"D:\\autoMate\\autoMate-master\\omniserver.py\", line 32, in <module>\n[SERVER-ERR]     omniparser = Omniparser(config)\n[SERVER-ERR]                  ^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\autoMate\\autoMate-master\\util\\omniparser.py\", line 12, in __init__\n[SERVER-ERR]     self.caption_model_processor = get_caption_model_processor(model_name=config['caption_model_name'], model_name_or_path=config['caption_model_path'], device=device)\n[SERVER-ERR]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\autoMate\\autoMate-master\\util\\utils.py\", line 63, in get_caption_model_processor\n[SERVER-ERR]     processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n[SERVER-ERR]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py\", line 305, in from_pretrained\n[SERVER-ERR]     config = AutoConfig.from_pretrained(\n[SERVER-ERR]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1075, in from_pretrained\n[SERVER-ERR]     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n[SERVER-ERR]                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 594, in get_config_dict\n[SERVER-ERR]     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n[SERVER-ERR]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 653, in _get_config_dict\n[SERVER-ERR]     resolved_config_file = cached_file(\n[SERVER-ERR]                            ^^^^^^^^^^^^\n[SERVER-ERR]   File \"D:\\anaconda3\\envs\\automate\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n[SERVER-ERR]     raise EnvironmentError(\n[SERVER-ERR] OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like microsoft/Florence-2-base is not the path to a directory containing a file named config.json.\n[SERVER-ERR] Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\nTraceback (most recent call last):\n  File \"D:\\autoMate\\autoMate-master\\main.py\", line 78, in <module>\n    run()\n  File \"D:\\autoMate\\autoMate-master\\main.py\", line 55, in run\n    raise RuntimeError(f\"服务器进程报错退出：{server_process.returncode}\")\nRuntimeError: 服务器进程报错退出：1"
      },
      {
        "user": "yuruotong1",
        "body": "刚才重构了代码，已经取消了服务，可以再试一下 @damaomaom "
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 78,
    "title": "请问为何会报10054的？我配置的是deepseek模型",
    "author": "jim-cuizhikun",
    "state": "closed",
    "created_at": "2025-03-12T03:38:18Z",
    "updated_at": "2025-03-13T11:06:15Z",
    "labels": [],
    "body": "E:\\PycharmProjects\\autoMate-master\\.venv\\Scripts\\python.exe E:\\PycharmProjects\\autoMate-master\\main.py \ncuda is_available:  False\nMPS is_available:  False\ncuda device_count 0\n显卡驱动不适配，请根据readme安装合适版本的 torch！\n已经检测到模型文件！\n启动Omniserver服务中，因为加载模型真的超级慢，请耐心等待！\n等待服务启动...\n等待服务启动...\n[SERVER-ERR] [2025-03-12 11:30:41,236] [ WARNING] easyocr.py:80 - Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n等待服务启动...\n等待服务启动...\n等待服务启动...\n等待服务启动...\n等待服务启动...\n等待服务启动...\n[SERVER-ERR] Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 馃憠v4.50馃憟 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n[SERVER-ERR]   - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n[SERVER-ERR]   - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n[SERVER-ERR]   - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n[SERVER-OUT] Server initialized!\n[SERVER-ERR] INFO:     Will watch for changes in these directories: ['E:\\\\PycharmProjects\\\\autoMate-master']\n[SERVER-ERR] INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n[SERVER-ERR] INFO:     Started reloader process [16728] using StatReload\n等待服务启动...\n[SERVER-ERR] [2025-03-12 11:32:05,977] [ WARNING] easyocr.py:80 - Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n等待服务启动...\n[SERVER-ERR] Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 馃憠v4.50馃憟 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n[SERVER-ERR]   - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n[SERVER-ERR]   - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n[SERVER-ERR]   - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n[SERVER-OUT] Server initialized!\n等待服务启动...\n[SERVER-OUT] Server initialized!\n[SERVER-ERR] INFO:     Started server process [16964]\n[SERVER-ERR] INFO:     Waiting for application startup.\n[SERVER-ERR] INFO:     Application startup complete.\n[SERVER-OUT] INFO:     127.0.0.1:54932 - \"GET /probe/ HTTP/1.1\" 200 OK\nOmniparser服务启动成功...\nE:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  warnings.warn(\n* Running on local URL:  http://0.0.0.0:7888\n\nTo create a public link, set `share=True` in `launch()`.\nin sampling_loop_sync, model: deepseek-chat\nscreen size: 2880, 1800\nStart the message loop. User messages: [{'role': <Sender.USER: 'user'>, 'content': [TextBlock(citations=None, text='打开edge浏览器', type='text')]}]\n[SERVER-OUT] start parsing...\n[SERVER-OUT] image size: (2880, 1800)\n[SERVER-OUT] \n[SERVER-OUT] 0: 800x1280 72 icons, 1250.1ms\n[SERVER-OUT] Speed: 69.8ms preprocess, 1250.1ms inference, 28.2ms postprocess per image at shape (1, 3, 800, 1280)\n[SERVER-OUT] len(filtered_boxes): 91 38\nTraceback (most recent call last):\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n    response = conn.getresponse()\n               ^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n    httplib_response = super().getresponse()\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 1430, in getresponse\n    response.begin()\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 331, in begin\n    version, status, reason = self._read_status()\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 292, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\socket.py\", line 720, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\util\\util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n    response = conn.getresponse()\n               ^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n    httplib_response = super().getresponse()\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 1430, in getresponse\n    response.begin()\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 331, in begin\n    version, status, reason = self._read_status()\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 292, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\socket.py\", line 720, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n    response = await route_utils.call_process_api(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2044, in process_api\n    result = await self.call_function(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1603, in call_function\n    prediction = await utils.async_iteration(iterator)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n    return await anext(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 722, in __anext__\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 705, in run_sync_iterator_async\n    return next(iterator)\n           ^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 866, in gen_wrapper\n    response = next(iterator)\n               ^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\gradio_ui\\app.py\", line 176, in process_input\n    for loop_msg in sampling_loop_sync(\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\gradio_ui\\loop.py\", line 59, in sampling_loop_sync\n    parsed_screen = omniparser_client()\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\gradio_ui\\agent\\llm_utils\\omniparserclient.py\", line 18, in __call__\n    response = requests.post(self.url, json={\"base64_image\": image_base64})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\api.py\", line 115, in post\n    return request(\"post\", url, data=data, json=json, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\adapters.py\", line 682, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "目前 deepseek 在多轮对话中还有些问题，我们最近会推一个新版本，新版本会先支持 openai，考虑到国内大家访问不了 openai， 我们也会推荐一个不用翻墙也能用的代理站"
      },
      {
        "user": "jim-cuizhikun",
        "body": "首先，非常感谢您的回复，期待新版本的发布！\n其次，我通过VPN可以访问openai，通过OmniTool配置了gpt-4o，依然报错10054。而且我也没有多次对话，只是发出一个指令，例如“你能介绍一下吗？”，就10054了。\n\nin sampling_loop_sync, model: gpt-4o\nscreen size: 2880, 1800\nStart the message loop. User messages: [{'role': <Sender.USER: 'user'>, 'content': [TextBlock(citations=None, text='介绍一下？', type='text')]}]\n[SERVER-OUT] start parsing...\n[SERVER-OUT] image size: (2880, 1800)\n[SERVER-OUT] \n[SERVER-OUT] 0: 800x1280 71 icons, 1902.2ms\n[SERVER-OUT] Speed: 77.4ms preprocess, 1902.2ms inference, 36.1ms postprocess per image at shape (1, 3, 800, 1280)\n[SERVER-OUT] len(filtered_boxes): 89 41\nTraceback (most recent call last):\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n    response = conn.getresponse()\n               ^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n    httplib_response = super().getresponse()\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 1430, in getresponse\n    response.begin()\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 331, in begin\n    version, status, reason = self._read_status()\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 292, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\socket.py\", line 720, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\util\\util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n    response = conn.getresponse()\n               ^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n    httplib_response = super().getresponse()\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 1430, in getresponse\n    response.begin()\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 331, in begin\n    version, status, reason = self._read_status()\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\http\\client.py\", line 292, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\py312\\Lib\\socket.py\", line 720, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n    response = await route_utils.call_process_api(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2044, in process_api\n    result = await self.call_function(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1603, in call_function\n    prediction = await utils.async_iteration(iterator)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n    return await anext(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 722, in __anext__\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 705, in run_sync_iterator_async\n    return next(iterator)\n           ^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 866, in gen_wrapper\n    response = next(iterator)\n               ^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\gradio_ui\\app.py\", line 176, in process_input\n    for loop_msg in sampling_loop_sync(\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\gradio_ui\\loop.py\", line 59, in sampling_loop_sync\n    parsed_screen = omniparser_client()\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\gradio_ui\\agent\\llm_utils\\omniparserclient.py\", line 18, in __call__\n    response = requests.post(self.url, json={\"base64_image\": image_base64})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\api.py\", line 115, in post\n    return request(\"post\", url, data=data, json=json, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\PycharmProjects\\autoMate-master\\.venv\\Lib\\site-packages\\requests\\adapters.py\", line 682, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))\n"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 68,
    "title": "autoMate>pip install -r requirements.txt rise error",
    "author": "paulownia99",
    "state": "closed",
    "created_at": "2025-03-10T06:54:05Z",
    "updated_at": "2025-03-13T09:32:40Z",
    "labels": [],
    "body": "  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [21 lines of output]\n      + C:\\Users\\FCMA\\.conda\\envs\\automate\\python.exe C:\\Users\\FCMA\\AppData\\Local\\Temp\\pip-install-z02mouyb\\numpy_a5ffa25f14bc4620850f07394f64c346\\vendored-meson\\meson\\meson.py setup C:\\Users\\FCMA\\AppData\\Local\\Temp\\pip-install-z02mouyb\\numpy_a5ffa25f14bc4620850f07394f64c346 C:\\Users\\FCMA\\AppData\\Local\\Temp\\pip-install-z02mouyb\\numpy_a5ffa25f14bc4620850f07394f64c346\\.mesonpy-p_meg130 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\FCMA\\AppData\\Local\\Temp\\pip-install-z02mouyb\\numpy_a5ffa25f14bc4620850f07394f64c346\\.mesonpy-p_meg130\\meson-python-native-file.ini\n      The Meson build system\n      Version: 1.2.99\n      Source dir: C:\\Users\\FCMA\\AppData\\Local\\Temp\\pip-install-z02mouyb\\numpy_a5ffa25f14bc4620850f07394f64c346\n      Build dir: C:\\Users\\FCMA\\AppData\\Local\\Temp\\pip-install-z02mouyb\\numpy_a5ffa25f14bc4620850f07394f64c346\\.mesonpy-p_meg130\n      Build type: native build\n      Project name: NumPy\n      Project version: 1.26.4\n      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n\n      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n      The following exception(s) were encountered:\n      Running `icl \"\"` gave \"[WinError 2] 系统找不到指定的文件。\"\n      Running `cl /?` gave \"[WinError 2] 系统找不到指定的文件。\"\n      Running `cc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n      Running `gcc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n      Running `clang --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n      Running `clang-cl /?` gave \"[WinError 2] 系统找不到指定的文件。\"\n      Running `pgcc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n\n      A full log can be found at C:\\Users\\FCMA\\AppData\\Local\\Temp\\pip-install-z02mouyb\\numpy_a5ffa25f14bc4620850f07394f64c346\\.mesonpy-p_meg130\\meson-logs\\meson-log.txt\n\nwhy vs?",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "You're encountering a compilation error when trying to install NumPy. The core problem is that the installation process cannot find any compatible C compiler (icl, cl, cc, gcc, clang, etc.) required to build NumPy's components.\n\nSince you appear to be using Anaconda, the simplest solution is to install NumPy using conda:\n\n```bash\nconda install numpy\n```"
      },
      {
        "user": "Zhanghao-19970918",
        "body": "直接用conda安装吧"
      },
      {
        "user": "yuruotong1",
        "body": "The code has been updated, you can try it again."
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 69,
    "title": "Illegal instruction",
    "author": "chan-yuu",
    "state": "closed",
    "created_at": "2025-03-10T18:47:39Z",
    "updated_at": "2025-03-13T09:31:00Z",
    "labels": [],
    "body": "```\nStart the message loop. User messages: [{'role': <Sender.USER: 'user'>, 'content': [TextBlock(citations=None, text='帮我在主目录中创建一个python文件', type='text')]}]\n[SERVER-OUT] start parsing...\n[SERVER-OUT] image size: (5120, 1600)\n[SERVER-OUT] \n[SERVER-OUT] 0: 416x1280 300 icons, 25.9ms\n[SERVER-OUT] Speed: 2.2ms preprocess, 25.9ms inference, 153.2ms postprocess per image at shape (1, 3, 416, 1280)\n[SERVER-OUT] len(filtered_boxes): 739 553\n[SERVER-OUT] time to get parsed content: 0.5575861930847168\n[SERVER-OUT] time: 7.074315309524536\n[SERVER-OUT] INFO:     127.0.0.1:47630 - \"POST /parse/ HTTP/1.1\" 200 OK\nomniparser latency: 7.074315309524536\n_render_message: -- Step 1: --\nError, llm response: b''\nTraceback (most recent call last):\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/requests/models.py\", line 974, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/queueing.py\", line 715, in process_events\n    response = await route_utils.call_process_api(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/blocks.py\", line 2044, in process_api\n    result = await self.call_function(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/blocks.py\", line 1603, in call_function\n    prediction = await utils.async_iteration(iterator)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/utils.py\", line 728, in async_iteration\n    return await anext(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/utils.py\", line 722, in __anext__\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/anyio/to_thread.py\", line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/utils.py\", line 705, in run_sync_iterator_async\n    return next(iterator)\n           ^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/utils.py\", line 866, in gen_wrapper\n    response = next(iterator)\n               ^^^^^^^^^^^^^^\n  File \"/home/cyun/agent_version_check/Agent/autoMate/gradio_ui/app.py\", line 176, in process_input\n    for loop_msg in sampling_loop_sync(\n  File \"/home/cyun/agent_version_check/Agent/autoMate/gradio_ui/loop.py\", line 60, in sampling_loop_sync\n    tools_use_needed, vlm_response_json = actor(messages=messages, parsed_screen=parsed_screen)\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/agent_version_check/Agent/autoMate/gradio_ui/agent/vlm_agent.py\", line 76, in __call__\n    vlm_response, token_usage = run_oai_interleaved(\n                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/cyun/agent_version_check/Agent/autoMate/gradio_ui/agent/llm_utils/oaiclient.py\", line 65, in run_oai_interleaved\n    return response.json()\n           ^^^^^^^^^^^^^^^\n  File \"/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/requests/models.py\", line 978, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n/home/cyun/anaconda3/envs/automate/lib/python3.12/site-packages/gradio/blocks.py:1777: UserWarning: A function (stop_app) returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n    Output components:\n        []\n    Output values returned:\n        [\"App stopped\"]\n  warnings.warn(\n```\n请问这是什么问题",
    "comments": [
      {
        "user": "chan-yuu",
        "body": "![Image](https://github.com/user-attachments/assets/f496e50a-56ca-44aa-8203-552ada6e2ec9)"
      },
      {
        "user": "yuruotong1",
        "body": "看报错，大模型没有返回内容。感觉是大模型那边的问题"
      },
      {
        "user": "yuruotong1",
        "body": "今天重构了代码，可以再试一下"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 74,
    "title": "Model Support Open router",
    "author": "bhupesh-sf",
    "state": "closed",
    "created_at": "2025-03-11T12:39:54Z",
    "updated_at": "2025-03-13T09:30:08Z",
    "labels": [],
    "body": "Hey, I just went through the model support file and it lists only deepseek & alibaba, I already have open router can't we use that and also what about local models??",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "Thanks for introducing a great tool like open router, the readme model list needs to be updated. Because our current code (refactored version) uses structured output and multimodal support features, according to our tests, Currently better support is the openai model. So the refactored version (expected in the next week) will give priority to openai's 4o model, We want to wait for other models to update this capability before we become compatible."
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 75,
    "title": "what is the hardware requirements ?",
    "author": "cn-knight",
    "state": "closed",
    "created_at": "2025-03-11T13:02:56Z",
    "updated_at": "2025-03-13T09:29:20Z",
    "labels": [],
    "body": "GPU 4G OK ?",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "Of course, my computer is 4g of video memory, marking a picture average of about 10s, However, it's important to note that the cuda version is compatible with the torch, which I've covered in readme."
      },
      {
        "user": "cn-knight",
        "body": "> Of course, my computer is 4g of video memory, marking a picture average of about 10s, However, it's important to note that the cuda version is compatible with the torch, which I've covered in readme.\n\nmany thanks"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 76,
    "title": "website config",
    "author": "sharkskin",
    "state": "closed",
    "created_at": "2025-03-11T15:06:03Z",
    "updated_at": "2025-03-13T09:29:13Z",
    "labels": [],
    "body": "How to enable external access? Where to set \"share=true\"？\n\nThank you!",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "You can try adding this to the startup code:\ndemo.launch(server_name=\"0.0.0.0\", server_port=7888, share=True)\n\n"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 85,
    "title": "default base url",
    "author": "cn-knight",
    "state": "closed",
    "created_at": "2025-03-13T08:26:35Z",
    "updated_at": "2025-03-13T09:24:54Z",
    "labels": [],
    "body": "虽然说以自己的placeholder为准，那个默认值\"https://api.openai-next.com/v1\"为什么不用正确的呢，这个url测不通openai，改成https://api.openai.com/v1才通了\n\n\n                with gr.Column():\n                    base_url = gr.Textbox(\n                        label=\"Base URL\",\n                        value=\"https://api.openai-next.com/v1\",\n                        placeholder=\"输入基础 URL\",\n                        interactive=True",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "这个是代理站的地址，国内大多数人无法用openai的，所以我放了代理站。不过你这是非常好的建议，我准备改回去了"
      },
      {
        "user": "yuruotong1",
        "body": "代码已经修改了，辛苦更新一下"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 84,
    "title": "这个功能能在linux上玩吗",
    "author": "llk2014",
    "state": "closed",
    "created_at": "2025-03-13T06:21:16Z",
    "updated_at": "2025-03-13T07:59:35Z",
    "labels": [],
    "body": "如题",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "目前在windows和mac上测试过都是可以的，我估计 linux 应该是没问题"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 83,
    "title": "服务启动报错",
    "author": "mrzzao",
    "state": "closed",
    "created_at": "2025-03-12T10:22:43Z",
    "updated_at": "2025-03-13T05:53:53Z",
    "labels": [],
    "body": "python main.py \ncuda is_available:  True\nMPS is_available:  False\ncuda device_count 1\ncuda device_name NVIDIA GeForce RTX 4090\n已经检测到模型文件！\n启动Omniserver服务中，因为加载模型真的超级慢，请耐心等待！\n等待服务启动...\n\n[SERVER-OUT] --------------------------------------\n[SERVER-OUT] 0   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\n[SERVER-OUT] 1   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\n[SERVER-OUT] 2   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)\n[SERVER-OUT] 3   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)\n[SERVER-OUT] 4   paddle::AnalysisPredictor::OptimizeInferenceProgram()\n[SERVER-OUT] 5   paddle::inference::analysis::Analyzer::RunAnalysis(paddle::inference::analysis::Argument*)\n[SERVER-OUT] 6   paddle::inference::analysis::IrAnalysisPass::RunImpl(paddle::inference::analysis::Argument*)\n[SERVER-OUT] 7   paddle::inference::analysis::IRPassManager::Apply(std::unique_ptr<paddle::framework::ir::Graph, std::default_delete<paddle::framework::ir::Graph> >)\n[SERVER-OUT] 8   paddle::framework::ir::Pass::Apply(paddle::framework::ir::Graph*) const\n[SERVER-OUT] 9   paddle::framework::ir::SelfAttentionFusePass::ApplyImpl(paddle::framework::ir::Graph*) const\n[SERVER-OUT] 10  paddle::framework::ir::GraphPatternDetector::operator()(paddle::framework::ir::Graph*, std::function<void (std::map<paddle::framework::ir::PDNode*, paddle::framework::ir::Node*, paddle::framework::ir::GraphPatternDetector::PDNodeCompare, std::allocator<std::pair<paddle::framework::ir::PDNode* const, paddle::framework::ir::Node*> > > const&, paddle::framework::ir::Graph*)>)\n[SERVER-OUT] \n[SERVER-OUT] ----------------------\n[SERVER-OUT] Error Message Summary:\n[SERVER-OUT] ----------------------\n[SERVER-OUT] FatalError: `Illegal instruction` is detected by the operating system.\n[SERVER-OUT]   [TimeInfo: *** Aborted at 1741774642 (unix time) try \"date -d @1741774642\" if you are using GNU date ***]\n[SERVER-OUT]   [SignalInfo: *** SIGILL (@0x7f8ef188860a) received by PID 17822 (TID 0x7f90047e8740) from PID 18446744073466840586 ***]\n[SERVER-OUT] \nTraceback (most recent call last):\n  File \"/root/autoMate/main.py\", line 79, in <module>\n    run()\n  File \"/root/autoMate/main.py\", line 56, in run\n    raise RuntimeError(f\"服务器进程报错退出：{server_process.returncode}\")\nRuntimeError: 服务器进程报错退出：-4\n\n显卡：4090\n驱动版本：565.57.01\ncuda版本：12.2\n系统：22.04.5 LTS\n内核版本：5.15.0-134-generic\n由于机器没法联网，调整了脚本模型文件是在魔塔下载的，其他的没变。",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "我准备把服务删了，这个服务好多人报错"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 64,
    "title": "执行报错",
    "author": "hope-ghost",
    "state": "closed",
    "created_at": "2025-03-08T03:14:40Z",
    "updated_at": "2025-03-11T07:45:43Z",
    "labels": [],
    "body": "填入项：\nModel：deepseek-chat  \nBase URL：https://api.deepseek.com/v1\nN most recent screenshots：2\nAPI Key：已填，且是从官网获取\n测试语句：帮我打开B站并刷几个科技向视频，并一键3连\n网络情况：无VPN，使用国内网络\napi连接情况：使用python已通过官网的使用案例，连接正常\n报错：\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\models.py\", line 974, in json\n    return complexjson.loads(self.text, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n    response = await route_utils.call_process_api(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\gradio\\blocks.py\", line 2044, in process_api\n    result = await self.call_function(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\gradio\\blocks.py\", line 1603, in call_function\n    prediction = await utils.async_iteration(iterator)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n    return await anext(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\gradio\\utils.py\", line 722, in __anext__\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\gradio\\utils.py\", line 705, in run_sync_iterator_async\n    return next(iterator)\n           ^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\gradio\\utils.py\", line 866, in gen_wrapper\n    response = next(iterator)\n               ^^^^^^^^^^^^^^\n  File \"E:\\AI\\autoMate\\gradio_ui\\app.py\", line 176, in process_input\n    for loop_msg in sampling_loop_sync(\n  File \"E:\\AI\\autoMate\\gradio_ui\\loop.py\", line 60, in sampling_loop_sync\n    tools_use_needed, vlm_response_json = actor(messages=messages, parsed_screen=parsed_screen)\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\AI\\autoMate\\gradio_ui\\agent\\vlm_agent.py\", line 79, in __call__\n    vlm_response, token_usage = run_oai_interleaved(\n                                ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\AI\\autoMate\\gradio_ui\\agent\\llm_utils\\oaiclient.py\", line 61, in run_oai_interleaved\n    print(f\"Error in interleaved openAI: {e}. This may due to your invalid API key. Please check the response: {response.json()} \")\n                                                                                                                ^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\models.py\", line 978, in json\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
    "comments": [
      {
        "user": "Gushroom",
        "body": "报错之前的log有吗？这个问题大概率是LLM返回的答案没有符合格式化要求"
      },
      {
        "user": "hope-ghost",
        "body": "不好意思，似乎并没有生成log文件"
      },
      {
        "user": "Gushroom",
        "body": "不是log文件 terminal会写当前步骤 看一下在报错信息之前是不是已经收到了Deepseek的回复？"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 67,
    "title": "服务器进程报错退出，退出代码：1",
    "author": "YXuan66",
    "state": "closed",
    "created_at": "2025-03-10T05:42:07Z",
    "updated_at": "2025-03-10T06:03:17Z",
    "labels": [],
    "body": "[SERVER-OUT] Progress: |--------------------------------------------------| 2.0% Complete\n[SERVER-OUT] Progress: |--------------------------------------------------| 2.0% Complete\n[SERVER-OUT] Progress: |--------------------------------------------------| 2.0% Complete\n[SERVER-OUT] Progress: |--------------------------------------------------| 2.0% Complete等待服务启动...\nTraceback (most recent call last):\n  File \"C:\\Users\\Administrator\\Desktop\\autoMate\\main.py\", line 89, in <module>\n    run()\n  File \"C:\\Users\\Administrator\\Desktop\\autoMate\\main.py\", line 54, in run\n    raise RuntimeError(f\"服务器进程报错退出，退出代码：{server_process.returncode}\\n错误信息：{stderr_output}\")\nRuntimeError: 服务器进程报错退出，退出代码：1\n错误信息：\nI have changed some code to detect the error but failed",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 63,
    "title": "求大佬做个一键包~",
    "author": "aimarxjg",
    "state": "closed",
    "created_at": "2025-03-08T02:07:11Z",
    "updated_at": "2025-03-09T01:16:07Z",
    "labels": [
      "enhancement"
    ],
    "body": "免去部署的麻烦，谢谢",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "收到，我们暂时先专注于 bug fix，后面会考虑推出一键部署包"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 54,
    "title": "需要的电脑配置要求高吗？",
    "author": "chenchenno11",
    "state": "closed",
    "created_at": "2025-03-06T10:58:52Z",
    "updated_at": "2025-03-08T06:06:32Z",
    "labels": [],
    "body": "我想在虚拟机上运行，但是怕不行啊，最低配置要求有吗",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "有个 2g 显存的显卡就能跑，其他没啥要求"
      },
      {
        "user": "chenchenno11",
        "body": "可以用这个解决cf的turnslite问题吗，折腾了好几天没解决，发现这个项目可能是个方法。"
      },
      {
        "user": "yuruotong1",
        "body": "> 可以用这个解决cf的turnslite问题吗，折腾了好几天没解决，发现这个项目可能是个方法。\n\n请问turnslite是什么问题呀"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 56,
    "title": "在打印出”启动Omniserver服务中，约40s左右，请耐心等待！“后报错",
    "author": "hope-ghost",
    "state": "closed",
    "created_at": "2025-03-07T01:10:24Z",
    "updated_at": "2025-03-08T06:04:58Z",
    "labels": [],
    "body": "在打印出”启动Omniserver服务中，约40s左右，请耐心等待！“后报错：\nTraceback (most recent call last):\n  File \"E:\\AI\\autoMate\\main.py\", line 47, in <module>\n    run()\n  File \"E:\\AI\\autoMate\\main.py\", line 38, in run\n    raise RuntimeError(\"Server process terminated unexpectedly\")\nRuntimeError: Server process terminated unexpectedly\n已检查端口占用，已重装过pytorch，已重配置过requirement.txt，问题没有解决",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "请更新一下最新的版本，刚才推送了！"
      },
      {
        "user": "hope-ghost",
        "body": "> 请更新一下最新的版本，刚才推送了！\n\n现在变成了Traceback (most recent call last):\n  File \"E:\\AI\\autoMate\\main.py\", line 74, in <module>\n    run()\n  File \"E:\\AI\\autoMate\\main.py\", line 34, in run\n    res = requests.get(\"http://127.0.0.1:8000/probe/\")\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\api.py\", line 73, in get\n    return request(\"get\", url, params=params, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /probe/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001C099733020>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n已尝试手段：\nVPN已关\n注册表中Internet setting中的ProxyEnable修改为0\n防火墙已关\n已检查端口占用\n问题没有解决"
      },
      {
        "user": "yuruotong1",
        "body": "> > 请更新一下最新的版本，刚才推送了！\n> \n> 现在变成了Traceback (most recent call last): File \"E:\\AI\\autoMate\\main.py\", line 74, in run() File \"E:\\AI\\autoMate\\main.py\", line 34, in run res = requests.get(\"http://127.0.0.1:8000/probe/\") ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\api.py\", line 73, in get return request(\"get\", url, params=params, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\api.py\", line 59, in request return session.request(method=method, url=url, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request resp = self.send(prep, **send_kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send r = adapter.send(request, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"E:\\Anaconda3\\envs\\automate\\Lib\\site-packages\\requests\\adapters.py\", line 700, in send raise ConnectionError(e, request=request) requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /probe/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001C099733020>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')) 已尝试手段： VPN已关 注册表中Internet setting中的ProxyEnable修改为0 防火墙已关 已检查端口占用 问题没有解决\n\n了解了，这个问题刚刚修复了，辛苦再更新一下代码"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 10,
    "title": "👑 [需求]加入连接不同代码块的功能。",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T23:35:19Z",
    "updated_at": "2025-03-03T09:41:25Z",
    "labels": [
      "enhancement"
    ],
    "body": "rt",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 11,
    "title": "👑 [需求]建立共享代码功能，可搜索、下载、更新其他人共享的代码块",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T23:36:55Z",
    "updated_at": "2025-03-03T09:41:19Z",
    "labels": [
      "enhancement"
    ],
    "body": "rt",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 12,
    "title": "👑 [需求]除第一次生成的代码外，无法自动更新代码块，在智子对话框中加入更新按钮或者自动更新！",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T23:46:53Z",
    "updated_at": "2025-03-03T09:41:13Z",
    "labels": [
      "enhancement"
    ],
    "body": "软件第一次是自动把代码放入代码区，后面修改的代码它不会自动更新到代码区，所以能自动更新，或能手动更新修改后的代码到代码区，效率会高好多。",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 15,
    "title": "👑 [需求]python执行器支持用户主动扩展第三方依赖",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-09T13:35:25Z",
    "updated_at": "2025-03-03T09:41:08Z",
    "labels": [
      "enhancement"
    ],
    "body": "rt",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 17,
    "title": "👑 [需求]文档中增加国产大模型的使用示范",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T10:20:58Z",
    "updated_at": "2025-03-03T09:41:03Z",
    "labels": [
      "documentation"
    ],
    "body": "<img width=\"582\" alt=\"68097a4d60a2c96b0bfe7d535cf6560c_\" src=\"https://github.com/yuruotong1/autoMate/assets/31992251/52a0e89e-edd8-4ab0-b670-dfc303c72574\">",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 16,
    "title": "👑 [需求]搜索界面“点击配置“修改为“点击配置 AI API”",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T10:20:04Z",
    "updated_at": "2025-03-03T09:40:58Z",
    "labels": [
      "enhancement"
    ],
    "body": "<img width=\"626\" alt=\"cf7c320eb7523bea0b2b7fc4291d842f_\" src=\"https://github.com/yuruotong1/autoMate/assets/31992251/e65f51d8-74a1-4e47-84a1-7b0aec6f68e3\">",
    "comments": [
      {
        "user": "HelloBojack",
        "body": "@yuruotong1 https://github.com/yuruotong1/autoMate/pull/39 Thank you for merge! This issue can close~"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 21,
    "title": "👑 [需求]飞书文档中增加Ollama的使用指南",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T11:37:58Z",
    "updated_at": "2025-03-03T09:40:50Z",
    "labels": [
      "documentation"
    ],
    "body": "rt",
    "comments": [
      {
        "user": "Lvan826199",
        "body": "文档已添加：https://o0h3vqpeoxs.feishu.cn/docx/QMmYdhjvood5wpxDWcscGVWYnCQ"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 22,
    "title": "👑 [需求]加入日志体系，可以在页面导出日志，方便排查错误",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T13:30:39Z",
    "updated_at": "2025-03-03T09:40:39Z",
    "labels": [
      "enhancement"
    ],
    "body": "rt",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 23,
    "title": "👑 [需求]自动更新的包使用github不稳定，改到gitee或者其他仓库 ",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T15:09:52Z",
    "updated_at": "2025-03-03T09:40:34Z",
    "labels": [
      "enhancement"
    ],
    "body": "![947f5d911b63c31788aa33fc9795fd8](https://github.com/yuruotong1/autoMate/assets/31992251/992772a5-1cc3-4ac6-ae0c-8fa9d4367c6d)",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 26,
    "title": "👑 [Feature Request]从 Flask 迁移到 FastAPI",
    "author": "linkedlist771",
    "state": "closed",
    "created_at": "2024-07-11T05:18:33Z",
    "updated_at": "2025-03-03T09:40:27Z",
    "labels": [
      "enhancement"
    ],
    "body": "\r\n\r\n### 🥰 需求描述\r\n\r\n将现有的 Flask 后端迁移到 FastAPI 框架。这个变更的目的是利用 FastAPI 的优势,如:\r\n- 更快的性能\r\n- 自动 API 文档生成\r\n- 更强大的类型提示和验证\r\n- 异步支持\r\n- 现代化的 Python 语法\r\n\r\n### 🧐 解决方案\r\n\r\n1. 评估当前 Flask 应用结构\r\n2. 设计 FastAPI 迁移计划\r\n3. 逐步将 Flask 路由转换为 FastAPI 路径操作\r\n4. 利用 Pydantic 模型进行数据验证\r\n5. 实现依赖注入系统\r\n6. 配置 FastAPI 的异步特性(如果需要)\r\n7. 更新数据库连接(如果适用)\r\n8. 迁移测试套件\r\n9. 更新部署流程\r\n10. 进行性能测试和优化\r\n11. 团队协作：利用您提供的帮助，分配任务并定期同步进度\r\n\r\n```mermaid\r\ngraph TD\r\n    A[评估当前Flask应用] --> B[设计FastAPI迁移计划]\r\n    B --> C[任务分配和团队协作]\r\n    C --> D[转换路由]\r\n    D --> E[实现Pydantic模型]\r\n    E --> F[配置依赖注入]\r\n    F --> G[启用异步特性]\r\n    G --> H[更新数据库连接]\r\n    H --> I[迁移测试]\r\n    I --> J[更新部署流程]\r\n    J --> K[性能测试和优化]\r\n    K --> L[最终审查和上线]\r\n    C -.-> M[定期进度同步]\r\n    M -.-> C\r\n```\r\n\r\n### 🚑 其他信息\r\n\r\n在进行迁移时,需要注意以下几点:\r\n\r\n1. 确保团队成员熟悉 FastAPI 的概念和最佳实践。\r\n2. 考虑是否需要保持向后兼容性,或者是否可以完全重写 API。\r\n3. 评估现有的第三方扩展是否有 FastAPI 等效替代品。\r\n4. 更新 API 文档,利用 FastAPI 的自动文档生成功能。\r\n> PS: 如果maintainer需要，我可以提供帮助， 我对fastapi比较熟悉",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "但感觉迁移的话对用户影响不大"
      },
      {
        "user": "phil616",
        "body": "如果这个迁移到FastAPI的方案通过，我也可加入"
      },
      {
        "user": "yuruotong1",
        "body": "> 如果这个迁移到FastAPI的方案通过，我也可加入\r\n\r\n有几个问题需要给出答案：\r\n\r\n1. 这么做能为用户带来什么更好的体验？\r\n2. 团队成员是否更熟悉fastapi？相比于flask来说？ \r\n3. 对已有项目变更是否会引入更大的风险？\r\n"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 30,
    "title": "👑 [需求] autoMate 内置 ollama",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-13T23:37:07Z",
    "updated_at": "2025-03-03T09:40:19Z",
    "labels": [
      "enhancement"
    ],
    "body": "1. 用户打开后走默认的本地模型；\r\n2. 点击配置可更换其他模型；",
    "comments": [
      {
        "user": "mbt1909432",
        "body": "在这"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 31,
    "title": "👑 [需求]自动学习api接口信息，完成对该api接口的自动化",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-13T23:38:17Z",
    "updated_at": "2025-03-03T09:40:13Z",
    "labels": [
      "enhancement"
    ],
    "body": "rt",
    "comments": [
      {
        "user": "phil616",
        "body": "如果API符合某个规范，可以直接调用规范提出者提供的生成器，以FastAPI为例，app启动参数提供了一个生成符合OpenAPI规范的域以生成端点ID，可以用来生成[TS客户端代码](https://fastapi.tiangolo.com/advanced/generate-clients/?h=gene#generate-a-typescript-client-with-the-preprocessed-openapi)，一般都包含在文档的SDK中，只要把常见平台的SDK文档包含到项目里就可以实现自动学习API的接口信息。"
      },
      {
        "user": "yuruotong1",
        "body": "> 如果API符合某个规范，可以直接调用规范提出者提供的生成器，以FastAPI为例，app启动参数提供了一个生成符合OpenAPI规范的域以生成端点ID，可以用来生成[TS客户端代码](https://fastapi.tiangolo.com/advanced/generate-clients/?h=gene#generate-a-typescript-client-with-the-preprocessed-openapi)，一般都包含在文档的SDK中，只要把常见平台的SDK文档包含到项目里就可以实现自动学习API的接口信息。\n\n有办法集成到咱们的项目中吗？"
      },
      {
        "user": "phil616",
        "body": "FastAPI毕竟只是单一应用，各个厂家采用的标准不同，集成到该项目里面需要项目开发者团队尽可能多的去主动适配模型提供商提供的SDK及相关规范，目前团队规模无力逐个适配，建议先搁置"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 32,
    "title": "👑 [需求]界面中增加客户反馈渠道，能够在界面直接提问题和需求",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-13T23:39:54Z",
    "updated_at": "2025-03-03T09:40:03Z",
    "labels": [
      "enhancement"
    ],
    "body": "可以考虑和已有的提问工具结合，比如“我来”",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 33,
    "title": "👑 [需求]对于在线模型，只需要添加一个token",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-14T00:04:36Z",
    "updated_at": "2025-03-03T09:39:55Z",
    "labels": [
      "enhancement"
    ],
    "body": "提供一个支持的模型供应商的列表供人选择    对于在线模型，只需要添加一个token就好，token的申请肯定各家都有详细的文档，而具体的和模型的交互，完全可以后端负责去处理转换 。",
    "comments": [
      {
        "user": "fjklqq",
        "body": "可以在后端添加一个获取支持的模型入口列表的接口，返回如下信息，前端依据此信息展示配置页面的选项。\r\n同时在和后端交互（`/llm`接口）时传递`msg`、 `enter_id` 及 `parameters`的配置内容(或存入数据库，后端由数据库直接读取)，由后端自读判断使用哪一个入口完成会话。\r\n```json\r\n[\r\n    {\r\n        \"enter_id\": \"OpenaiEnterPoint\",\r\n        \"name\": \"OpenAI\",\r\n        \"parameters\": {\r\n            \"api_key\": {\r\n                \"default\": null,\r\n                \"name\": \"API KEY\",\r\n                \"required\": true,\r\n                \"typing\": \"string\"\r\n            },\r\n            \"base_url\": {\r\n                \"default\": \"https://api.openai.com/v1/engines/davinci/completions\",\r\n                \"name\": \"Base URL\",\r\n                \"required\": false,\r\n                \"typing\": \"string\"\r\n            },\r\n            \"model_name\": {\r\n                \"default\": \"gpt-3.5-turbo\",\r\n                \"name\": \"模型名称\",\r\n                \"required\": false,\r\n                \"typing\": \"string\"\r\n            }\r\n        }\r\n    },\r\n    {\r\n        \"enter_id\": \"QwenEnterPoint\",\r\n        \"name\": \"通义千问\",\r\n        \"parameters\": {\r\n            \"api_key\": {\r\n                \"default\": null,\r\n                \"name\": \"Token\",\r\n                \"required\": true,\r\n                \"typing\": \"string\"\r\n            },\r\n            \"model_name\": {\r\n                \"default\": \"qwen2-7b-instruct\",\r\n                \"name\": \"模型名称\",\r\n                \"required\": false,\r\n                \"typing\": \"string\"\r\n            }\r\n        }\r\n    },\r\n    {\r\n        \"enter_id\": \"VLLMEnterPoint\",\r\n        \"name\": \"VLLM\",\r\n        \"parameters\": {\r\n            \"api_key\": {\r\n                \"default\": null,\r\n                \"name\": \"API KEY\",\r\n                \"required\": false,\r\n                \"typing\": \"string\"\r\n            },\r\n            \"base_url\": {\r\n                \"default\": null,\r\n                \"name\": \"Base URL\",\r\n                \"required\": true,\r\n                \"typing\": \"string\"\r\n            },\r\n            \"model_name\": {\r\n                \"default\": null,\r\n                \"name\": \"模型名称\",\r\n                \"required\": true,\r\n                \"typing\": \"string\"\r\n            }\r\n        }\r\n    }\r\n]\r\n```"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 35,
    "title": "👑 [需求]走向国际化，github首页及文档增加英文",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-15T01:20:51Z",
    "updated_at": "2025-03-03T09:39:48Z",
    "labels": [
      "documentation"
    ],
    "body": "rt",
    "comments": [
      {
        "user": "FredaZero",
        "body": "我明天搞一版出来"
      },
      {
        "user": "yuruotong1",
        "body": "> 我明天搞一版出来\n\n感谢 pr"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 44,
    "title": "🐛[BUG] ERROR  Could not resolve \"./shortcut\" from \"src/main/index.ts\"",
    "author": "changye-chen",
    "state": "closed",
    "created_at": "2024-08-10T09:00:09Z",
    "updated_at": "2025-03-03T09:39:41Z",
    "labels": [],
    "body": "### 🐛 bug 描述\r\n\r\n<!--\r\n详细地描述 bug，让大家都能理解\r\n-->\r\n前端启动失败，原因在于index.ts中shortCut写成了shortcut\r\n### 📷 复现步骤\r\n\r\n<!--\r\n清晰描述复现步骤，让别人也能看到问题\r\n-->\r\n\r\n### 🏞 期望结果\r\n\r\n<!--\r\n描述你原本期望看到的结果\r\n-->\r\n\r\n### 💻 复现代码\r\n\r\n<!--\r\n提供可复现的代码，仓库，或线上示例\r\n(可在下方 codesandbox 链接中添加你的最小可复现 demo)\r\n-->\r\n\r\n[可复现 demo](https://codesandbox.io/s/html2ksetch-demo-m53be?file=/src/Demo.tsx)\r\n\r\n### © 版本信息\r\n\r\n- @yuruotong1/autoMate 版本: [e.g. 1.0.0]\r\n- 浏览器环境\r\n- 开发环境 [e.g. mac OS]\r\n\r\n### 🚑 其他信息\r\n\r\n<!--\r\n如截图等其他信息可以贴在这里\r\n-->",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "感谢提供反馈！"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 46,
    "title": "🧐[问题]Missing Activation Script of Backend in Readme",
    "author": "Gushroom",
    "state": "closed",
    "created_at": "2024-09-04T13:55:01Z",
    "updated_at": "2025-03-03T09:39:32Z",
    "labels": [],
    "body": "### 🧐 问题描述\r\n\r\nMissing Activation Script of Backend in Readme\r\nFollowing the steps in Readme will install python dependencies in root pip, which is not the expected behavior.\r\n\r\n### 💻 示例代码\r\n\r\n<img width=\"414\" alt=\"image\" src=\"https://github.com/user-attachments/assets/12f4aaf0-6254-44bf-b4e2-2b7dc21f19d2\">\r\n",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "yes, thank you remind!"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 45,
    "title": "🧐[问题]would it be possible to have a direct download link without going through github I have too many problems installing that I can't solve, is there any method?",
    "author": "RaffaeleCazzorla98",
    "state": "closed",
    "created_at": "2024-08-23T09:53:37Z",
    "updated_at": "2025-03-03T09:39:25Z",
    "labels": [],
    "body": "### 🧐 问题描述\r\n\r\n<!--\r\n详细地描述问题，让大家都能理解\r\n-->\r\n\r\n### 💻 示例代码\r\n\r\n<!--\r\n如果有必要，展示代码，线上示例，或仓库\r\n-->\r\n\r\n### 🚑 其他信息\r\n\r\n<!--\r\n如截图等其他信息可以贴在这里\r\n-->",
    "comments": [
      {
        "user": "yuruotong1",
        "body": "sorry, only can download by  github.I can send you exe by weixin or feishu, do you have these app?"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 7,
    "title": "👑 [需求]在搜索框检索到代码标题后输入回车，直接执行代码而不是跳转到代码编辑页面",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T02:21:16Z",
    "updated_at": "2024-07-24T02:08:19Z",
    "labels": [
      "enhancement"
    ],
    "body": null,
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 19,
    "title": "👑 [需求] 智子助手图标下方增加文字",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T10:22:34Z",
    "updated_at": "2024-07-16T03:14:26Z",
    "labels": [
      "enhancement"
    ],
    "body": "<img width=\"251\" alt=\"5debe07bda6b8b9c7fa302633c9b2cbf_\" src=\"https://github.com/yuruotong1/autoMate/assets/31992251/701676a8-ab27-4ce2-bf42-9641b8ab6f6b\">",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 18,
    "title": "👑 [需求]增加悬浮显示“编辑代码”",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T10:21:44Z",
    "updated_at": "2024-07-16T03:03:31Z",
    "labels": [
      "enhancement"
    ],
    "body": "<img width=\"597\" alt=\"1ba925961ebeb98388218c739811f6c3_\" src=\"https://github.com/yuruotong1/autoMate/assets/31992251/21390713-553c-4417-91b3-17db74d8db7e\">",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 5,
    "title": "👑 [需求]与智子对话，能够对已有代码进行修改，而不是重新生成代码",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T02:18:47Z",
    "updated_at": "2024-07-15T02:01:38Z",
    "labels": [
      "enhancement"
    ],
    "body": null,
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 14,
    "title": "👑 [需求]自动更新时，加入加载进度界面",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-09T12:50:03Z",
    "updated_at": "2024-07-15T02:01:36Z",
    "labels": [
      "enhancement"
    ],
    "body": "rt",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 24,
    "title": "👑 [需求]右下角托盘右键增加“关于”，可查看当前软件版本",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T15:16:56Z",
    "updated_at": "2024-07-15T02:01:19Z",
    "labels": [
      "enhancement"
    ],
    "body": "![image](https://github.com/yuruotong1/autoMate/assets/31992251/1970522d-8f08-4575-a290-cfb609659cde)",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 9,
    "title": "👑 [需求]智子生成代码后，在其右上角加入运行按钮",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T23:33:58Z",
    "updated_at": "2024-07-15T01:51:08Z",
    "labels": [
      "enhancement"
    ],
    "body": "rt",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 25,
    "title": "👑 [需求]搞清楚启动时为什么会出现5个autoMate.exe和2个autoMateServer.exe进程",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T15:40:21Z",
    "updated_at": "2024-07-13T23:29:12Z",
    "labels": [
      "bug"
    ],
    "body": "![f86ebb6f25594b7ed639255587300c2](https://github.com/yuruotong1/autoMate/assets/31992251/c04ab471-4fb4-49f6-a47c-9f88385e69b1)",
    "comments": [
      {
        "user": "xujiajiadexiaokeai",
        "body": "![automate-screenshot](https://github.com/yuruotong1/autoMate/assets/30225423/e4d124fa-4584-4661-91a2-73e198bf973c)\r\n这边看下来应该是通过Electron框架引入的,类似Chromium,有`gpu-process`、`utility`、`renderer`这些进程.\r\n可以通过[process-explorer](https://learn.microsoft.com/en-us/sysinternals/downloads/process-explorer)这个工具来看到进程的具体信息.\r\n希望对这个问题能有些帮助..."
      },
      {
        "user": "yuruotong1",
        "body": "> ![automate-screenshot](https://private-user-images.githubusercontent.com/30225423/347862398-e4d124fa-4584-4661-91a2-73e198bf973c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3OTY0MzMsIm5iZiI6MTcyMDc5NjEzMywicGF0aCI6Ii8zMDIyNTQyMy8zNDc4NjIzOTgtZTRkMTI0ZmEtNDU4NC00NjYxLTkxYTItNzNlMTk4YmY5NzNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzEyVDE0NTUzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU2YTBiMzgwYzEwZGVjYjFkMTFjNDkxNWFiYmMwODM0MDk2ODA5MGE5MGUwY2QwNDdmNTBkN2RlMTM3MDE0YmQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lo0NM51a_q8oPpVjnRxplhtpeqyde4dbc_4pqbnlUkk) 这边看下来应该是通过Electron框架引入的,类似Chromium,有`gpu-process`、`utility`、`renderer`这些进程. 可以通过[process-explorer](https://learn.microsoft.com/en-us/sysinternals/downloads/process-explorer)这个工具来看到进程的具体信息. 希望对这个问题能有些帮助...\r\n\r\n目前来看应该不影响使用，感谢提供线索！这是不是electron的正常机制呀"
      },
      {
        "user": "xujiajiadexiaokeai",
        "body": "> > ![automate-screenshot](https://private-user-images.githubusercontent.com/30225423/347862398-e4d124fa-4584-4661-91a2-73e198bf973c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3OTY0MzMsIm5iZiI6MTcyMDc5NjEzMywicGF0aCI6Ii8zMDIyNTQyMy8zNDc4NjIzOTgtZTRkMTI0ZmEtNDU4NC00NjYxLTkxYTItNzNlMTk4YmY5NzNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzEyVDE0NTUzM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU2YTBiMzgwYzEwZGVjYjFkMTFjNDkxNWFiYmMwODM0MDk2ODA5MGE5MGUwY2QwNDdmNTBkN2RlMTM3MDE0YmQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lo0NM51a_q8oPpVjnRxplhtpeqyde4dbc_4pqbnlUkk) 这边看下来应该是通过Electron框架引入的,类似Chromium,有`gpu-process`、`utility`、`renderer`这些进程. 可以通过[process-explorer](https://learn.microsoft.com/en-us/sysinternals/downloads/process-explorer)这个工具来看到进程的具体信息. 希望对这个问题能有些帮助...\r\n> \r\n> 目前来看应该不影响使用，感谢提供线索！这是不是electron的正常机制呀\r\n\r\n是的，是Electron项目正常的进程架构模型"
      }
    ],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 20,
    "title": "👑 [需求]配置大模型的报错信息更详细",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-10T11:33:24Z",
    "updated_at": "2024-07-10T14:42:45Z",
    "labels": [
      "enhancement"
    ],
    "body": "目前只有一个TypeError，期望加入更多的报错信息：\r\n\r\n![83d0144eb6540c03a70aff64099dff1](https://github.com/yuruotong1/autoMate/assets/31992251/b3f2799a-3c2e-4ea9-98be-ac4e437d34e5)\r\n",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 8,
    "title": "👑 [需求]检测更新后自动更新，无需重新下载exe",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T02:33:31Z",
    "updated_at": "2024-07-09T12:36:55Z",
    "labels": [
      "enhancement"
    ],
    "body": "rt",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 13,
    "title": "👑 [需求]把通义千问设置为默认模型",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-08T04:45:46Z",
    "updated_at": "2024-07-08T08:43:36Z",
    "labels": [
      "enhancement"
    ],
    "body": "![e576ed963c90fdab23cf179ba03cac1](https://github.com/yuruotong1/autoMate/assets/31992251/c15632d5-f074-418a-899e-6f6bd1c5b97a)",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 6,
    "title": "👑 [需求]在搜索框输入内容，按下回车可直接与智子对话",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T02:20:02Z",
    "updated_at": "2024-07-05T10:59:15Z",
    "labels": [
      "enhancement"
    ],
    "body": null,
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 4,
    "title": "👑 [需求]配置LLM由json输入改为输入框，加入取消保存按钮",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T02:17:41Z",
    "updated_at": "2024-07-05T10:59:15Z",
    "labels": [
      "enhancement"
    ],
    "body": "RT",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  },
  {
    "issue_number": 3,
    "title": "👑 [需求]配置LLM的地方增加校验",
    "author": "yuruotong1",
    "state": "closed",
    "created_at": "2024-07-05T02:16:53Z",
    "updated_at": "2024-07-05T10:59:14Z",
    "labels": [
      "enhancement"
    ],
    "body": "RT",
    "comments": [],
    "repository": "yuruotong1/autoMate"
  }
]