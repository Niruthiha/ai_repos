[
  {
    "issue_number": 18,
    "title": "support for open source local llms using different model providers.",
    "author": "Deepansharora27",
    "state": "open",
    "created_at": "2024-09-12T09:40:15Z",
    "updated_at": "2024-10-24T00:07:34Z",
    "labels": [],
    "body": "Hi, \r\n\r\nI see in the Config File that Most Of the Open Source Models are being used via TogetherAI as the Orchestration Layer. Can We Change this to it using Ollama Instead for Example ?",
    "comments": [],
    "repository": "junruxiong/IncarnaMind"
  }
]