[
  {
    "issue_number": 14,
    "title": "IndexError issue while uploading pdf file. ",
    "author": "coderooz",
    "state": "open",
    "created_at": "2024-08-26T19:28:10Z",
    "updated_at": "2024-08-26T19:28:39Z",
    "labels": [],
    "body": "While upooading a simple pdf file, this is the error that pops up. \n\nIndexError: This app has encountered an error. The original error message is redacted to prevent data leaks. Full error details have been recorded in the logs (if you're on Streamlit Cloud, click on 'Manage app' in the lower right of your app).\n\n```\nTraceback:\n\nFile \"/home/adminuser/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 589, in _run_script exec(code, module.__dict__)\n\nFile \"/mount/src/gemini_multipdf_chat/app.py\", line 144, in <module> main()\n\nFile \"/mount/src/gemini_multipdf_chat/app.py\", line 103, in main get_vector_store(text_chunks)\n\nFile \"/mount/src/gemini_multipdf_chat/app.py\", line 43, in get_vector_store vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n\nFile \"/home/adminuser/venv/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\", line 931, in from_texts return cls.__from(\n\nFile \"/home/adminuser/venv/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\", line 888, in __from index = faiss.IndexFlatL2(len(embeddings[0]))```\n\n",
    "comments": [],
    "repository": "kaifcoder/gemini_multipdf_chat"
  },
  {
    "issue_number": 13,
    "title": "BlockedPromptException: block_reason: OTHER",
    "author": "dhruv21995",
    "state": "open",
    "created_at": "2024-07-03T07:52:23Z",
    "updated_at": "2024-07-03T09:20:11Z",
    "labels": [],
    "body": "```\r\n2024-07-03 13:16:24.042 Uncaught app exception\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\streamlit\\runtime\\scriptrunner\\script_runner.py\", line 600, in _run_script\r\n    exec(code, module.__dict__)\r\n  File \"C:\\Users\\BT428FR\\gemini_multipdf_chat\\app.py\", line 169, in <module>\r\n    main()\r\n  File \"C:\\Users\\BT428FR\\gemini_multipdf_chat\\app.py\", line 154, in main\r\n    response = user_input(prompt)\r\n               ^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\gemini_multipdf_chat\\app.py\", line 98, in user_input\r\n    response = chain.invoke(\r\n               ^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\r\n    raise e\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 137, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n                                ^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 244, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\llm.py\", line 316, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py\", line 148, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\r\n    return self.invoke(\r\n           ^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\r\n    raise e\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\r\n    return self.llm.generate_prompt(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 599, in generate_prompt\r\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 456, in generate\r\n    raise e\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 446, in generate\r\n    self._generate_with_cache(\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 671, in _generate_with_cache\r\n    result = self._generate(\r\n             ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py\", line 666, in _generate\r\n    response: genai.types.GenerateContentResponse = _chat_with_retry(\r\n                                                    ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py\", line 171, in _chat_with_retry\r\n    return _chat_with_retry(**kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\r\n    return self(f, *args, **kw)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\r\n    do = self.iter(retry_state=retry_state)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py\", line 314, in iter\r\n    return fut.result()\r\n           ^^^^^^^^^^^^\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py\", \r\nline 449, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py\", \r\nline 401, in __get_result\r\n    raise self._exception\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\r\n    result = fn(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py\", line 169, in _chat_with_retry\r\n    raise e\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py\", line 153, in _chat_with_retry\r\n    return generation_method(**kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\generativeai\\generative_models.py\", line 505, in send_message\r\n    self._check_response(response=response, stream=stream)\r\n  File \"C:\\Users\\BT428FR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\generativeai\\generative_models.py\", line 524, in _check_response\r\n    raise generation_types.BlockedPromptException(response.prompt_feedback)\r\ngoogle.generativeai.types.generation_types.BlockedPromptException: block_reason: OTHER\r\n```",
    "comments": [
      {
        "user": "dhruv21995",
        "body": "i made some changes to the code & it still works fine. changed the chain.__call__ to chain.invoke due to deprecation. uploaded a couple PDFs. it worked fine for 1 but failed for another giving this issue. plz help."
      },
      {
        "user": "kaifcoder",
        "body": "Check your prompt as it gets blocked by the gemini may be context contains some offensive stuff for the llm"
      }
    ],
    "repository": "kaifcoder/gemini_multipdf_chat"
  }
]