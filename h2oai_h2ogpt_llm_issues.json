[
  {
    "issue_number": 1950,
    "title": "Ubuntu 22.04.5 LTS, et al. h2ogpt install script fail",
    "author": "ob83",
    "state": "closed",
    "created_at": "2025-05-11T23:26:01Z",
    "updated_at": "2025-05-14T11:25:35Z",
    "labels": [],
    "body": "This all worked back in Nov.2024 on Linux Mint, but now fails at the exact same point on Mint and Ubuntu regardless of OS versions I've tried, on fresh OS installs.\n\nFailing installer: Linux Quick Install (..h2ogpt/blob/main/docs/README_LINUX.md)\n\nFailing script: curl -fsSL https://h2o-release.s3.amazonaws.com/h2ogpt/linux_install_full.sh | bash\n\nError message (always the same & at the same exact location): \n\t\t\t...\n  Downloading posthog-3.22.0-py2.py3-none-any.whl.metadata (3.0 kB)\n  Downloading posthog-3.21.0-py2.py3-none-any.whl.metadata (2.9 kB)\n  Downloading posthog-3.20.0-py2.py3-none-any.whl.metadata (2.9 kB)\n  Downloading posthog-3.19.1-py2.py3-none-any.whl.metadata (2.9 kB)\nerror: resolution-too-deep\n\n× Dependency resolution exceeded maximum depth\n╰─> Pip cannot resolve the current dependencies as the dependency graph is too complex for pip to solve efficiently.\n\t\t\t...\n\t\t\t\nLet me know anything you'd like me to try. Many thanks!",
    "comments": [
      {
        "user": "ob83",
        "body": "I fixed it locally & it works good. There were several bugs I will investigate how they got in. Thanks for making this code available. Salute, H2o.ai!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1949,
    "title": "Unable to Upload Documents",
    "author": "pseudomushi",
    "state": "open",
    "created_at": "2025-05-11T05:20:47Z",
    "updated_at": "2025-05-11T11:46:38Z",
    "labels": [],
    "body": "Really appreciate for making H2O.ai and making it available for the public. Been using H2O.ai for the last 2 weeks however, recently I have encountered an issue towards uploading documents. To be precise on my issue, when I click on 'Add Documents' a new dialogue box opens asking to browse the files to upload and thereafter, when I upload the required documents, the 'Add' button is not working. \n\nPlease see the below picture for reference.\n\n![Image](https://github.com/user-attachments/assets/9cd3cbde-39bd-433a-845b-729d1cb635d1)\n\nSimilarly, the 'Add' button within the 'Import from file system' is also not working. Please see the below screenshot for reference.\n\n![Image](https://github.com/user-attachments/assets/2c300d6c-bd67-4747-8e09-96d29e80c5f5)\n\nPlease help me out understand the issue, Thank you.",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1944,
    "title": "Use gocloud.dev as a drop in replacement for *sql.DB",
    "author": "avnerv",
    "state": "open",
    "created_at": "2025-04-29T06:54:14Z",
    "updated_at": "2025-04-29T06:54:14Z",
    "labels": [],
    "body": "details in: https://github.com/h2oai/cloud-platform/issues/107",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1943,
    "title": "User-Created Shared Collections Not Visible to Other Users",
    "author": "Abhishek577",
    "state": "open",
    "created_at": "2025-04-26T10:41:48Z",
    "updated_at": "2025-04-26T10:41:48Z",
    "labels": [],
    "body": "I have developed an application using H2O GPT as the back-end.\nWhen I create a shared collection through the CLI, it works correctly — all users are able to view and access the collection after logging in.\n\nHowever, when a user creates a collection via the UI (with type set to shared), the collection becomes visible only to that specific user. Other users are unable to see or access it, even though it is marked as shared.\n\nExpected Behavior:\nShared collections created by any user should be accessible and visible to all users, just like collections created through the CLI.\n\nCurrent Behavior:\nCollections created via the UI in shared mode are restricted only to the creating user.\n\nAdditional Info:\n\nCLI-created shared collections behave correctly.\n\nUI-created shared collections are incorrectly restricted.\n\nCould you please help clarify if this is a bug or if some additional configuration is needed?\n\n",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1932,
    "title": "Are the Gradio  and  OpenWebUI Demos cancelled?",
    "author": "DorinDXN",
    "state": "closed",
    "created_at": "2025-02-21T17:15:59Z",
    "updated_at": "2025-04-16T17:17:21Z",
    "labels": [],
    "body": "Thanks for your work.\n\nAre the Gradio Demo (https://gpt.h2o.ai/) and  OpenWebUI Demo (https://gpt-docs.h2o.ai/)  cancelled from now on?\n\ncheers,\nDorin",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Yes, you can still use fremium h2ogpte that they point to."
      },
      {
        "user": "DorinDXN",
        "body": "Thanks,\nI guess that is\nhttps://h2ogpte.genai.h2o.ai/\nI see the costs\nhttps://h2ogpte.genai.h2o.ai/models\nbut is not clear how and when they are applied\nin the video https://www.youtube.com/watch?v=R5geRo1ijm8\nthere is an 'Usage' tab that is not present in the new variant\nhttps://h2ogpte.genai.h2o.ai/usage\ngives 404 error.\n\nPlease give some more details.\n\ncheers,\nDorin "
      },
      {
        "user": "pseudotensor",
        "body": "The usage is in the models page too:\n\n![Image](https://github.com/user-attachments/assets/854bdbe1-0c26-4fdd-9c40-53896be2d1a3)"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 950,
    "title": "Add option to disable ssl verify check ",
    "author": "kalaiselvan263",
    "state": "closed",
    "created_at": "2023-10-12T16:52:27Z",
    "updated_at": "2025-04-08T09:41:56Z",
    "labels": [
      "reporter/support",
      "type/feature",
      "cust-citi"
    ],
    "body": "https://support.h2o.ai/a/tickets/106627\r\n\r\nNeed option to disable ssl verify in code \r\n\r\n```\r\nfrom gradio_client import Client\r\nimport ast\r\nHOST_URL =\"[https://eas-aimlservices-gtdcu.nam.nsroot.net:21425/\"](https://eas-aimlservices-gtdcu.nam.nsroot.net:21425/%22)\r\nclient = Client(HOST_URL)\r\n # string of dict for input\r\nkwargs = dict(instruction_nochat='Who are you?')\r\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\r\n # string of dict for output\r\nresponse = ast.literal_eval(res)['response']\r\nprint(response)\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "This was done earlier.\r\n\r\nThese options from gradio are exposed:\r\n\r\n```\r\n        ssl_verify: bool = True,\r\n        ssl_keyfile: str | None = None,\r\n        ssl_certfile: str | None = None,\r\n        ssl_keyfile_password: str | None = None,\r\n\r\n```"
      },
      {
        "user": "kalaiselvan263",
        "body": "Customer have added the ssl_verify=False , However still they are getting the same error.\r\n\r\n```\r\n(gradioclient) -bash-4.2$ python generate.py --ssl_verify=False\r\nLoaded as API: https://eas-aimlservices-gtdcu.nam.nsroot.net:21425/ ✔\r\nTraceback (most recent call last):\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1042, in _validate_conn\r\n    conn.connect()\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connection.py\", line 414, in connect\r\n    self.sock = ssl_wrap_socket(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\r\n    ssl_sock = _ssl_wrap_socket_impl(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\r\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/ssl.py\", line 512, in wrap_socket\r\n    return self.sslsocket_class._create(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/ssl.py\", line 1070, in _create\r\n    self.do_handshake()\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/ssl.py\", line 1341, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)\r\n \r\nDuring handling of the above exception, another exception occurred:\r\n \r\nTraceback (most recent call last):\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/requests/adapters.py\", line 489, in send\r\n    resp = conn.urlopen(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\r\n    retries = retries.increment(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/util/retry.py\", line 592, in increment\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='eas-aimlservices-gtdcu.nam.nsroot.net', port=21425): Max retries exceeded with url: /config (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\r\n\r\n(base) [root@aingtu4689 h2ogpt]# curl -LIk https://eas-aimlservices-gtdcu.nam.nsroot.net:21234/\r\nHTTP/1.1 200 OK\r\ndate: Tue, 17 Oct 2023 07:48:53 GMT\r\nserver: uvicorn\r\ncontent-length: 320578\r\ncontent-type: text/html; charset=utf-8\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "@kalaiselvan263 Did they do something like this?  I don't know SSL much, but maybe even if don't verify, still need to set the other things:\r\n\r\nhttps://stackoverflow.com/a/76405442\r\n\r\nBut unclear if really solved or solvable:\r\nhttps://github.com/gradio-app/gradio/issues/3846#issuecomment-1653460562\r\nhttps://github.com/gradio-app/gradio/issues/5497"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1929,
    "title": "Container Registry?",
    "author": "devinrouthuzh",
    "state": "open",
    "created_at": "2025-02-05T12:28:12Z",
    "updated_at": "2025-04-01T10:54:23Z",
    "labels": [],
    "body": "Greetings,\n\nI noticed that the [container registry](https://console.cloud.google.com/gcr/images/vorvan/global/h2oai/h2ogpt-runtime) linked in the [Prebuild Docker section](https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#prebuild-docker-for-windowslinux-x86) of the docs isn't available. Is there an issue here, or am I doing/referencing something incorrectly?\n\nThanks!\n\nBest wishes,\nDevin Routh",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "https://github.com/h2oai/h2ogpt/issues/1924#issuecomment-2606492121"
      },
      {
        "user": "devinrouthuzh",
        "body": "I'm still seeing an error when loading the linked container registry. Is this just me?\n\n<img width=\"788\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b82f753c-c5f8-4b75-90f3-348c06f2b721\" />"
      },
      {
        "user": "devinrouthuzh",
        "body": "I wanted to followup as I'm unable to view the container registry. Am I missing something, or is the registry not available for others as well?"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1925,
    "title": "Best Way to Enable Multi-Database Selection in H2O GPT?",
    "author": "BhoomikaMuralidhara",
    "state": "open",
    "created_at": "2025-01-21T10:57:22Z",
    "updated_at": "2025-03-31T04:51:21Z",
    "labels": [],
    "body": "Hello,\nI’m trying to implement a feature in H2O GPT where users can select multiple databases in real-time using checkboxes, and the system queries all selected databases dynamically as the selection changes. My goals are:\n\nAllow users to select or deselect databases (via checkboxes) dynamically.\nPerform queries in real-time based on the current selection.\nCombine results from all selected databases into a single response.\nI’m working with the evaluate_nochat function and want to modify:\n\nlangchain_mode to support multi-selection (as a list).\nQuery logic to dynamically loop through the selected databases and update results in real-time.\nIf anyone has experience with real-time query updates or can suggest the best approach to achieve this, I’d really appreciate your insights!\n\nThanks in advance!",
    "comments": [
      {
        "user": "NKM999",
        "body": "> Hello, I’m trying to implement a feature in H2O GPT where users can select multiple databases in real-time using checkboxes, and the system queries all selected databases dynamically as the selection changes. My goals are:\n> \n> Allow users to select or deselect databases (via checkboxes) dynamically. Perform queries in real-time based on the current selection. Combine results from all selected databases into a single response. I’m working with the evaluate_nochat function and want to modify:\n> \n> langchain_mode to support multi-selection (as a list). Query logic to dynamically loop through the selected databases and update results in real-time. If anyone has experience with real-time query updates or can suggest the best approach to achieve this, I’d really appreciate your insights!\n> \n> Thanks in advance!\n\nHi, could you get any solution??? @pseudotensor  Please help and share!!!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1941,
    "title": "Add \"All\" Collection Tab to Query Data from All Collections",
    "author": "intelligenceabhii",
    "state": "open",
    "created_at": "2025-03-30T17:26:18Z",
    "updated_at": "2025-03-31T04:46:54Z",
    "labels": [],
    "body": "Hi @pseudotensor I would like to request a feature to add an \"All\" collection tab that allows users to query data across all available collections at once. Selecting the \"All\" tab should fetch and display data from every collection, simplifying the process of querying and viewing data across collections without switching between them.\n\n**If this feature is already available, please provide guidance on how to enable it.**",
    "comments": [
      {
        "user": "NKM999",
        "body": "> Hi [@pseudotensor](https://github.com/pseudotensor) I would like to request a feature to add an \"All\" collection tab that allows users to query data across all available collections at once. Selecting the \"All\" tab should fetch and display data from every collection, simplifying the process of querying and viewing data across collections without switching between them.\n> \n> **If this feature is already available, please provide guidance on how to enable it.**\n\nI am also having the same issue. I tried harder but could not get it.  @pseudotensor Please save me!!!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1940,
    "title": "GCR is Depricated, unable to pull images",
    "author": "z0rx0r",
    "state": "closed",
    "created_at": "2025-03-26T17:42:28Z",
    "updated_at": "2025-03-26T22:21:52Z",
    "labels": [],
    "body": "As of March 18th GCR is no longer available as a container registry - this makes pulling gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 not possible. \n\nPlease update where you are storing the base images. ",
    "comments": [
      {
        "user": "lakinduakash",
        "body": "The new repository is at docker hub,\nwe can use `docker pull h2oairelease/h2oai-h2ogpt-runtime:v0.2.1-1245`"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1934,
    "title": "MyData is not deleting docs on page load or re-load with authentication.",
    "author": "intelligenceabhii",
    "state": "open",
    "created_at": "2025-03-11T04:49:52Z",
    "updated_at": "2025-03-11T04:49:52Z",
    "labels": [],
    "body": "It has been observed that when accessed without authentication, the MyData PDF is deleted as expected. However, the MyData collection is not deleted upon page load / re-load on the Document tab when accessed with authentication. How to achieve the docs deletion with authentication? Please help @pseudotensor !!!",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 369,
    "title": "About UserData and MyData",
    "author": "joaonevesvisualnuts",
    "state": "closed",
    "created_at": "2023-06-30T19:02:32Z",
    "updated_at": "2025-03-10T09:58:33Z",
    "labels": [],
    "body": "When I upload anything to these databases, are they public?\r\nFrom what I understood, \"MyData\" is for me only, \"UserData\" is shared, but shared with whom?\r\nIs it with everyone connected to my private server or is it to a public server?\r\n\r\nThank you in advance.\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "UserData: Shared with anyone who is on your server.  Persisted across sessions in single location for entire server.  Control upload via allow_upload_to_user_data option.  Useful for collaboration.\r\n\r\nMyData: Scratch space that is inaccessible if one goes into a new browser session.  Useful for public demonstrations so that every instance is independent.  Or useful  if user is not allowed to upload to shared UserData and wants to do Q/A."
      },
      {
        "user": "intelligenceabhii",
        "body": "It has been observed that when accessed without authentication, the MyData PDF is deleted as expected. However,  the MyData collection is not deleted upon page load / re-load on the Document tab when accessed with authentication. How to achieve the docs deletion with authentication? Please help @pseudotensor !!!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1933,
    "title": "Fresh install fails on Ubuntu 24.04 LTS",
    "author": "dawnarius",
    "state": "open",
    "created_at": "2025-02-27T08:57:43Z",
    "updated_at": "2025-02-27T10:43:00Z",
    "labels": [],
    "body": "Hello\nSince this project is obviously no longer maintained I can't manage to install it anymore (on a fresh Ubuntu 24.04 LTS with python 3.10 and following the official guides)\nToo much errors and conflicts since some packages are actively maintained but not h2ogpt code\nCan anyone provide me with list of the correct versions of the used packages? or a clear and working guide to install it on a fresh Ubuntu 24.04 LTS with python 3.10?\nAlso if anyone has a working version, please do \"pip env list\" and give me the resulting list\nIt could really help\nThanks ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I haven't tried Ubuntu 24 yet, just 22.  Maybe someone else has tried it.\n\nI don't expect any conflicts etc. because most packages are fixed in version that ever had had issues in past.\n\nE.g. you could try docker build but change to Ubuntu 24.  I expect it would work fine.\n\nA separate internal version is maintained and i haven't noticed any issues in any builds."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1818,
    "title": "I encountered an error while trying to use the tool. This was the error: 1 validation error for SerperDevToolSchema search_query   str type expected (type=type_error.str).  Tool Search the internet accepts these inputs: Search the internet(search_query: 'string') - A tool that can be used to search the internet with a search_query. search_query: 'Mandatory search query you want to use to search the internet'",
    "author": "RaghavMangla",
    "state": "open",
    "created_at": "2024-08-28T11:26:01Z",
    "updated_at": "2025-02-09T09:22:10Z",
    "labels": [],
    "body": "how to fix this error, occurs while running a crewai agent",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I've seen that sometimes, I think the serp stuff maybe fails or does something odd sometimes.\r\n\r\nDo you see this sometimes or all the time?"
      },
      {
        "user": "Ansumanbhujabal",
        "body": "I am seeing that frequently these days , Any solutions?"
      },
      {
        "user": "pseudotensor",
        "body": "Can you give more details?  Full back trace?  When it occurs?  What system?  Thanks!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1930,
    "title": "GoogleSerperAPIWrapper gives error     \" Input should be a valid string [type=string_type, input_value={'description': 'Dr. Mihi...n field', 'type': 'str'}, input_type=dict]     For further information visit https://errors.pydantic.dev/2.10/v/string_type.\"",
    "author": "Ansumanbhujabal",
    "state": "open",
    "created_at": "2025-02-06T15:24:49Z",
    "updated_at": "2025-02-06T15:24:49Z",
    "labels": [],
    "body": "### **Sample Serper use**\n```\nsearch = GoogleSerperAPIWrapper()\n@tool(\"GoogleSearchTool\")\ndef Google_search_tool(search_query: str):\n    \"\"\"Performs a search using the GoogleSearchTool.\"\"\"\n    return search().run(search_query)\n## Defining Agents\n\nfamous_personality_reviewer_agent = Agent(role=\n                          \"Senior Content Validator and fact checker\",\n                          goal=\"Validate the content with facts and get the most correct content\", \n                          backstory=\n                          \"\"\"\n                          You are a Senior Content Validator and fact checker  who has been assigned to validate the task \n                          for the {career_name} field if {famous_personalities_name} You will give best fact based \n                          \"\"\",\n                          llm=llm,\n                          tools = [Google_search_tool],\n                          verbose=True)\n```\n\n### **Error is** \n\n```\nI encountered an error while trying to use the tool. This was the error: Arguments validation failed: 1 validation error for Googlesearchtool\nsearch_query\n  Input should be a valid string [type=string_type, input_value={'description': 'Dr. Mihi...n field', 'type': 'str'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type.\n Tool GoogleSearchTool accepts these inputs: Tool Name: GoogleSearchTool\nTool Arguments: {'search_query': {'description': None, 'type': 'str'}}\nTool Description: Performs a search using the GoogleSearchTool.\n```\n\n\n### **Dependecies**\n\n```\nagentneo==1.2.3\ncachetools==5.5.0\ncertifi==2024.12.14\ncrewai==0.95.0\ncrewai-tools==0.32.0\nGitPython==3.1.44\ngoogle-api-core==2.24.0\ngoogle-auth==2.37.0\ngoogle-cloud-aiplatform==1.77.0\ngoogle-cloud-bigquery==3.27.0\ngoogle-cloud-core==2.4.1\ngoogle-cloud-resource-manager==1.14.0\ngoogle-cloud-storage==2.19.0\ngoogle-crc32c==1.6.0\ngoogle-resumable-media==2.7.2\ngoogleapis-common-protos==1.66.0\ngroq==0.13.1\ngrpc-google-iam-v1==0.14.0\nlangchain==0.3.17\nlangchain-cohere==0.3.4\nlangchain-community==0.3.16\nlangchain-core==0.3.33\nlangchain-exa==0.2.1\nlangchain-experimental==0.3.4\nlangchain-groq==0.2.2\nlangchain-openai==0.2.14\nlangchain-pinecone==0.2.0\nlangchain-text-splitters==0.3.5\npydantic==2.10.4\npydantic-settings==2.7.1\npydantic_core==2.27.2\nscrapegraph_py==1.10.0\nselenium==4.27.1\nserpapi==0.1.5\nsetuptools==75.8.0\n\n```\n",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1928,
    "title": "[Errno 24] https://gpt.h2o.ai/",
    "author": "DorinDXN",
    "state": "closed",
    "created_at": "2025-01-28T18:42:24Z",
    "updated_at": "2025-01-28T19:10:58Z",
    "labels": [],
    "body": "In https://gpt.h2o.ai/ \nthis error occurs, then multiple errors about connections\n[Errno 24] Too many open files: 'sources_dir/sources_LLM_53720439-1770-44b7-a5e5-0ac3ac94738f'\n\nPlease investigate,\ncheers,\nDorin",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Is it ok after restart?  I seem to have to restart once a month or so."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1926,
    "title": "Issue with Large-Scale Document Embedding in H2O GPT",
    "author": "BhoomikaMuralidhara",
    "state": "open",
    "created_at": "2025-01-22T11:57:49Z",
    "updated_at": "2025-01-22T11:57:49Z",
    "labels": [],
    "body": "Hi everyone,\n\nA mode was created specifically for an email folder in H2O GPT, where all documents are .docx. An issue has been observed when embedding a large number of documents into this mode.\n\nHere’s what happens:\n\nWhen embedding fewer documents (e.g., around 100 or less), everything works fine—all documents are successfully added to the database, and new ones can be added without any problems.\nHowever, when embedding a large number of documents (e.g., around 13,000 .docx files), only a portion of the documents (approximately 4,000) appears in the database. After that, adding new documents becomes impossible.\nThis issue seems specific to the email folder mode. Since all documents are .docx, it doesn’t appear to be related to missing libraries.\n\nCould this behavior be related to:\n\nA database size limit?\nMemory constraints?\nA misconfiguration in the mode or embedding setup?\nAny insights or suggestions for resolving this would be greatly appreciated.\n\nThanks in advance!",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1924,
    "title": "Status of project? No new versions since 24.06.04; docker images on soon to be deprecated container registry",
    "author": "agm-eratosth",
    "state": "open",
    "created_at": "2025-01-20T22:27:34Z",
    "updated_at": "2025-01-22T07:37:30Z",
    "labels": [],
    "body": "Hi,\n\nCould you please comment on the status and roadmap of this project? I'm seeing some code commits but version bumps have ended since June. \n\nAdditionally Docker images used to get a commit specific build at https://gcr.io/vorvan/h2oai/h2ogpt-runtime but that hasn't happened since August. Finally, there is a deprecation warning when visiting that URL that the google cloud container registry will be deprecated on March 18th 2025 in favor of the artifact registry.\n\nThanks.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "The pypi version is somewhat incomplete compared to the full version for windows and linux via docker or the installer, so it hasn't been much of a focus to frequently update releases.  There are just alot of non-pypi package things that are required for the full thing.\n\n@EshamAaqib Perhaps the link to the OSS h2oGPT vorvan is different at some point?  Aug is too early a cut-off, so it must have moved."
      },
      {
        "user": "pseudotensor",
        "body": "The current location on vorvan is https://console.cloud.google.com/gcr/images/vorvan/global/h2oai/h2oai-h2ogpt-runtime\n\n![Image](https://github.com/user-attachments/assets/ebcf797a-e51f-4a7f-9d89-1a55ad6a43ca)"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1921,
    "title": "`pydantic.errors.PydanticUserError` on CPU",
    "author": "denmuslimov",
    "state": "open",
    "created_at": "2025-01-15T03:39:45Z",
    "updated_at": "2025-01-15T03:39:45Z",
    "labels": [],
    "body": "Trying to run a CPU-based model on Windows 11.\nA clean installation following this instruction (changed the name of conda environment to avoid collisions):\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md\n\nTried running any model from the instruction below:\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_CPU.md\n\nFirst, I got errors for missing libraries:\n```\npip install fire\npip install langchain\npip install -U langchain-anthropic\npip install langchain-community\npip install langchain-experimental\npip install langchain-google-genai\n```\n\nAfter I got just this message:\n```\nC:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n  warnings.warn(message, UserWarning)\nTraceback (most recent call last):\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\generate.py\", line 20, in <module>\n    entrypoint_main()\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\generate.py\", line 16, in entrypoint_main\n    H2O_Fire(main)\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\src\\utils.py\", line 79, in H2O_Fire\n    fire.Fire(component=component, command=args)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\fire\\core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\fire\\core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\fire\\core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\src\\gen.py\", line 2058, in main\n    from gpt_langchain import get_embedding\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\src\\gpt_langchain.py\", line 60, in <module>\n    from langchain_mistralai.chat_models import ChatMistralAI\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\src\\langchain_mistralai\\chat_models.py\", line 317, in <module>\n    class ChatMistralAI(BaseChatModel):\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py\", line 224, in __new__\n    complete_model_class(\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py\", line 602, in complete_model_class\n    schema = cls.__get_pydantic_core_schema__(cls, handler)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\main.py\", line 702, in __get_pydantic_core_schema__\n    return handler(source)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 84, in __call__\n    schema = self._handler(source_type)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 610, in generate_schema\n    schema = self._generate_schema_inner(obj)\n  ...\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 612, in generate_schema\n    metadata_js_function = _extract_get_pydantic_json_schema(obj, schema)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2395, in _extract_get_pydantic_json_schema\n    raise PydanticUserError(\npydantic.errors.PydanticUserError: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n```\n",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1778,
    "title": "AutoGen Steps",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-08-01T21:38:29Z",
    "updated_at": "2025-01-11T07:11:55Z",
    "labels": [],
    "body": "Example prompts:\r\n* `Make a matplotlib plot and save as titanic.png, for the titanic surivors vs. other parameters in interesting way.`\r\n* `Today is August 31, 2024.  Write Python code to plot TSLA's and META's stock price gains YTD vs. time per week, and save the plot to a file named 'stock_gains.png'`\r\n\r\n\r\n- [x] code executation agent\r\n- [x] Add OpenAI Files API\r\n- [x] Return files in some way supported by OpenAI API for files so give back file objects instead of disk locations. use usage to return file_ids\r\n- [x] Prevent autogen from going on OpenAI server\r\n- [x] handle pip install issues like AgentZero\r\n- [x] Avoid newline in streaming of iostream\r\n- [x] Remove empty vs. None messages\r\n- [x] Collapse assistants\r\n- [ ] Check how continue works, seems to get stuck in loop (i.e. max_tokens small)\r\n- [x] Control docker life time externally, too slow (10s) for each chat to stop it then\r\n- [x] If no pyautogen, then don't allow autogen_server True\r\n- [x] DinD needs `--privileged` for docker in docker run.  But better is Docker out of Docker: https://microsoft.github.io/autogen/docs/topics/code-execution/cli-code-executor/#combining-autogen-in-docker-with-a-docker-based-executor with `-v /var/run/docker.sock:/var/run/docker.sock` to base `docker run`\r\n\r\n\r\nhttps://microsoft.github.io/autogen/docs/tutorial/conversation-patterns/\r\nhttps://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment\r\nhttps://microsoft.github.io/autogen/docs/tutorial/code-executors/\r\n\r\n\r\nSome discussions:\r\nhttps://www.reddit.com/r/LangChain/comments/1db6evc/best_production_agent_framework_langraph_vs/\r\nhttps://www.reddit.com/r/LangChain/comments/1b7q44y/autogen_vs_langgraph/\r\nlangchain tools with autogen: https://github.com/microsoft/autogen/blob/main/notebook/agentchat_langchain.ipynb",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1919,
    "title": "Windows 11: Can't run LLama",
    "author": "denmuslimov",
    "state": "open",
    "created_at": "2025-01-10T04:09:58Z",
    "updated_at": "2025-01-10T04:09:58Z",
    "labels": [],
    "body": "I'm experimenting with different models.\r\nSome refuse to run.\r\n\r\nWindows 11, RTX 4070 12Gb\r\n\r\nThe following commands: \r\n`python generate.py --base_model=TheBloke/Llama-2-7b-Chat-GPTQ --load_gptq=\"model\" --use_safetensors=True --prompt_type=llama2 --save_dir='save'`\r\n`python generate.py --base_model=TheBloke/Nous-Hermes-13B-GPTQ --score_model=None --load_gptq=model --use_safetensors=True --prompt_type=instruct --langchain_mode=UserData`\r\n\r\nReturn the this error:\r\n```\r\n...\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\utils.py\", line 79, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gen.py\", line 2329, in main\r\n    model_state_trial = model_lock_to_state(model_dict, cache_model_state=False, **kwargs_model_lock_to_state)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 1689, in model_lock_to_state\r\n    return __model_lock_to_state(model_dict1, **kwargs)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 1767, in __model_lock_to_state\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 425, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 1208, in get_model\r\n    return get_hf_model(load_8bit=load_8bit,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 1534, in get_hf_model\r\n    model = exllama_set_max_input_length(model, tokenizer.model_max_length)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\auto_gptq\\utils\\exllama_utils.py\", line 15, in exllama_set_max_input_length\r\n    from exllama_kernels import prepare_buffers, cleanup_buffers_cuda\r\nImportError: DLL load failed while importing exllama_kernels: The specified module could not be found.\r\n```",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1918,
    "title": "Windows Install -- getting error when running.",
    "author": "denmuslimov",
    "state": "closed",
    "created_at": "2025-01-06T02:15:18Z",
    "updated_at": "2025-01-08T05:01:16Z",
    "labels": [],
    "body": "After installing it, I can run the H2O GPT on `http://localhost:7860/`.\r\nBut every question results in an error.\r\n\r\nI have a Windows 11 with RTX 4070.\r\nFollowed this link for installation:\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md\r\nInstalled LLaMa simply with `pip install llama-cpp-python`.\r\n\r\nError:\r\n\r\n```\r\nINFO:     127.0.0.1:56398 - \"POST /queue/join HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:56398 - \"GET /queue/data?session_hash=00eo3wbgbcmln HTTP/1.1\" 200 OK\r\nevaluate_nochat exception: LlamaTokenizerFast has no attribute _pad_token: ('', '', '', True, False, 'zephyr'\r\n  ... \r\n, 'host2': '127.0.0.1', 'picture': 'None'}, {}, [['Hello', '']])\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\queueing.py\", line 575, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\r\n    result = await self.call_function(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\blocks.py\", line 1532, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\utils.py\", line 671, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\utils.py\", line 664, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\r\n    result = context.run(func, *args)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\utils.py\", line 647, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\utils.py\", line 809, in gen_wrapper\r\n    response = next(iterator)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gradio_funcs.py\", line 1215, in bot\r\n    for res in get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gradio_funcs.py\", line 625, in get_response\r\n    yield from _get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1, tts_speed1,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gradio_funcs.py\", line 870, in _get_response\r\n    for output_fun in fun1():\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gen.py\", line 4165, in evaluate\r\n    stopping_criteria = get_stopping(prompt_type, prompt_dict, tokenizer, device, base_model,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\stopping.py\", line 183, in get_stopping\r\n    if tokenizer._pad_token:  # use hidden variable to avoid annoying properly logger bug\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 1104, in __getattr__\r\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\r\nAttributeError: LlamaTokenizerFast has no attribute _pad_token. Did you mean: '_add_tokens'?\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "07ad82bc498e127a22381d98dbffbf1b845ef4e9"
      },
      {
        "user": "denmuslimov",
        "body": "Thank you!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1917,
    "title": "Throwing:  AttributeError: PreTrainedTokenizerFast has no attribute _pad_token. Did you mean: '_add_tokens' at runtime",
    "author": "l0r3zz",
    "state": "closed",
    "created_at": "2025-01-05T19:21:20Z",
    "updated_at": "2025-01-08T01:21:03Z",
    "labels": [],
    "body": "Hello all,\r\nRunning on an \r\n\r\n- Core™ i7-11800H @ 2.30GHz × 16\r\n- NVIDIA GeForce RTX 3070 Laptop GPU/PCIe/SSE2 / NVIDIA Corporation GA104M \r\n- Memory :64GiB\r\n- OS: Pop!_OS 22.04 LTS\r\n\r\nUsing startup...\r\npython generate.py \\\r\n        --base_model=h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \\\r\n        --score_model=None \\\r\n        --prompt_type=human_bot \\\r\n        --cli=True \\\r\n        --gradio_offline_level=1 \\\r\n        --load4bit=True\r\n\r\ncurrent repo version: \r\nbase) l0r3zz@tarnover:[2025-01-05 11:18:40]-$git log -1\r\ncommit a0fcc3344d53a834fe3cb5b26265aaeb84993b77 (HEAD -> main, origin/main, origin/HEAD)\r\nAuthor: Jonathan C. McKinney <pseudotensor@gmail.com>\r\nDate:   Tue Dec 3 23:58:28 2024 -0800\r\n\r\n\r\n(Got here after watching): https://youtu.be/Coj72EzmX20?si=ofBAsNACnB7JAKe7\r\n\r\ngot through all the build issues, but after startup, and the printing of:\r\n_Enter an instruction:_ \r\n\r\nIt blows up no matter what I enter...\r\n\r\n\r\n(base) l0r3zz@tarnover:[2025-01-04 07:52:56]-$./model.sh \r\nMust install langchain for transcription, disabling\r\nUsing Model h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\r\nMust install langchain for preloading embedding model, disabling\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\n/home/l0r3zz/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\ndevice_map: {'': 0}\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\ndevice_map: {'': 0}\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.62s/it]\r\n\r\nEnter an instruction: Hello World\r\nTraceback (most recent call last):\r\n  File \"/home/l0r3zz/github/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/l0r3zz/github/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/l0r3zz/github/h2ogpt/src/utils.py\", line 79, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/l0r3zz/.local/lib/python3.12/site-packages/fire/core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/.local/lib/python3.12/site-packages/fire/core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/.local/lib/python3.12/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/github/h2ogpt/src/gen.py\", line 2430, in main\r\n    return run_cli(**get_kwargs(run_cli, **local_kwargs))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/github/h2ogpt/src/cli.py\", line 226, in run_cli\r\n    for gen_output in gener:\r\n                      ^^^^^\r\n  File \"/home/l0r3zz/github/h2ogpt/src/gen.py\", line 4165, in evaluate\r\n    stopping_criteria = get_stopping(prompt_type, prompt_dict, tokenizer, device, base_model,\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/github/h2ogpt/src/stopping.py\", line 183, in get_stopping\r\n    if tokenizer._pad_token:  # use hidden variable to avoid annoying properly logger bug\r\n       ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 1104, in __getattr__\r\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\r\nAttributeError: PreTrainedTokenizerFast has no attribute _pad_token. Did you mean: '_add_tokens'?\r\n\r\n\r\nI tried some troubleshooting but can't get anywhere...\r\n\r\n\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "07ad82bc498e127a22381d98dbffbf1b845ef4e9"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1915,
    "title": "HTML format",
    "author": "Abhishek577",
    "state": "open",
    "created_at": "2025-01-01T09:39:52Z",
    "updated_at": "2025-01-01T09:39:52Z",
    "labels": [],
    "body": "While using own collection for querying the **auth.db** is storing the meta data, HTML tags and glimpse of source instead of the replay. All these are visible from the text_output of data column in auth.db. How to store the response only",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1912,
    "title": "Docker container not running in offline mode despite following documentation",
    "author": "llmIntruder",
    "state": "open",
    "created_at": "2024-12-26T15:15:59Z",
    "updated_at": "2024-12-26T15:15:59Z",
    "labels": [],
    "body": "Hi,\r\nI'm trying to run a Docker container offline, but I am encountering issues where the container still tries to download models and dependencies from the internet during runtime. I've followed the official documentation for Docker offline setup and have manually copied the .huggingface_cache and all necessary files, but the container is still connecting to the internet.",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1911,
    "title": "Files Uploaded via API Stored in Temp Folder Instead of Specific Path",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-12-24T06:17:49Z",
    "updated_at": "2024-12-24T06:17:49Z",
    "labels": [],
    "body": "## Description\r\nWhen I upload documents using the UI upload button, the files are correctly stored in the specified path associated with the collection (e.g., test_path/Introduction - Ladakh Expedition.pdf). However, when uploading the same files using the /upload_api and /add_file_api endpoints, the files are being stored in a temporary folder (e.g., /tmp/gradio/<random_hash>/<filename>).\r\nThis discrepancy in behavior between the UI and API uploads causes inconsistency in file storage, making it difficult to manage uploaded files.\r\n\r\nSteps to Reproduce\r\nUpload a file through the UI upload button.\r\nResult: File is stored in the correct path (e.g., test_path/).\r\nUpload the same file using the /upload_api and /add_file_api endpoints.\r\nResult: File is stored in a temp folder (e.g., /tmp/gradio/<random_hash>/).\r\n\r\nExpected Behavior\r\nFiles uploaded via the API should be stored in the same path as those uploaded through the UI (e.g., test_path/).\r\nActual Behavior\r\nFiles uploaded via the API are stored in a temporary folder instead of the intended path.\r\n\r\n```\r\n@app.post(\"/upload_add_document\")\r\nasync def upload_add_document(\r\n    langchain_mode: str = Form(...),\r\n    file: UploadFile = File(...)\r\n):\r\n    try:\r\n        allowed_extensions = {\".pdf\", \".txt\", \".docx\"}\r\n        _, ext = os.path.splitext(file.filename)\r\n        if ext.lower() not in allowed_extensions:\r\n            raise HTTPException(status_code=400, detail=\"Unsupported file type. Allowed types: .pdf, .txt, .docx\")\r\n\r\n        # Save the uploaded file locally\r\n        local_file_path = f\"{file.filename}\"\r\n        with open(local_file_path, \"wb\") as f:\r\n            f.write(file.file.read())\r\n\r\n        # Step 1: Upload the document\r\n        try:\r\n            with tqdm(total=100, desc=f\"Uploading {file.filename}\", unit='%', ncols=80) as pbar:\r\n                x, server_file_path = client.predict(local_file_path, api_name='/upload_api')\r\n                pbar.update(100)\r\n        except Exception as e:\r\n            raise HTTPException(status_code=500, detail=f\"Failed to upload document: {str(e)}\")\r\n\r\n        # Step 2: Add the document with OCR processing\r\n        try:\r\n            loaders = tuple([None, None, None, None, None, None])  # Adjust loaders as needed\r\n            with tqdm(total=100, desc=f\"Adding {file.filename} to {langchain_mode}\", unit='%', ncols=80) as pbar:\r\n                response = client.predict(\r\n                    server_file_path, langchain_mode, True, 512, True, *loaders, api_name='/add_file_api'\r\n                )\r\n                pbar.update(100)\r\n        except Exception as e:\r\n            raise HTTPException(status_code=500, detail=f\"Failed to add document: {str(e)}\")\r\n\r\n        # Cleanup: Delete the local file after processing\r\n        if os.path.exists(local_file_path):\r\n            os.remove(local_file_path)\r\n\r\n        return {\r\n            \"status\": \"success\",\r\n            \"langchain_mode\": langchain_mode,\r\n            \"filename\": file.filename,\r\n            \"server_file_path\": server_file_path,\r\n        }\r\n\r\n    except Exception as e:\r\n        if os.path.exists(local_file_path):\r\n            os.remove(local_file_path)\r\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\r\n```     \r\n       \r\n### Observed File Paths\r\nUI Uploads: test_path/Introduction - Ladakh Expedition.pdf\r\nAPI Uploads: /tmp/gradio/<random_hash>/Rahul_text.txt\r\n\r\nIts urgent please help me or suggest a way to resolve this issue on priority basis.\r\n@pseudotensor",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1910,
    "title": "Unable to Locate Login Page File for Adding Download Button",
    "author": "Abhishek577",
    "state": "open",
    "created_at": "2024-12-20T11:59:16Z",
    "updated_at": "2024-12-20T11:59:16Z",
    "labels": [],
    "body": "I am looking to add a download button to the login page to allow users to access an instruction document easily. However, I am unable to locate the specific file that handles the login page within the project. I’ve searched through the project directories but couldn’t identify where the login page is defined or rendered.\r\n\r\nCould someone please guide me on how to find the correct file or component for the login page? Additionally, any suggestions on the best way to implement a download button in this context would be greatly appreciated.\r\n\r\nThank you for your help!",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1907,
    "title": "Installing clean environment fails after execution of \"bash docs/linux_install_full.sh\"",
    "author": "juerware",
    "state": "open",
    "created_at": "2024-12-13T10:19:07Z",
    "updated_at": "2024-12-18T10:22:01Z",
    "labels": [],
    "body": "From clear environment, installing full environment is wanted executing:\r\n```bash docs/linux_install_full.sh```\r\nCurrent checkout of the respository ```a0fcc334``` (latest one at the moment of execution)\r\nError shown:\r\n```\r\n+ bash ./docs/run_patches.sh\r\n++ python3.10 -c 'import site; print(site.getsitepackages()[0])'\r\n+ sp=/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages\r\n+ sed -i 's/with HiddenPrints():/if True:/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/utilities/serpapi.py\r\n+ sed -i 's/client='\\''ANDROID_MUSIC'\\''/client='\\''ANDROID'\\''/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/17.31.35/19.08.35/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/17.33.2/19.08.35/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/17.31.35/19.08.35/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/17.33.2/19.08.35/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/5.16.51/6.40.52/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/5.21/6.41/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/pytubefixfix/pytubefix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i s/Pytube/PytubeFix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i s/PytubeFixFix/PytubeFix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i s/Pytube/Pytubefix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i 's/pytube>=15/pytube>=6/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i s/pytube/pytubefix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i 's/except OSError:/except (OSError, RuntimeError):/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\r\n+ sed -i 's/while True:/while True:\\n            time.sleep(0.001)\\n/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/gradio_client/client.py\r\n+ patch /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py docs/trans.patch\r\npatching file /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py\r\nHunk #1 FAILED at 3412.\r\n1 out of 1 hunk FAILED -- saving rejects to file /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py.rej\r\n```\r\n\r\nIn the same commit but following the install documentation with following error:\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#install\r\n```\r\n+ bash ./docs/run_patches.sh\r\n++ python3.10 -c 'import site; print(site.getsitepackages()[0])'\r\n+ sp=/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages\r\n+ sed -i 's/with HiddenPrints():/if True:/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/utilities/serpapi.py\r\n+ sed -i 's/except OSError:/except (OSError, RuntimeError):/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\r\n+ sed -i 's/while True:/while True:\\n            time.sleep(0.001)\\n/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/gradio_client/client.py\r\n+ patch /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py docs/trans.patch\r\npatching file /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py\r\nHunk #1 FAILED at 3412.\r\n1 out of 1 hunk FAILED -- saving rejects to file /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py.rej\r\n```",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1908,
    "title": "[Errno 28] No space left on device",
    "author": "DorinDXN",
    "state": "closed",
    "created_at": "2024-12-13T19:01:55Z",
    "updated_at": "2024-12-14T21:15:32Z",
    "labels": [],
    "body": "Hi there, hope you're well :)\r\n\r\nI'm getting this error \r\n'[Errno 28] No space left on device'\r\n in the browser. \r\nI think it started 2-3 days ago. I thought the issue was on my machine, but maybe it's a server issue. \r\nPlease check.\r\n\r\nbest regards,\r\nDorin",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You mean gpt.h2o.ai or gpt-docs.h2o.ai\r\n\r\nYes, I see that, I cleared some space and restarted the underlying gpt.h2o.ai server.  Both seem ok now."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1906,
    "title": "Llama 3.2 vision and 3.3",
    "author": "chengchu88",
    "state": "open",
    "created_at": "2024-12-12T13:29:23Z",
    "updated_at": "2024-12-12T13:29:23Z",
    "labels": [],
    "body": "Hello sir,\r\nI am running h2ogpt 0.2.0, and I can deploy llama 3 with no issue, but got error when trying to deploy llama. 3.2 vision and llama 3.3. is there any solution to it? Does anyone have any success with h2ogpt 0.2.1?\r\n\r\nThanks\r\n",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1905,
    "title": "Collections Disappear",
    "author": "InesBenAmor99",
    "state": "closed",
    "created_at": "2024-11-28T16:17:49Z",
    "updated_at": "2024-12-03T07:12:37Z",
    "labels": [],
    "body": "Hello, I'm facing an issue with collections. When I create collections and then log out and log back in, the collections disappear. The same happens when I refresh the page. What could be the problem?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Ensure auth is enabled so that you can login as a specific user, else you have to login in the login page."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1600,
    "title": "random assertion errors due to evaluate_nochat",
    "author": "Blacksuan19",
    "state": "closed",
    "created_at": "2024-05-06T16:01:02Z",
    "updated_at": "2024-12-03T00:39:14Z",
    "labels": [],
    "body": "when using the docker image, I randomly get assertion errors when making a request from the gradio UI, sometimes it works and sometimes it does not, here is the raised error.\r\n\r\nthis occurs with the latest two docker images tagged `4059a2c9` and `7297519c`.\r\n\r\n<details>\r\n\r\n<summary>Full error</summary>\r\n\r\n```python\r\nthread exception: Traceback (most recent call last):                                                                                                                                                                                         File \"/workspace/src/utils.py\", line 502, in run                                                                                                                                                                                             self._return = self._target(*self._args, **self._kwargs)                                                                                                                                                                                 File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\r\n    return self.llm.generate_prompt(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\r\n    output = self._generate_helper(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\r\n    self._generate(\r\n  File \"/workspace/src/gpt_langchain.py\", line 2339, in _generate\r\n    rets = super()._generate(prompts, stop=stop, run_manager=run_manager, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 267, in _generate\r\n    responses = self.pipeline(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1223, in __call__\r\n    outputs = list(final_iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\r\n    item = next(self.iterator)\r\n File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\r\n    processed = self.infer(item, **self.params)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1149, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 271, in _forward\r\n    return self.__forward(model_inputs, **generate_kwargs)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 309, in __forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1576, in generate\r\n    result = self._greedy_search(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2494, in _greedy_search\r\n    outputs = self(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1158, in forward\r\n    outputs = self.model(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/model.py\", line 127, in forward\r\n    h, _, _ = layer(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/block.py\", line 123, in forward\r\n    attn_output, _, past_key_value = self.attn.forward(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 235, in forward\r\n    xq, xk = self.rope.forward(xq, xk, self.start_pos, seqlen)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 60, in forward\r\n    freqs_cis = self.reshape_for_broadcast(freqs_cis, xq_).to(xq_.device)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 48, in reshape_for_broadcast\r\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\r\nAssertionError\r\n\r\nmake stop: Traceback (most recent call last):\r\n  File \"/workspace/src/utils.py\", line 502, in run\r\n    self._return = self._target(*self._args, **self._kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\r\n    return self.llm.generate_prompt(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\r\n    output = self._generate_helper(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\r\n    self._generate(\r\n  File \"/workspace/src/gpt_langchain.py\", line 2339, in _generate\r\n    rets = super()._generate(prompts, stop=stop, run_manager=run_manager, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 267, in _generate\r\n    responses = self.pipeline(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1223, in __call__\r\n    outputs = list(final_iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\r\n    item = next(self.iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\r\n    processed = self.infer(item, **self.params)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1149, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 271, in _forward\r\n    return self.__forward(model_inputs, **generate_kwargs)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 309, in __forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1576, in generate\r\n    result = self._greedy_search(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2494, in _greedy_search\r\n    outputs = self(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1158, in forward\r\n    outputs = self.model(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/model.py\", line 127, in forward\r\n    h, _, _ = layer(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/block.py\", line 123, in forward\r\n    attn_output, _, past_key_value = self.attn.forward(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 235, in forward\r\n    xq, xk = self.rope.forward(xq, xk, self.start_pos, seqlen)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 60, in forward\r\n    freqs_cis = self.reshape_for_broadcast(freqs_cis, xq_).to(xq_.device)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 48, in reshape_for_broadcast\r\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\r\nAssertionError\r\n\r\nhit stop\r\nevaluate_nochat exception: : ('', '', '', True, 'open_chat', \"{   'PreInput': None,\\n    'PreInstruct': 'GPT4 User: ',\\n    'PreResponse': 'GPT4 Assistant:',\\n    'botstr': 'GPT4 Assistant:',\\n    'can_handle_system_prompt': False,\\n\r\n  'chat_sep': '<|end_of_turn|>',\\n    'chat_turn_sep': '<|end_of_turn|>',\\n    'generates_leading_space': False,\\n    'humanstr': 'GPT4 User: ',\\n    'promptA': '',\\n    'promptB': '',\\n    'system_prompt': '',\\n    'terminate_response\r\n': ['GPT4 Assistant:', '<|end_of_turn|>']}\", 0, 1, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0.0, True, '', '', 'UserData', True, 'Query', [], 10, True, 512, 'Relevant', ['/workspace/user_path/9b999f43-2ade-4148-97cf-d2448125168c/r\r\nes/e6a9ce98_user_upload_protocols.pdf'], [], [], [], [], 'Pay attention and remember the information below, which will help to answer the question or imperative after the context ends.', 'According to only the information in the docume\r\nnt sources provided within the context above, write an insightful and well-structured response to: ', 'In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text.', 'Using only the inform\r\nation in the document sources above, write a condensed and concise summary of key results (preferably as about 10 bullet points).', 'Answer this question with vibrant details in order for some NLP embedding model to use that answer as\r\nbetter query than original question: ', 'Who are you and what do you do?', 'Ensure your entire response is outputted as a single piece of strict valid JSON text.', 'Ensure your response is strictly valid JSON text.', 'Ensure your entir\r\ne response is outputted as strict valid JSON text inside a Markdown code block with the json language identifier.   Ensure all JSON keys are less than 64 characters, and ensure JSON key names are made of only alphanumerics, underscores\r\n, or hyphens.', 'Ensure you follow this JSON schema:\\n```json\\n{properties_schema}\\n```', 'auto', ['OCR', 'DocTR', 'Caption', 'ASR'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', [], [], '', False, '[]', '[]', 'best_near_prompt', 51\r\n2, -1.0, -1.0, 'split_or_merge', '\\n\\n', 0, 'auto', False, False, '[]', 'None', None, [], 1.0, None, None, 'text', '', '', '', '', {'model': 'model', 'tokenizer': 'tokenizer', 'device': 'cuda', 'base_model': 'TheBloke/openchat_3.5-16k-\r\nAWQ', 'tokenizer_base_model': '', 'lora_weights': '[]', 'inference_server': '[]', 'prompt_type': 'open_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': 'GPT4 User: ', 'PreInput': None, 'PreResponse': 'GPT4 Assistant:\r\n', 'terminate_response': ['GPT4 Assistant:', '<|end_of_turn|>'], 'chat_sep': '<|end_of_turn|>', 'chat_turn_sep': '<|end_of_turn|>', 'humanstr': 'GPT4 User: ', 'botstr': 'GPT4 Assistant:', 'generates_leading_space': False, 'system_promp\r\nt': '', 'can_handle_system_prompt': False}, 'visible_models': 0, 'h2ogpt_key': None}, {'MyData': [None, '90711427-650d-458e-ac69-bc1629b452be', 'test']}, {'langchain_modes': ['Disabled', 'LLM', 'UserData'], 'langchain_mode_paths': {'Us\r\nerData': '/workspace/user_path/'}, 'langchain_mode_types': {'UserData': 'shared', 'github h2oGPT': 'shared', 'DriverlessAI docs': 'shared', 'wiki': 'shared', 'wiki_full': '', 'LLM': 'personal', 'Disabled': 'personal'}}, {'headers': '',\r\n 'host': '0.0.0.0:7850', 'username': 'test', 'connection': 'Upgrade', 'pragma': 'no-cache', 'cache-control': 'no-cache', 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/\r\n537.36 Edg/124.0.0.0', 'upgrade': 'websocket', 'origin': 'http://0.0.0.0:7850', 'sec-websocket-version': '13', 'accept-encoding': 'gzip, deflate', 'accept-language': 'en-US,en;q=0.9,ar;q=0.8', 'cookie': 'access-token-unsecure-hhN8p\r\ny5JLVRfL-0OTPND8TGcb3qhs2GvSJQ8qV1LI50=vrLRNuXKqoKCZDSCqo1OHg; access-token-unsecure-s-dRx26Pws-xf2TfvaYIjqwWsGjiH9960S06PrlT6tg=AnrezJi1hR1NjfFx29n_bg; access-token-unsecure-SF0CZ7POfi6Imk0jDfN44qO9W9VB0hu3nUcGevVPMYw=SU1SQYZL79hpAN43\r\nhEDgIQ; access-token-unsecure-9LIDZewsE4If1yY7ixHa-yOZJO20M-PQVSDjJtfYQYA=o8YMAhHGtoLQDjMVZVITsQ; access-token-unsecure-qS0zsQdPdQYJsrMX4RXh3HQwEDeknaNz0RppngdPvGY=AGmuVQm8_KVKkMg8HdQtqg; access-token-unsecure--qfFGcbj-JQc0O0MamjIfNGlf\r\ngUrb6t7xyB3hRUL1I8=NVbKjP5O7Q3xJxHYvaiUfw; access-token-unsecure-YeY4iDfE2-hlA1izGtL7vBNbLbCosRLpSAJFo-j6_e0=xkWJTIiCTZGbhG1H60OTBg; access-token-unsecure-BwVTmtTwIzOYqtTpvsZkHQvnjr8N60WJaX_V6njwUAw=8uPW51j557W7S8ZO_e5iSQ', 'sec-websoc\r\nket-key': 'CS4lXFJi7AM2jwkdWyhKyQ==', 'sec-websocket-extensions': 'permessage-deflate; client_max_window_bits', 'host2': '14.1.206.49', 'picture': 'None'}, {}, [['summarize the given document', '']])\r\n\r\n``` \r\n\r\n\r\n</details>\r\n\r\n\r\n<details>\r\n\r\n<summary>Docker command</summary>\r\n\r\n\r\nused command to run h2ogpt\r\n\r\n```bash\r\n\r\nexport CONTEXT_LENGTH=16384\r\nexport IMAGE_TAG=7297519c\r\n\r\ndocker run \\\r\n --init \\\r\n--gpus all \\\r\n--runtime=nvidia \\\r\n--shm-size=2g \\\r\n-p 7850:7860 \\\r\n-v /etc/passwd:/etc/passwd:ro \\\r\n-v /etc/group:/etc/group:ro \\\r\n-u $(id -u):$(id -g) \\\r\ngcr.io/vorvan/h2oai/h2ogpt-runtime:$IMAGE_TAG /workspace/generate.py \\\r\n--page_title=\"GenNet AI\" \\\r\n--favicon_path=\"/workspace/assets/gennet_logo.svg\" \\\r\n--height=700 \\\r\n--gradio_size=\"medium\" \\\r\n--enable_heap_analytics=False \\\r\n--document_choice_in_sidebar=True \\\r\n--actions_in_sidebar=True \\\r\n--openai_server=False \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--prompt_type=open_chat \\\r\n--base_model=TheBloke/openchat_3.5-16k-AWQ \\\r\n--compile_model=True \\\r\n--use_cache=True \\\r\n--use_flash_attention_2=True \\\r\n--attention_sinks=True \\\r\n--sink_dict=\"{'num_sink_tokens': 4, 'window_length': $CONTEXT_LENGTH }\" \\\r\n--save_dir='/workspace/save/' \\\r\n--user_path='/workspace/user_path/' \\\r\n--langchain_mode=\"UserData\" \\\r\n--langchain_modes=\"['UserData', 'LLM']\" \\\r\n--visible_langchain_actions=\"['Query']\" \\\r\n--visible_langchain_agents=\"[]\" \\\r\n--use_llm_if_no_docs=True \\\r\n--max_seq_len=$CONTEXT_LENGTH \\\r\n--enable_ocr=True \\\r\n--enable_tts=False \\\r\n--enable_stt=False\r\n\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, I see the issue is from awq:\r\n```\r\nFile \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 48, in reshape_for_broadcast\r\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\r\n```\r\n\r\nIt's likely a bug in awq, perhaps when combined with attention sinks, flash attention, or compile of model.  While we expose those options from transformers, I cannot be sure arbitrary combinations work.\r\n\r\n\r\nIf I run:\r\n```\r\npython generate.py \\\r\n--enable_heap_analytics=False \\\r\n--document_choice_in_sidebar=True \\\r\n--actions_in_sidebar=True \\\r\n--openai_server=False \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--prompt_type=open_chat \\\r\n--base_model=TheBloke/openchat_3.5-16k-AWQ \\\r\n--compile_model=True \\\r\n--use_cache=True \\\r\n--use_flash_attention_2=True \\\r\n--attention_sinks=True \\\r\n--sink_dict=\"{'num_sink_tokens': 4, 'window_length': 16384 }\" \\\r\n--use_llm_if_no_docs=True \\\r\n--max_seq_len=16384 \\\r\n--enable_ocr=True\r\n```\r\n\r\nI don't have a generic issue running.  I removed things that shouldn't be relevant to the awq issue.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/a14a70d2-eacb-4345-96be-c23ee2377d8b)\r\n\r\nHowever, when I upload some text and then ask a question, I get the same issue:\r\n\r\n```\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\r\n  warn_deprecated(\r\nthread exception: Traceback (most recent call last):\r\n  File \"/home/jon/h2ogpt/src/utils.py\", line 502, in run\r\n    self._return = self._target(*self._args, **self._kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\r\n    return self.llm.generate_prompt(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\r\n    output = self._generate_helper(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\r\n    raise e\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\r\n    self._generate(\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 2339, in _generate\r\n    rets = super()._generate(prompts, stop=stop, run_manager=run_manager, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 267, in _generate\r\n    responses = self.pipeline(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1223, in __call__\r\n    outputs = list(final_iterator)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\r\n    item = next(self.iterator)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\r\n    processed = self.infer(item, **self.params)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1149, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File \"/home/jon/h2ogpt/src/h2oai_pipeline.py\", line 271, in _forward\r\n    return self.__forward(model_inputs, **generate_kwargs)\r\n  File \"/home/jon/h2ogpt/src/h2oai_pipeline.py\", line 309, in __forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1576, in generate\r\n    result = self._greedy_search(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2494, in _greedy_search\r\n    outputs = self(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1158, in forward\r\n    outputs = self.model(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/model.py\", line 119, in forward\r\n    h, _, past_key_value = layer(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/block.py\", line 113, in forward\r\n    attn_output, _, past_key_value = self.attn.forward(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 210, in forward\r\n    xq, xk = self.rope.forward(xq, xk, self.start_pos, seqlen)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 62, in forward\r\n    freqs_cis = self.reshape_for_broadcast(freqs_cis, xq_).to(xq_.device)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 50, in reshape_for_broadcast\r\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\r\nAssertionError\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "This does the same thing:\r\n```\r\npython generate.py \\\r\n--enable_heap_analytics=False \\\r\n--document_choice_in_sidebar=True \\\r\n--actions_in_sidebar=True \\\r\n--openai_server=False \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--prompt_type=open_chat \\\r\n--base_model=TheBloke/openchat_3.5-16k-AWQ \\\r\n--attention_sinks=True \\\r\n--sink_dict=\"{'num_sink_tokens': 4, 'window_length': 16384 }\" \\\r\n--use_llm_if_no_docs=True \\\r\n--max_seq_len=16384 \\\r\n--enable_ocr=True\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "As does this:\r\n```\r\npython generate.py \\\r\n--enable_heap_analytics=False \\\r\n--document_choice_in_sidebar=True \\\r\n--actions_in_sidebar=True \\\r\n--openai_server=False \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--prompt_type=open_chat \\\r\n--base_model=TheBloke/openchat_3.5-16k-AWQ \\\r\n--use_llm_if_no_docs=True \\\r\n--max_seq_len=16384 \\\r\n--enable_ocr=True\r\n```\r\n\r\nSo it seems to be a pure awq issue.\r\n\r\nThe latest 0.2.5 does the same thing.  Reducing to (say) 15000 does same thing."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1904,
    "title": "RAG ",
    "author": "CommitAndPray",
    "state": "open",
    "created_at": "2024-11-27T16:04:01Z",
    "updated_at": "2024-11-27T16:04:01Z",
    "labels": [],
    "body": "Hello, could someone please guide me to where I can find the exact implementation of the RAG part?",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1903,
    "title": "Multiple collections simultaneously",
    "author": "oumaymajr",
    "state": "open",
    "created_at": "2024-11-26T10:27:24Z",
    "updated_at": "2024-11-26T10:27:24Z",
    "labels": [],
    "body": "Hello , i was looking if there are any update about this suggestion ? i need to test if it works with multiple collections simultaneously .",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1867,
    "title": "Newly created Collection available for all the users when authentication is enabled",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-10-03T11:58:52Z",
    "updated_at": "2024-11-16T03:27:35Z",
    "labels": [],
    "body": "When authentication is enabled, I want to create a shared collection, such as `UserData`, that remains accessible to all users at all times, regardless of who created it.\r\n\r\nI encountered an issue when attempting to create a new collection (`UserData2`). Here’s the process I followed:\r\n\r\n1. I first created the collection using the UI.\r\n2. I then ran the following command:\r\n\r\n`python generate.py --base_model=meta-llama/Llama-2-13b-chat-hf --score_model=None --langchain_modes=\"['UserData',UserData2,LLM,MyData]\" --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --max_new_tokens=2048 --min_new_tokens=128 --prompt_type=llama2 --enable_stt=False --enable_tts=False --auth_filename=$auth_filename --auth_access=open --guest_name=avser --auth=\"[(admin, admin)]\"`\r\n\r\nThe issue is that the newly created collection (`UserData2`) is only accessible to the user who created it, similar to how `MyData` functions. However, I would like this collection to behave like `UserData`, where it is available to all users globally, even when authentication is enabled.\r\n\r\nHowever When authentication is disabled during collection creation, the collection is accessible to all users as expected.\r\n\r\nso Could you please provide guidance on how to create a shared collection like `UserData` that remains accessible to all users, even when authentication is enabled?\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "How did you create it in the UI?  The box takes a few args, including the collection type (assumed to be personal if no passed)."
      },
      {
        "user": "llmwesee",
        "body": "Document Selection >> Add Collection then type `UserData2, shared, userpath`  then put `UserData2` in the\r\n `--langchain_modes=\"['UserData',UserData2,LLM,MyData]\"`"
      },
      {
        "user": "llmwesee",
        "body": "However i also created the collection with **src/make_db.py:**  by adding all the files in the folder **user_path3** then \r\n`python src/make_db.py --user_path=user_path3 --collection_name=UserData3 --langchain_type=shared ` \r\n\r\n`python generate.py --base_model=meta-llama/Llama-2-13b-chat-hf --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --batch_size=16 --prompt_type=llama2 --langchain_modes=['UserData','UserData3','MyData','LLM'] --auth_filename=$auth_filename --auth_access=open --guest_name=avser --auth=\"[(admin, admin)]\"\r\n`\r\n\r\nthen still didn't showing UserData3 in the collections for all the users although the embedding are stored in **db_dir_UserData3** folder\r\n\r\nAnd when adding `--langchain_modes=['UserData','UserData3'] --langchain_mode_paths={'UserData':'user_path','UserData3':'user_path3'} --langchain_mode_types={'UserData':'shared','UserData3':'shared'}` \r\n\r\nto the command like:\r\n\r\n`python generate.py --base_model=meta-llama/Llama-2-13b-chat-hf --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --batch_size=16 --prompt_type=llama2 --langchain_modes=['UserData','UserData3'] --langchain_mode_paths={'UserData':'user_path','UserData3':'user_path3'} --langchain_mode_types={'UserData':'shared','UserData3':'shared'} --auth_filename=$auth_filename --auth_access=open --guest_name=avser --auth=\"[(admin, admin)]\"`\r\n\r\nthen it showing the following error:\r\n\r\n`File \"/home/xxxx/src/gen.py\", line 1383, in main\r\n    langchain_mode_paths = str_to_dict(langchain_mode_paths)\r\n  File \"/home/xxxx/src/utils.py\", line 1863, in str_to_dict\r\n    raise ValueError(\"Invalid str_to_dict for %s\" % x)\r\nValueError: Invalid str_to_dict for UserData3:user_path3`\r\n\r\nNote:  I created the authentication server through the **LDAP**\r\n@pseudotensor please help me regarding this!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1896,
    "title": "taking long time to give response (around 2 min)",
    "author": "mbbutt",
    "state": "open",
    "created_at": "2024-11-07T12:31:59Z",
    "updated_at": "2024-11-12T21:48:14Z",
    "labels": [],
    "body": "Hello\r\n\r\nI am running in the following machine.\r\n\r\nCPU: 12th Gen Intel(R) Core(TM) i7-12700\r\nRAM: 32GB, speed: 4400MT/s\r\nNVIDIA RTX A2000 12GB\r\n\r\nmodel is:\r\nllama-2-7b-chat.Q6_K.gguf\r\n\r\nAnd it takes around 2 min to start giving a response.\r\nis it reasonable or it should be faster?\r\n\r\n\r\nbat command to start the bot\r\n\r\n```\r\n\"C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\Scripts\\python.exe\"^\r\n \"generate.py\"^\r\n --share=False ^\r\n --auth=[('jon','password')] ^\r\n --auth_access=closed ^\r\n --gradio_offline_level=1 ^\r\n --base_model=\"llama\" ^\r\n --prompt_type=llama2 ^\r\n --model_path_llama=C:\\Users\\Public\\git\\h2ogpt\\llama-2-7b-chat.Q6_K.gguf^\r\n --score_model=None ^\r\n --langchain_mode=\"LLLM\" ^\r\n --user_path=user_path ^\r\n --load_4bit=True ^\r\n --llamacpp_dict=\"{'n_gpu_layers':5}\"\r\n\r\n```\r\n\r\nWhile running idle \r\nit is taking 7GB GPU memory (remains same when running the query)\r\n24.4GB RAM (remains same when running the query)\r\nCPU utilization stays 2 to 3%\r\n\r\n\r\nWhen running the query CPU utilization goes closer to 100%\r\nGPU remains 1% to 2%\r\n\r\nand it takes around 2 min to start giving a response.\r\n\r\nIt seems it is not utilizing GPU at all.\r\ncould you please see what i am doing wrong here?\r\nI want to get faster response \r\n\r\ncuda version is\r\n\r\n```\r\nC:\\Windows\\System32>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2022 NVIDIA Corporation\r\nBuilt on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\r\nCuda compilation tools, release 11.8, V11.8.89\r\nBuild cuda_11.8.r11.8/compiler.31833905_0\r\n```\r\n\r\n\r\n\r\n\r\nbelow is my pip list\r\n\r\n```\r\nPackage                                  Version\r\n---------------------------------------- ---------------\r\nabsl-py                                  2.1.0\r\naccelerate                               0.32.1\r\naiofiles                                 23.2.1\r\naiohappyeyeballs                         2.4.3\r\naiohttp                                  3.10.9\r\naiosignal                                1.3.1\r\naltair                                   5.4.1\r\nannotated-types                          0.7.0\r\nanthropic                                0.8.1\r\nantlr4-python3-runtime                   4.9.3\r\nanyio                                    4.6.0\r\nappdirs                                  1.4.4\r\nAPScheduler                              3.10.4\r\nargcomplete                              3.5.1\r\narxiv                                    1.4.8\r\nasgiref                                  3.8.1\r\nasync-timeout                            4.0.3\r\nattributedict                            0.3.0\r\nattrs                                    24.2.0\r\naudioread                                3.0.1\r\nAuthlib                                  1.3.1\r\nauto-gptq                                0.6.0\r\nautoawq                                  0.1.8+cu118\r\nautoawq_kernels                          0.0.3+cu118\r\nbabel                                    2.16.0\r\nbackoff                                  2.2.1\r\nbackports.tarfile                        1.2.0\r\nbcrypt                                   4.2.0\r\nbeautifulsoup4                           4.12.3\r\nbioc                                     2.1\r\nbitsandbytes                             0.41.1\r\nblessings                                1.7\r\nboto3                                    1.35.35\r\nbotocore                                 1.35.35\r\nBrotli                                   1.1.0\r\nbs4                                      0.0.2\r\nbuild                                    1.2.2.post1\r\ncachetools                               5.5.0\r\ncertifi                                  2024.8.30\r\ncffi                                     1.17.1\r\nchardet                                  5.2.0\r\ncharset-normalizer                       3.3.2\r\nchroma-bullet                            2.2.0\r\nchroma-hnswlib                           0.7.3\r\nchroma-migrate                           0.0.7\r\nchromadb                                 0.4.23\r\nchromamigdb                              0.3.26\r\nclick                                    8.1.7\r\nclickhouse-connect                       0.6.6\r\ncodecov                                  2.1.13\r\ncolorama                                 0.4.6\r\ncoloredlogs                              15.0.1\r\ncolour-runner                            0.1.1\r\ncontourpy                                1.3.0\r\ncoverage                                 7.6.1\r\ncryptography                             43.0.1\r\ncssselect2                               0.7.0\r\ncutlet                                   0.3.0\r\ncycler                                   0.12.1\r\ndacite                                   1.7.0\r\ndataclasses-json                         0.6.7\r\nDataProperty                             1.0.1\r\ndatasets                                 2.16.1\r\ndateparser                               1.1.8\r\ndecorator                                5.1.1\r\ndeepdiff                                 8.0.1\r\ndefusedxml                               0.7.1\r\nDeprecated                               1.2.14\r\ndiffusers                                0.24.0\r\ndill                                     0.3.7\r\ndiskcache                                5.6.3\r\ndistlib                                  0.3.8\r\ndistro                                   1.9.0\r\ndnspython                                2.7.0\r\ndocopt                                   0.6.2\r\ndocutils                                 0.20.1\r\nduckdb                                   0.7.1\r\nduckduckgo_search                        6.3.0\r\ndurationpy                               0.9\r\neffdet                                   0.4.1\r\neinops                                   0.8.0\r\nemoji                                    2.14.0\r\net-xmlfile                               1.1.0\r\neval_type_backport                       0.2.0\r\nevaluate                                 0.4.0\r\nexceptiongroup                           1.2.2\r\nexecnet                                  2.1.1\r\nexllama                                  0.0.18+cu118\r\nfastapi                                  0.115.0\r\nfeedparser                               6.0.11\r\nffmpeg                                   1.4\r\nffmpy                                    0.4.0\r\nfiftyone                                 1.0.0\r\nfiftyone-brain                           0.17.0\r\nfiftyone_db                              1.1.6\r\nfilelock                                 3.16.1\r\nfiletype                                 1.2.0\r\nfire                                     0.5.0\r\nflatbuffers                              24.3.25\r\nfonttools                                4.54.1\r\nfrozenlist                               1.4.1\r\nfsspec                                   2023.10.0\r\nftfy                                     6.2.3\r\nfugashi                                  1.3.2\r\nfuture                                   1.0.0\r\ng2pkk                                    0.1.2\r\ngekko                                    1.2.1\r\nglob2                                    0.7\r\ngoogle-ai-generativelanguage             0.4.0\r\ngoogle-api-core                          2.20.0\r\ngoogle-auth                              2.35.0\r\ngoogle-generativeai                      0.3.2\r\ngoogle_search_results                    2.4.2\r\ngoogleapis-common-protos                 1.65.0\r\ngpt4all                                  1.0.5\r\ngradio                                   3.50.2\r\ngradio_client                            0.6.1\r\ngradio_pdf                               0.0.15\r\ngradio_tools                             0.0.9\r\ngraphql-core                             3.2.4\r\ngreenlet                                 3.0.3\r\ngrpcio                                   1.66.2\r\ngrpcio-health-checking                   1.62.3\r\ngrpcio-status                            1.62.3\r\ngrpcio-tools                             1.62.3\r\ngruut                                    2.2.3\r\ngruut-ipa                                0.13.0\r\ngruut-lang-de                            2.0.1\r\ngruut-lang-en                            2.0.1\r\ngruut-lang-es                            2.0.1\r\ngruut_lang_fr                            2.0.2\r\nh11                                      0.14.0\r\nh2                                       4.1.0\r\nh5py                                     3.12.1\r\nhf_transfer                              0.1.8\r\nhnswlib                                  0.8.0\r\nhnswmiglib                               0.7.0\r\nhpack                                    4.0.0\r\nhtml2text                                2024.2.26\r\nhtml5lib                                 1.1\r\nhttpcore                                 1.0.6\r\nhttptools                                0.6.1\r\nhttpx                                    0.27.0\r\nhuggingface-hub                          0.25.1\r\nhumanfriendly                            10.0\r\nhumanize                                 4.11.0\r\nHypercorn                                0.17.3\r\nhyperframe                               6.0.1\r\nidna                                     3.10\r\nimageio                                  2.35.1\r\nimportlib_metadata                       8.4.0\r\nimportlib_resources                      6.4.5\r\nimutils                                  0.5.4\r\ninflate64                                1.0.0\r\niniconfig                                2.0.0\r\ninspecta                                 0.1.3\r\nInstructorEmbedding                      1.0.1\r\nintervaltree                             3.1.0\r\niopath                                   0.1.10\r\njaconv                                   0.4.0\r\njamo                                     0.4.1\r\njaraco.context                           6.0.1\r\njieba                                    0.42.1\r\nJinja2                                   3.1.4\r\njiter                                    0.6.1\r\njmespath                                 1.0.1\r\njoblib                                   1.4.2\r\njsonlines                                1.2.0\r\njsonpatch                                1.33\r\njsonpath-python                          1.0.6\r\njsonpointer                              3.0.0\r\njsonschema                               4.23.0\r\njsonschema-specifications                2024.10.1\r\nkaleido                                  0.2.1\r\nkiwisolver                               1.4.7\r\nkubernetes                               31.0.0\r\nlangchain                                0.0.354\r\nlangchain-community                      0.0.8\r\nlangchain-core                           0.1.6\r\nlangchain-experimental                   0.0.47\r\nlangchain-google-genai                   0.0.6\r\nlangchain-mistralai                      0.0.2\r\nlangdetect                               1.0.9\r\nlangid                                   1.1.6\r\nlangsmith                                0.0.77\r\nlayoutparser                             0.3.4\r\nlazy_loader                              0.4\r\nlibrosa                                  0.10.1\r\nllama_cpp_python                         0.2.26+cpuavx2\r\nllama_cpp_python_cuda                    0.2.26+cu121avx\r\nllvmlite                                 0.43.0\r\nlm-dataformat                            0.0.20\r\nlm_eval                                  0.4.4\r\nloralib                                  0.1.2\r\nlxml                                     5.3.0\r\nlz4                                      4.3.3\r\nMarkdown                                 3.7\r\nmarkdown-it-py                           3.0.0\r\nMarkupSafe                               2.1.5\r\nmarshmallow                              3.22.0\r\nmatplotlib                               3.9.2\r\nmbstrdecoder                             1.1.3\r\nmdurl                                    0.1.2\r\nmistralai                                0.0.8\r\nmmh3                                     5.0.1\r\nmojimoji                                 0.0.13\r\nmongoengine                              0.24.2\r\nmonotonic                                1.6\r\nmore-itertools                           10.5.0\r\nmotor                                    3.5.3\r\nmplcursors                               0.5.3\r\nmpmath                                   1.3.0\r\nmsg-parser                               1.2.0\r\nmsgpack                                  1.1.0\r\nmultidict                                6.1.0\r\nmultiprocess                             0.70.15\r\nmultivolumefile                          0.2.3\r\nmutagen                                  1.47.0\r\nmypy-extensions                          1.0.0\r\nnarwhals                                 1.9.1\r\nnest-asyncio                             1.6.0\r\nnetworkx                                 2.8.8\r\nnltk                                     3.9.1\r\nnum2words                                0.5.13\r\nnumba                                    0.60.0\r\nnumexpr                                  2.10.1\r\nnumpy                                    1.23.4\r\noauthlib                                 3.2.2\r\nolefile                                  0.47\r\nomegaconf                                2.3.0\r\nonnx                                     1.17.0\r\nonnxruntime                              1.15.1\r\nonnxruntime-gpu                          1.15.0\r\nopenai                                   1.51.2\r\nopencv-python                            4.10.0.84\r\nopencv-python-headless                   4.10.0.84\r\nopenpyxl                                 3.1.5\r\nopentelemetry-api                        1.27.0\r\nopentelemetry-exporter-otlp-proto-common 1.27.0\r\nopentelemetry-exporter-otlp-proto-grpc   1.27.0\r\nopentelemetry-instrumentation            0.48b0\r\nopentelemetry-instrumentation-asgi       0.48b0\r\nopentelemetry-instrumentation-fastapi    0.48b0\r\nopentelemetry-proto                      1.27.0\r\nopentelemetry-sdk                        1.27.0\r\nopentelemetry-semantic-conventions       0.48b0\r\nopentelemetry-util-http                  0.48b0\r\nopenvino                                 2022.3.0\r\noptimum                                  1.16.1\r\norderly-set                              5.2.2\r\norjson                                   3.10.7\r\noutcome                                  1.3.0.post0\r\noverrides                                7.7.0\r\npackaging                                24.1\r\npandas                                   2.0.2\r\npathvalidate                             3.2.1\r\npdf2image                                1.17.0\r\npdfminer.six                             20221105\r\npdfplumber                               0.10.4\r\npeft                                     0.13.1\r\npikepdf                                  9.3.0\r\npillow                                   10.4.0\r\npillow_heif                              0.18.0\r\npip                                      23.0.1\r\npip-licenses                             5.0.0\r\nplatformdirs                             4.3.6\r\nplaywright                               1.47.0\r\nplotly                                   5.24.1\r\npluggy                                   1.5.0\r\npooch                                    1.8.2\r\nportalocker                              2.10.1\r\nposthog                                  3.7.0\r\npprintpp                                 0.4.0\r\nprettytable                              3.11.0\r\nprimp                                    0.6.3\r\npriority                                 2.0.0\r\npropcache                                0.2.0\r\nproto-plus                               1.24.0\r\nprotobuf                                 4.25.5\r\npsutil                                   6.0.0\r\npulsar-client                            3.5.0\r\npy7zr                                    0.22.0\r\npyarrow                                  17.0.0\r\npyarrow-hotfix                           0.6\r\npyasn1                                   0.6.1\r\npyasn1_modules                           0.4.1\r\npybcj                                    1.0.2\r\npybind11                                 2.13.6\r\npyclipper                                1.3.0.post5\r\npycocotools                              2.0.8\r\npycparser                                2.22\r\npycryptodomex                            3.21.0\r\npydantic                                 2.9.2\r\npydantic_core                            2.23.4\r\npydantic-settings                        2.1.0\r\npydash                                   8.0.3\r\npydub                                    0.25.1\r\npydyf                                    0.11.0\r\npyee                                     12.0.0\r\nPygments                                 2.18.0\r\npymongo                                  4.8.0\r\nPyMuPDF                                  1.24.11\r\npynvml                                   11.5.3\r\npypandoc                                 1.14\r\npypandoc_binary                          1.14\r\npyparsing                                3.1.4\r\npypdf                                    5.0.1\r\npypdfium2                                4.30.0\r\npyphen                                   0.16.0\r\nPyPika                                   0.48.9\r\npyppmd                                   1.1.0\r\npyproject-api                            1.8.0\r\npyproject_hooks                          1.2.0\r\npyreadline3                              3.5.4\r\nPySocks                                  1.7.1\r\npytablewriter                            1.2.0\r\npytesseract                              0.3.13\r\npytest                                   8.3.3\r\npytest-xdist                             3.6.1\r\npython-crfsuite                          0.9.11\r\npython-dateutil                          2.8.2\r\npython-doctr                             0.5.4a0\r\npython-docx                              1.1.2\r\npython-dotenv                            1.0.1\r\npython-iso639                            2024.4.27\r\npython-magic                             0.4.27\r\npython-magic-bin                         0.4.14\r\npython-multipart                         0.0.12\r\npython-pptx                              0.6.23\r\npytube                                   15.0.0\r\npytz                                     2024.2\r\npywin32                                  307\r\nPyYAML                                   6.0.2\r\npyzstd                                   0.16.1\r\nRapidFuzz                                3.10.0\r\nrarfile                                  4.2\r\nreferencing                              0.35.1\r\nregex                                    2024.9.11\r\nreplicate                                0.20.0\r\nrequests                                 2.32.3\r\nrequests-file                            2.1.0\r\nrequests-oauthlib                        2.0.0\r\nrequests-toolbelt                        1.0.0\r\nresponses                                0.18.0\r\nretrying                                 1.3.4\r\nrich                                     13.9.2\r\nrootpath                                 0.1.1\r\nrouge                                    1.0.1\r\nrouge_score                              0.1.2\r\nrpds-py                                  0.20.0\r\nrsa                                      4.9\r\nruff                                     0.6.9\r\ns3transfer                               0.10.2\r\nsacrebleu                                2.3.1\r\nsafetensors                              0.4.5\r\nscikit-image                             0.24.0\r\nscikit-learn                             1.2.2\r\nscipy                                    1.13.1\r\nselenium                                 4.25.0\r\nsemantic-version                         2.10.0\r\nsemanticscholar                          0.8.4\r\nsentence-transformers                    2.2.2\r\nsentencepiece                            0.1.99\r\nsetuptools                               65.5.0\r\nsgmllib3k                                1.0.0\r\nShapely                                  1.8.5.post1\r\nshellingham                              1.5.4\r\nsix                                      1.16.0\r\nsniffio                                  1.3.1\r\nsortedcontainers                         2.4.0\r\nsoundfile                                0.12.1\r\nsoupsieve                                2.6\r\nsoxr                                     0.5.0.post1\r\nSQLAlchemy                               2.0.35\r\nsqlitedict                               2.1.0\r\nsse-starlette                            0.10.3\r\nsseclient-py                             1.8.0\r\nstarlette                                0.38.6\r\nstrawberry-graphql                       0.246.0\r\nsympy                                    1.13.3\r\ntabledata                                1.3.3\r\ntabulate                                 0.9.0\r\ntaskgroup                                0.0.0a4\r\ntcolorpy                                 0.1.6\r\ntenacity                                 8.5.0\r\ntermcolor                                2.5.0\r\ntext-generation                          0.7.0\r\ntextstat                                 0.7.4\r\ntexttable                                1.7.0\r\nthreadpoolctl                            3.5.0\r\ntifffile                                 2024.9.20\r\ntiktoken                                 0.8.0\r\ntimm                                     1.0.9\r\ntinycss2                                 1.3.0\r\ntokenizers                               0.19.1\r\ntoml                                     0.10.2\r\ntomli                                    2.0.2\r\ntomlkit                                  0.12.0\r\ntorch                                    2.1.2+cu118\r\ntorchvision                              0.16.2+cu118\r\ntox                                      4.21.2\r\ntqdm                                     4.66.5\r\ntqdm-multiprocess                        0.0.11\r\ntransformers                             4.40.2\r\ntrio                                     0.26.2\r\ntrio-websocket                           0.11.1\r\ntypepy                                   1.3.2\r\ntyper                                    0.12.5\r\ntyping_extensions                        4.12.2\r\ntyping-inspect                           0.9.0\r\ntzdata                                   2024.2\r\ntzlocal                                  5.2\r\nujson                                    5.10.0\r\nUnidecode                                1.3.8\r\nuniversal-analytics-python3              1.1.1\r\nunstructured                             0.12.5\r\nunstructured-client                      0.26.0\r\nunstructured-inference                   0.7.23\r\nunstructured.pytesseract                 0.3.13\r\nurllib3                                  2.2.3\r\nuvicorn                                  0.31.0\r\nvalidators                               0.34.0\r\nvirtualenv                               20.26.6\r\nvoxel51-eta                              0.13.0\r\nwatchfiles                               0.24.0\r\nwavio                                    0.0.8\r\nwcwidth                                  0.2.13\r\nweasyprint                               62.3\r\nweaviate-client                          4.8.1\r\nwebencodings                             0.5.1\r\nwebsocket-client                         1.8.0\r\nwebsockets                               11.0.3\r\nwikipedia                                1.4.0\r\nwolframalpha                             5.1.3\r\nword2number                              1.1\r\nwrapt                                    1.16.0\r\nwsproto                                  1.2.0\r\nxlrd                                     2.0.1\r\nXlsxWriter                               3.2.0\r\nxmltodict                                0.13.0\r\nxxhash                                   3.5.0\r\nyarl                                     1.14.0\r\nyt-dlp                                   2023.10.13\r\nzipp                                     3.20.2\r\nzopfli                                   0.2.3\r\nzstandard                                0.23.0\r\n```",
    "comments": [
      {
        "user": "mbbutt",
        "body": "below is the log if it may help.\r\n\r\n```\r\n\r\n-----\r\n\r\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting\r\n\r\n-----\r\n\r\n\r\n-----\r\n\r\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting\r\n\r\n-----\r\n\r\nSTT enabled, may use more GPU, set --enable_stt=False for low-memory systems\r\nTTS enabled, may use more GPU, set --enable_tts=False for low-memory systems\r\nUsing Model llama\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nStarting get_model: llama\r\nFailed to listen to n_gpus: Failed to load shared library 'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll': Could not find module 'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax.\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q6_K:  226 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 5.15 GiB (6.56 BPW)\r\nllm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size       =    0.11 MiB\r\nllm_load_tensors: system memory used  = 5272.45 MiB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_build_graph: non-view tensors processed: 676/676\r\nllama_new_context_with_model: compute buffer total size = 42.19 MiB\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\r\nAuto-detected LLaMa n_ctx=4096, will unload then reload with this setting.\r\nwarning: failed to VirtualUnlock buffer: The segment is already unlocked.\r\n\r\nFailed to listen to n_gpus: Failed to load shared library 'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll': Could not find module 'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax.\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q6_K:  226 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 5.15 GiB (6.56 BPW)\r\nllm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size       =    0.11 MiB\r\nllm_load_tensors: system memory used  = 5272.45 MiB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 4096\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\r\nllama_build_graph: non-view tensors processed: 676/676\r\nllama_new_context_with_model: compute buffer total size = 75.19 MiB\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\r\nModel {'base_model': 'llama', 'base_model0': 'llama', 'tokenizer_base_model': '', 'lora_weights': '', 'inference_server': '', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'visible_models': None, 'h2ogpt_key': None, 'load_8bit': 'pause', 'load_4bit': True, 'low_bit_mode': 1, 'load_half': True, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': True, 'gpu_id': 0, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 5, 'n_gqa': 0, 'model_path_llama': 'C:\\\\Users\\\\Public\\\\git\\\\h2ogpt\\\\llamacpp_path\\\\llama-2-7b-chat.Q6_K.gguf', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': '', 'n_batch': 128}, 'rope_scaling': {}, 'max_seq_len': 4096, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\r\nBegin auto-detect HF cache text generation models\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nNo loading model openai/whisper-base.en because is_encoder_decoder=True\r\nNo loading model microsoft/speecht5_hifigan because The checkpoint you are trying to load has model type `hifigan` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\r\nNo loading model microsoft/speecht5_tts because is_encoder_decoder=True\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models\r\nEnd auto-detect llama.cpp models\r\nC:\\Users\\Public\\git\\h2ogpt\\gradio_utils\\prompt_form.py:211: GradioUnusedKwargWarning: You have unused kwarg parameters in Chatbot, please remove them: {'likeable': True}\r\n  text_output = gr.Chatbot(label=output_label0,\r\nC:\\Users\\Public\\git\\h2ogpt\\gradio_utils\\prompt_form.py:216: GradioUnusedKwargWarning: You have unused kwarg parameters in Chatbot, please remove them: {'likeable': True}\r\n  text_output2 = gr.Chatbot(label=output_label0_model2,\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStarted Gradio Server and/or GUI: server_name: localhost port: None\r\nUse local URL: http://localhost:7860/\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in ModelInfoResponse has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_names\" in ModelListResponse has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nOpenAI API URL: http://0.0.0.0:5000\r\nOpenAI API key: EMPTY\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "The logs don't mention using the GPU, which is probably why it's slow.  Something wrong with the llamacpp_python installation."
      },
      {
        "user": "mbbutt",
        "body": "Hello \r\nThanks for the response.\r\ni can see in pip list, both lama_cpp (for CPU) and llama_cpp (for cuda) are installed\r\ncould this be the reason?  \r\n\r\n```\r\nllama_cpp_python                         0.2.26+cpuavx2\r\nllama_cpp_python_cuda                    0.2.26+cu121avx\r\n\r\n```\r\n\r\nshould i uninstall or get some different version?\r\n\r\n\r\nthis is the issue\r\n\r\n```\r\nFailed to listen to n_gpus: Failed to load shared library\r\n'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll': Could not find module\r\n'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax.\r\n\r\n\r\n```\r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1899,
    "title": "ModuleNotFoundError: No module named 'langchain_openai.chat_models'; 'langchain_openai' is not a package",
    "author": "ricvolpi",
    "state": "closed",
    "created_at": "2024-11-12T10:33:57Z",
    "updated_at": "2024-11-12T18:58:16Z",
    "labels": [],
    "body": "Running `python src/make_db.py --download_some=True` I get the following error\r\n\r\n```unix\r\nTraceback (most recent call last):\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/make_db.py\", line 9, in <module>\r\n    from gpt_langchain import path_to_docs, get_some_dbs_from_hf, all_db_zips, some_db_zips, create_or_update_db, \\\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/gpt_langchain.py\", line 2077, in <module>\r\n    from langchain_together import ChatTogether\r\n  File \"/home/jupyter/conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_together/__init__.py\", line 1, in <module>\r\n    from langchain_together.chat_models import ChatTogether\r\n  File \"/home/jupyter/conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_together/chat_models.py\", line 18, in <module>\r\n    from langchain_openai.chat_models.base import BaseChatOpenAI\r\nModuleNotFoundError: No module named 'langchain_openai.chat_models'; 'langchain_openai' is not a package\r\n```\r\n\r\nthe reason is the langchain_openai wrapper `src/langchain_openai.py` (see a similar issue [here](https://github.com/langchain-ai/langchain/issues/2079#issuecomment-1487416187)). I fixed it by renaming `src/langchain_openai.py` into `src/langchain_openai_wrapper.py` and updating related imports. Things work now, but maybe we can think about a cleaner fix for this? I wonder if the other wrappers cause other issues here and there.\r\n\r\nOS Linux / Debian 11\r\nPython 10.3",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's already been renamed last week to langchain_openai_local.py .  You must be out of date on repo. Thanks!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1898,
    "title": "pydantic.errors.PydanticUserError: A non-annotated attribute was detected: `allow_map_1 = True`",
    "author": "ricvolpi",
    "state": "closed",
    "created_at": "2024-11-12T08:42:03Z",
    "updated_at": "2024-11-12T09:50:03Z",
    "labels": [],
    "body": "I have installed packages in `requirements.txt` and `reqs_optional/requirements_optional_gpu_only.txt`.\r\n\r\nWhen I run `python src/make_db.py --download_some=True`, I get\r\n\r\n```unix\r\nTraceback (most recent call last):\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/make_db.py\", line 9, in <module>\r\n    from gpt_langchain import path_to_docs, get_some_dbs_from_hf, all_db_zips, some_db_zips, create_or_update_db, \\\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/gpt_langchain.py\", line 97, in <module>\r\n    from h2o_serpapi import H2OSerpAPIWrapper\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/h2o_serpapi.py\", line 8, in <module>\r\n    from utils_langchain import _chunk_sources, add_parser, _add_meta\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/utils_langchain.py\", line 396, in <module>\r\n    class H2OMapReduceDocumentsChain(MapReduceDocumentsChain):\r\n  File \"/home/jupyter/conda/envs/h2o/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py\", line 115, in __new__\r\n    private_attributes = inspect_namespace(\r\n  File \"/home/jupyter/conda/envs/h2o/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py\", line 428, in inspect_namespace\r\n    raise PydanticUserError(\r\npydantic.errors.PydanticUserError: A non-annotated attribute was detected: `allow_map_1 = True`. All model fields require a type annotation; if `allow_map_1` is not meant to be a field, you may be able to resolve this error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\r\n```\r\n\r\nAm I missing any dependency?\r\n\r\nOS: Debian GNU/Linux 11.\r\nPython version: 3.10\r\nPackages:\r\n\r\n```unix\r\nabsl-py                                  2.1.0\r\naccelerate                               1.1.1\r\naiofiles                                 23.2.1\r\naiohappyeyeballs                         2.4.3\r\naiohttp                                  3.10.10\r\naiosignal                                1.3.1\r\nannotated-types                          0.7.0\r\nanthropic                                0.39.0\r\nanyio                                    4.6.2.post1\r\nappdirs                                  1.4.4\r\nAPScheduler                              3.10.4\r\nasgiref                                  3.8.1\r\nasync-timeout                            4.0.3\r\nattrs                                    24.2.0\r\nauto_gptq                                0.7.1\r\nautoawq                                  0.2.6\r\nautoawq_kernels                          0.0.7\r\nbackoff                                  2.2.1\r\nbcrypt                                   4.2.0\r\nbeautifulsoup4                           4.12.3\r\nbioc                                     2.1\r\nbitsandbytes                             0.44.1\r\nblis                                     0.7.11\r\nboto3                                    1.35.58\r\nbotocore                                 1.35.58\r\nbuild                                    1.2.2.post1\r\ncachetools                               5.5.0\r\ncatalogue                                2.0.10\r\ncertifi                                  2024.8.30\r\ncharset-normalizer                       3.4.0\r\nchroma-hnswlib                           0.7.6\r\nchromadb                                 0.5.18\r\nclick                                    8.1.7\r\ncloudpathlib                             0.20.0\r\ncolorama                                 0.4.6\r\ncoloredlogs                              15.0.1\r\nconfection                               0.1.5\r\ncontourpy                                1.3.0\r\ncycler                                   0.12.1\r\ncymem                                    2.0.8\r\ndataclasses-json                         0.6.7\r\ndatasets                                 3.1.0\r\ndefusedxml                               0.7.1\r\nDeprecated                               1.2.14\r\ndill                                     0.3.8\r\ndistro                                   1.9.0\r\ndocopt                                   0.6.2\r\ndocutils                                 0.21.2\r\ndurationpy                               0.9\r\neinops                                   0.8.0\r\net_xmlfile                               2.0.0\r\nevaluate                                 0.4.3\r\nexceptiongroup                           1.2.2\r\nexecnet                                  2.1.1\r\nexllama                                  0.0.18+cu121\r\nfaiss-gpu                                1.7.2\r\nfastapi                                  0.115.4\r\nfastapi-utils                            0.8.0\r\nffmpy                                    0.4.0\r\nfilelock                                 3.16.1\r\nfire                                     0.7.0\r\nflatbuffers                              24.3.25\r\nfonttools                                4.54.1\r\nfrozenlist                               1.5.0\r\nfsspec                                   2024.9.0\r\ngekko                                    1.2.1\r\ngoogle-ai-generativelanguage             0.6.10\r\ngoogle-api-core                          2.23.0\r\ngoogle-api-python-client                 2.151.0\r\ngoogle-auth                              2.36.0\r\ngoogle-auth-httplib2                     0.2.0\r\ngoogle-generativeai                      0.8.3\r\ngoogleapis-common-protos                 1.65.0\r\ngradio                                   4.44.0\r\ngradio_client                            1.3.0\r\ngreenlet                                 3.1.1\r\ngrpcio                                   1.67.1\r\ngrpcio-status                            1.67.1\r\ngunicorn                                 23.0.0\r\nh11                                      0.14.0\r\nhf_transfer                              0.1.8\r\nhttpcore                                 1.0.6\r\nhttplib2                                 0.22.0\r\nhttptools                                0.6.4\r\nhttpx                                    0.27.2\r\nhttpx-sse                                0.4.0\r\nhuggingface-hub                          0.26.2\r\nhumanfriendly                            10.0\r\nidna                                     3.10\r\nimportlib_metadata                       8.5.0\r\nimportlib_resources                      6.4.5\r\niniconfig                                2.0.0\r\nInstructorEmbedding                      1.0.1\r\nintervaltree                             3.1.0\r\nJinja2                                   3.1.4\r\njiter                                    0.7.0\r\njmespath                                 1.0.1\r\njoblib                                   1.4.2\r\njson_repair                              0.30.1\r\njsonlines                                4.0.0\r\njsonpatch                                1.33\r\njsonpointer                              3.0.0\r\njsonschema                               4.23.0\r\njsonschema-specifications                2024.10.1\r\nkiwisolver                               1.4.7\r\nkubernetes                               31.0.0\r\nlangchain                                0.3.4\r\nlangchain-anthropic                      0.2.4\r\nlangchain-chroma                         0.1.4\r\nlangchain-community                      0.3.3\r\nlangchain-core                           0.3.13\r\nlangchain-experimental                   0.3.3\r\nlangchain-google-genai                   2.0.4\r\nlangchain-mistralai                      0.2.0\r\nlangchain-ollama                         0.2.0\r\nlangchain-text-splitters                 0.3.0\r\nlangcodes                                3.4.1\r\nlangsmith                                0.1.137\r\nlanguage_data                            1.2.0\r\nlimits                                   3.13.0\r\nlm-dataformat                            0.0.20\r\nloralib                                  0.1.2\r\nlxml                                     5.3.0\r\nmarisa-trie                              1.2.1\r\nMarkdown                                 3.7\r\nmarkdown-it-py                           3.0.0\r\nMarkupSafe                               2.1.5\r\nmarshmallow                              3.23.1\r\nmatplotlib                               3.9.2\r\nmdurl                                    0.1.2\r\nmmh3                                     5.0.1\r\nmonotonic                                1.6\r\nmpmath                                   1.3.0\r\nmultidict                                6.1.0\r\nmultiprocess                             0.70.16\r\nmurmurhash                               1.0.10\r\nmypy-extensions                          1.0.0\r\nnetworkx                                 3.4.2\r\nnltk                                     3.9.1\r\nnumpy                                    1.26.4\r\nnvidia-cublas-cu12                       12.1.3.1\r\nnvidia-cuda-cupti-cu12                   12.1.105\r\nnvidia-cuda-nvrtc-cu12                   12.1.105\r\nnvidia-cuda-runtime-cu12                 12.1.105\r\nnvidia-cudnn-cu12                        8.9.2.26\r\nnvidia-cufft-cu12                        11.0.2.54\r\nnvidia-curand-cu12                       10.3.2.106\r\nnvidia-cusolver-cu12                     11.4.5.107\r\nnvidia-cusparse-cu12                     12.1.0.106\r\nnvidia-nccl-cu12                         2.20.5\r\nnvidia-nvjitlink-cu12                    12.6.77\r\nnvidia-nvtx-cu12                         12.1.105\r\noauthlib                                 3.2.2\r\nollama                                   0.3.3\r\nonnxruntime                              1.20.0\r\nonnxruntime-gpu                          1.15.0\r\nopenai                                   1.54.3\r\nopenpyxl                                 3.1.5\r\nopentelemetry-api                        1.28.1\r\nopentelemetry-exporter-otlp-proto-common 1.28.1\r\nopentelemetry-exporter-otlp-proto-grpc   1.28.1\r\nopentelemetry-instrumentation            0.49b1\r\nopentelemetry-instrumentation-asgi       0.49b1\r\nopentelemetry-instrumentation-fastapi    0.49b1\r\nopentelemetry-proto                      1.28.1\r\nopentelemetry-sdk                        1.28.1\r\nopentelemetry-semantic-conventions       0.49b1\r\nopentelemetry-util-http                  0.49b1\r\norjson                                   3.10.11\r\noverrides                                7.7.0\r\npackaging                                24.2\r\npandas                                   2.2.3\r\npeft                                     0.13.2\r\npillow                                   10.4.0\r\npip                                      24.3.1\r\npluggy                                   1.5.0\r\nportalocker                              2.10.1\r\nposthog                                  3.7.0\r\npreshed                                  3.0.9\r\npropcache                                0.2.0\r\nproto-plus                               1.25.0\r\nprotobuf                                 5.28.3\r\npsutil                                   5.9.8\r\npyarrow                                  18.0.0\r\npyasn1                                   0.6.1\r\npyasn1_modules                           0.4.1\r\npydantic                                 2.9.2\r\npydantic_core                            2.23.4\r\npydantic-settings                        2.6.1\r\npydub                                    0.25.1\r\npyexiv2                                  2.15.3\r\nPygments                                 2.18.0\r\npynvml                                   11.5.3\r\npypandoc_binary                          1.14\r\npyparsing                                3.2.0\r\npyphen                                   0.17.0\r\nPyPika                                   0.48.9\r\npyproject_hooks                          1.2.0\r\npytest                                   8.3.3\r\npytest-xdist                             3.6.1\r\npython-dateutil                          2.9.0.post0\r\npython-dotenv                            1.0.1\r\npython-multipart                         0.0.17\r\npytz                                     2024.2\r\nPyYAML                                   6.0.2\r\nreferencing                              0.35.1\r\nregex                                    2024.11.6\r\nrequests                                 2.32.3\r\nrequests-oauthlib                        2.0.0\r\nrequests-toolbelt                        1.0.0\r\nrich                                     13.9.4\r\nrouge                                    1.0.1\r\nrouge_score                              0.1.2\r\nrpds-py                                  0.21.0\r\nrsa                                      4.9\r\nruff                                     0.7.3\r\ns3transfer                               0.10.3\r\nsacrebleu                                2.4.3\r\nsafetensors                              0.4.5\r\nscikit-learn                             1.5.2\r\nscipy                                    1.14.1\r\nsemantic-version                         2.10.0\r\nsentence-transformers                    3.3.0\r\nsentence-transformers-old                2.2.2\r\nsentencepiece                            0.2.0\r\nsetuptools                               75.3.0\r\nshellingham                              1.5.4\r\nsix                                      1.16.0\r\nslowapi                                  0.1.9\r\nsmart-open                               7.0.5\r\nsniffio                                  1.3.1\r\nsortedcontainers                         2.4.0\r\nsoupsieve                                2.6\r\nspacy                                    3.7.5\r\nspacy-legacy                             3.0.12\r\nspacy-loggers                            1.0.5\r\nSQLAlchemy                               2.0.35\r\nsrsly                                    2.4.8\r\nsse-starlette                            2.1.3\r\nstarlette                                0.41.2\r\nsympy                                    1.13.3\r\ntabulate                                 0.9.0\r\ntenacity                                 9.0.0\r\ntermcolor                                2.5.0\r\ntext-generation                          0.7.0\r\ntextstat                                 0.7.4\r\nthinc                                    8.2.5\r\nthreadpoolctl                            3.5.0\r\ntiktoken                                 0.8.0\r\ntokenizers                               0.20.3\r\ntomli                                    2.1.0\r\ntomlkit                                  0.12.0\r\ntorch                                    2.3.1\r\ntorchvision                              0.18.1\r\ntqdm                                     4.67.0\r\ntransformers                             4.46.2\r\ntriton                                   2.3.1\r\ntyper                                    0.13.0\r\ntyping_extensions                        4.12.2\r\ntyping-inspect                           0.9.0\r\ntzdata                                   2024.2\r\ntzlocal                                  5.2\r\nujson                                    5.10.0\r\nuritemplate                              4.1.1\r\nurllib3                                  2.2.3\r\nuvicorn                                  0.32.0\r\nuvloop                                   0.21.0\r\nwasabi                                   1.1.3\r\nwatchfiles                               0.24.0\r\nweasel                                   0.4.1\r\nwebsocket-client                         1.8.0\r\nwebsockets                               12.0\r\nwheel                                    0.45.0\r\nwrapt                                    1.16.0\r\nxxhash                                   3.5.0\r\nyarl                                     1.17.1\r\nzipp                                     3.21.0\r\nzstandard                                0.23.0\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "ricvolpi",
        "body": "My bad, just found proper install instructions [here](https://github.com/h2oai/h2ogpt/blob/main/docs/linux_install.sh)."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1889,
    "title": "Role-Based Login for collections addone or removal Management:",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-10-29T12:07:58Z",
    "updated_at": "2024-11-10T20:40:53Z",
    "labels": [],
    "body": "Hi,\r\nRequest for Assistance in the following important aspect:\r\n\r\n1. I need help with the  **Role-Based Login for collections addone or removal Management:**\r\nI’d like to implement a role-based login system to restrict the Add/Delete functionality for specific folders to admin users only. Could you provide guidance on how to set this up?\r\n2. I just simply want only admin can control the **Document Selection** tab. Except chat all tabs should be hidden for the users. Only admin have access all the tabs and add or remove collection functionality.\r\n\r\nThe above issue affecting the implementation.  Request specific links/ suggestions please.\r\n\r\nThanks and Warm regards",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's certainly possible, but one would have to dig into the code and see what the best way is for exactly what you want to do.  It could be quite simple or more complex."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1897,
    "title": "Automation of file upload and embedding generation for MyData collection through api",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-11-08T12:06:22Z",
    "updated_at": "2024-11-08T12:06:22Z",
    "labels": [],
    "body": "When I'm uploading files through the '/upload_api' and '/add_file_api' then they upload the files in the user_path and embedding are automatically generated in the vector store. However, the same i'm not able to replicate in for **MyData** collection. \r\n\r\nBelow is the sample code for uploading files in **UserData** through api:\r\n\r\n`import os\r\nimport time\r\nfrom tqdm import tqdm\r\nfrom gradio_client import Client\r\n\r\nclass DocumentUploader:\r\n    def __init__(self, host_url: str, api_key: str = 'EMPTY'):\r\n        self.client = Client(host_url)\r\n        self.api_key = api_key\r\n\r\n    def upload_document(self, local_file_path: str) -> str:\r\n        \"\"\"Uploads a document to the server and returns the server file path.\"\"\"\r\n        with tqdm(total=100, desc=f\"Uploading {os.path.basename(local_file_path)}\", unit='%', ncols=80) as pbar:\r\n            _, server_file_path = self.client.predict(local_file_path, api_name='/upload_api')\r\n            pbar.update(100)\r\n        return server_file_path\r\n\r\n    def add_document_and_ocr(self, server_file_path: str, loaders: list) -> dict:\r\n        \"\"\"Adds the document to the server with OCR processing.\"\"\"\r\n        with tqdm(total=100, desc=f\"Processing {os.path.basename(server_file_path)}\", unit='%', ncols=80) as pbar:\r\n            res = self.client.predict(\r\n                server_file_path, \"UserData\", True, 512, True, *loaders, api_name='/add_file_api'\r\n            )\r\n            pbar.update(100)\r\n        print(\"Document processing completed and is ready for querying.\")\r\n        return res\r\n\r\n    def process_all_files_in_folder(self, folder_path: str):\r\n        \"\"\"Process all PDF files in the specified folder.\"\"\"\r\n        pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\r\n        if not pdf_files:\r\n            print(\"No PDF files found in the folder.\")\r\n            return\r\n\r\n        print(f\"Found {len(pdf_files)} PDF files. Starting upload and processing...\")\r\n\r\n        loaders = [None] * 6  # Adjust loaders as needed\r\n        for pdf_file in pdf_files:\r\n            file_path = os.path.join(folder_path, pdf_file)\r\n            server_file_path = self.upload_document(file_path)\r\n            self.add_document_and_ocr(server_file_path, loaders)`\r\n\r\nPlease help me for automate the same in the **MyData** collection! ",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1895,
    "title": "ImportError: cannot import name 'cached_download' from 'huggingface_hub'",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-11-06T13:48:55Z",
    "updated_at": "2024-11-06T20:47:11Z",
    "labels": [],
    "body": "Commit: f7971ea1 --> lastest at the time to write this issue in the main branch\r\nThe environment was created launching the script: ```bash docs/linux_install_full.sh```\r\nThe error was: ```cannot import name 'cached_download'```\r\nSolution found in this issue: ```https://github.com/FunAudioLLM/CosyVoice/issues/516```\r\nTo solve execute the following command line: ```pip install huggingface-hub==0.25.2``` in the conda environment for downgrading\r\n",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1894,
    "title": "ModuleNotFoundError: No module named 'langchain_openai.chat_models'; 'langchain_openai' is not a package",
    "author": "Vidminas",
    "state": "closed",
    "created_at": "2024-11-04T15:02:12Z",
    "updated_at": "2024-11-05T19:37:33Z",
    "labels": [],
    "body": "I installed h2ogpt locally on Windows following the guide at <https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md>.\r\nFollowing the docs in <https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md>, I was trying to set up a user database by running `python src/make_db.py --add_if_exists=True --user_path=userdata` (where `userdata` is a directory I created).\r\nIt results in the error:\r\n```shell\r\n$ python src/make_db.py --add_if_exists=True --user_path=userdata\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vidminas\\GitHub\\h2ogpt\\src\\make_db.py\", line 9, in <module>\r\n    from gpt_langchain import path_to_docs, get_some_dbs_from_hf, all_db_zips, some_db_zips, create_or_update_db, \\\r\n  File \"C:\\Users\\Vidminas\\GitHub\\h2ogpt\\src\\gpt_langchain.py\", line 2076, in <module>\r\n    from langchain_together import ChatTogether\r\n  File \"C:\\dev\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_together\\__init__.py\", line 3, in <module>\r\n    from langchain_together.chat_models import ChatTogether\r\n  File \"C:\\dev\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_together\\chat_models.py\", line 18, in <module>\r\n    from langchain_openai.chat_models.base import BaseChatOpenAI\r\nModuleNotFoundError: No module named 'langchain_openai.chat_models'; 'langchain_openai' is not a package\r\n```\r\n\r\nDigging into this, I found that it's caused by the file `src/langchain_openai.py` which overrides the `langchain-openai` package visibility. This file should be renamed to something else.",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1883,
    "title": "Getting functional server error",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-10-22T10:22:38Z",
    "updated_at": "2024-10-24T23:12:04Z",
    "labels": [],
    "body": "### Description\r\nHi, I'm encountering an issue while attempting to use a functional server to upload files through an API. Specifically, when I run the server with the following command, I receive an error related to connection establishment. However, if I run the server without enabling the functional server, it works as expected.\r\n\r\nPlease help me to solve the issue.\r\n\r\nThank You\r\n\r\n### Command\r\n\r\n```\r\n python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct  --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --visible_visible_models=False --max_seq_len=8192 --max_max_new_tokens=4096 --max_new_tokens=4096 --min_new_tokens=256 --api_open=True --allow_api=True --max_quality=True  --function_server=True --function_server_workers=5 --multiple_workers_gunicorn=True --function_server_port=5002 --function_api_key=API_KEY\r\n ```\r\n \r\n \r\n### Issue Details\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connection.py\", line 196, in _new_conn\r\n    sock = connection.create_connection(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\r\n    raise err\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection\r\n    sock.connect(sa)\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\r\n    response = self._make_request(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 495, in _make_request\r\n    conn.request(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connection.py\", line 398, in request\r\n    self.endheaders()\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connection.py\", line 236, in connect\r\n    self.sock = self._new_conn()\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connection.py\", line 211, in _new_conn\r\n    raise NewConnectionError(\r\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7de388ea7010>: Failed to establish a new connection: [Errno 111] Connection refused\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\r\n    resp = conn.urlopen(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\r\n    retries = retries.increment(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/util/retry.py\", line 519, in increment\r\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='0.0.0.0', port=5002): Max retries exceeded with url: /execute_function/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7de388ea7010>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/versha/Documents/narad050824_pmdemo_mobile_lake_apitest/weseecore/src/gpt_langchain.py\", line 9383, in update_user_db\r\n    return _update_user_db(file, db1s=db1s,\r\n  File \"/home/versha/Documents/narad050824_pmdemo_mobile_lake_apitest/weseecore/src/gpt_langchain.py\", line 9664, in _update_user_db\r\n    sources = call_function_server('0.0.0.0', function_server_port, 'path_to_docs', (file,), simple_kwargs,\r\n  File \"/home/versha/Documents/narad050824_pmdemo_mobile_lake_apitest/weseecore/src/function_client.py\", line 50, in call_function_server\r\n    execute_result = execute_function_on_server(host, port, function_name, args, kwargs, use_disk, use_pickle,\r\n  File \"/home/versha/Documents/narad050824_pmdemo_mobile_lake_apitest/weseecore/src/function_client.py\", line 21, in execute_function_on_server\r\n    response = requests.post(url, json=payload, headers=headers)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/api.py\", line 115, in post\r\n    return request(\"post\", url, data=data, json=json, **kwargs)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/adapters.py\", line 700, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='0.0.0.0', port=5002): Max retries exceeded with url: /execute_function/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7de388ea7010>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n```\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "That means the client side can't reach the function server.  You can check the start-up shows the 5002 port started, and you can look at the openai_logs/* function server related files to see if present\r\n\r\ni.e. they may look like:\r\n\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ ls -alrt openai_logs/\r\ntotal 224\r\n-rw-------  1 jon jon     0 Oct 20 10:00 gunicorn_OpenAI_d9ffa2da-638d-4d78-a66b-2ddf28748f4a_stdout.log\r\n-rw-------  1 jon jon     0 Oct 20 10:00 gunicorn_Function_c8db57d7-aa31-494f-b4a6-01cc810663ed_stdout.log\r\n-rw-------  1 jon jon  1154 Oct 20 10:00 gunicorn_OpenAI_d9ffa2da-638d-4d78-a66b-2ddf28748f4a_stderr.log\r\n-rw-------  1 jon jon  1154 Oct 20 10:00 gunicorn_Function_c8db57d7-aa31-494f-b4a6-01cc810663ed_stderr.log\r\n-rw-------  1 jon jon   299 Oct 20 15:48 gunicorn_OpenAI_110a8b60-9f67-4045-ad93-4fa75b21ff44_stdout.log\r\n-rw-------  1 jon jon 11486 Oct 20 15:48 gunicorn_Agent_f900440d-2f0f-4b81-a2b0-0b44dfd6231c_stdout.log\r\n-rw-------  1 jon jon 21009 Oct 20 15:56 gunicorn_OpenAI_110a8b60-9f67-4045-ad93-4fa75b21ff44_stderr.log\r\n-rw-------  1 jon jon 20392 Oct 20 15:56 gunicorn_Agent_f900440d-2f0f-4b81-a2b0-0b44dfd6231c_stderr.log\r\n-rw-------  1 jon jon   167 Oct 20 15:58 gunicorn_OpenAI_2a853fe3-bc2b-4069-a3a0-7ab4f3346568_stdout.log\r\n-rw-------  1 jon jon 11486 Oct 20 15:58 gunicorn_Agent_a3da0ca9-6eef-4f01-b5d6-0cff2d36c92c_stdout.log\r\n-rw-------  1 jon jon 19283 Oct 20 16:00 gunicorn_OpenAI_2a853fe3-bc2b-4069-a3a0-7ab4f3346568_stderr.log\r\n-rw-------  1 jon jon 20073 Oct 20 16:00 gunicorn_Agent_a3da0ca9-6eef-4f01-b5d6-0cff2d36c92c_stderr.log\r\n-rw-------  1 jon jon     0 Oct 20 16:01 gunicorn_OpenAI_96b3c872-136c-45f8-a0f2-c9a228887872_stdout.log\r\n-rw-------  1 jon jon     0 Oct 20 16:01 gunicorn_Agent_5e81accd-e122-4890-902f-dddccbcc6edb_stdout.log\r\ndrwx------  2 jon jon  4096 Oct 20 16:01 ./\r\n-rw-------  1 jon jon 15287 Oct 20 16:02 gunicorn_OpenAI_96b3c872-136c-45f8-a0f2-c9a228887872_stderr.log\r\n-rw-------  1 jon jon 15923 Oct 20 16:02 gunicorn_Agent_5e81accd-e122-4890-902f-dddccbcc6edb_stderr.log\r\ndrwxrwxr-x 83 jon jon 61440 Oct 23 12:51 ../\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```\r\nover multiple runs (new file each run).  \r\n\r\ne.g. this would be a bad startup:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ cat openai_logs/gunicorn_Function_c8db57d7-aa31-494f-b4a6-01cc810663ed_stderr.log\r\n[2024-10-20 10:00:37 -0700] [907130] [INFO] Starting gunicorn 23.0.0\r\n[2024-10-20 10:00:37 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:37 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:38 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:38 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:39 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:39 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:40 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:40 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:41 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:41 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:42 -0700] [907130] [ERROR] Can't connect to ('0.0.0.0', 5002)\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```\r\n\r\nWhile a good output would be:\r\n\r\n```\r\n(h2ogpte) jon@pseudotensor:~/h2ogpte/docker_data/h2ogpt/docker_logs/h2ogpt_openai$ tail -10 gunicorn_Function_296f27f4-e7ae-4284-85e2-5fd7a741b25c_stdout.log\r\ngit_hash: cf74d576ecfca0e24cac27588b52a4701dd7cb1d\r\nvisible_models: ['meta-llama/Meta-Llama-3.1-8B-Instruct']\r\nvisible_vision_models: ['mistralai/Pixtral-12B-2409']\r\nCommand: /usr/bin/gunicorn -w 5 -k uvicorn.workers.UvicornWorker --timeout 60 -b 0.0.0.0:5002 openai_server.function_server:app\r\nHash: cf74d576ecfca0e24cac27588b52a4701dd7cb1d\r\nPrep: persist_directory=db_dir_UserData exists, using\r\nPrep: persist_directory=db_dir_UserData exists, using\r\nPrep: persist_directory=db_dir_UserData exists, using\r\nPrep: persist_directory=db_dir_UserData exists, using\r\nPrep: persist_directory=db_dir_UserData exists, using\r\n(h2ogpte) jon@pseudotensor:~/h2ogpte/docker_data/h2ogpt/docker_logs/h2ogpt_openai$ tail -10 gunicorn_Function_296f27f4-e7ae-4284-85e2-5fd7a741b25c_stderr.log \r\n[2024-10-23 22:37:58 +0000] [198] [INFO] Application startup complete.\r\n[2024-10-23 22:37:58 +0000] [200] [INFO] Started server process [200]\r\n[2024-10-23 22:37:58 +0000] [200] [INFO] Waiting for application startup.\r\n[2024-10-23 22:37:58 +0000] [200] [INFO] Application startup complete.\r\n[2024-10-23 22:37:58 +0000] [201] [INFO] Started server process [201]\r\n[2024-10-23 22:37:58 +0000] [201] [INFO] Waiting for application startup.\r\n[2024-10-23 22:37:58 +0000] [201] [INFO] Application startup complete.\r\n[2024-10-23 22:37:58 +0000] [197] [INFO] Started server process [197]\r\n[2024-10-23 22:37:58 +0000] [197] [INFO] Waiting for application startup.\r\n[2024-10-23 22:37:58 +0000] [197] [INFO] Application startup complete.\r\n(h2ogpte) jon@pseudotensor:~/h2ogpte/docker_data/h2ogpt/docker_logs/h2ogpt_openai$ \r\n\r\n```\r\n "
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1873,
    "title": "request for combining multiple chromadb  ",
    "author": "mbbutt",
    "state": "open",
    "created_at": "2024-10-09T09:44:25Z",
    "updated_at": "2024-10-24T23:11:55Z",
    "labels": [],
    "body": "I would like to request an enhancement to that allows users to select multiple chromadb databases simultaneously (in the Resources section), rather than being restricted to one database at a time. Currently, the software only supports single database selection, which limits efficiency when users need to query or access documents across multiple datasets. The proposed enhancement includes updating the user interface to allow multi-database selection via checkbox list (or similar)",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1871,
    "title": "[HELM] Refactor Chart and include isolated agents",
    "author": "EshamAaqib",
    "state": "open",
    "created_at": "2024-10-07T09:39:24Z",
    "updated_at": "2024-10-24T23:11:46Z",
    "labels": [],
    "body": "# Description\r\n\r\nIt would be better to refactor the h2oGPT Helm chart as we did with the Enterprise version. Also we need to include isolated agents in the chart, so agents can run independently from h2oGPT OSS ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Should be optional as to whether agent pod is separate or not."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1864,
    "title": "Feature request: Include User ID, input prompt, and elapsed time in reviews.csv",
    "author": "yokomizo-tech",
    "state": "open",
    "created_at": "2024-10-03T02:11:51Z",
    "updated_at": "2024-10-24T23:11:25Z",
    "labels": [],
    "body": "The reviews.csv file is helpful, but it would be more useful if it included additional columns for User ID, input prompt, and elapsed time to generate a response, rather than just generated answers, reviews, and timestamps.",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1853,
    "title": "Upload API takes longer time than uploading through the UI.",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-09-25T08:46:49Z",
    "updated_at": "2024-10-24T23:11:17Z",
    "labels": [],
    "body": "The documentation provides sample code for uploading a file from the client side through the `upload_api`. However, I have observed that uploading files through this API takes longer than uploading through the UI. I would like to know the reason for this discrepancy and how I can make the process faster. Also, if multiple clients send files through this API, how is this handled?\r\n\r\n```\r\nimport os\r\nimport ast\r\nimport time\r\nfrom tqdm import tqdm\r\nfrom watchdog.observers import Observer\r\nfrom watchdog.events import FileSystemEventHandler\r\nfrom gradio_client import Client\r\n\r\nclass DocumentUploader:\r\n    def __init__(self, host_url: str, api_key: str = 'EMPTY'):\r\n        self.client = Client(host_url)\r\n        self.api_key = api_key\r\n\r\n    def upload_document(self, local_file_path: str) -> str:\r\n        \"\"\"Uploads a document to the server and returns the server file path with a progress bar.\"\"\"\r\n        with tqdm(total=100, desc=f\"Uploading {os.path.basename(local_file_path)}\", unit='%', ncols=80) as pbar:\r\n            _, server_file_path = self.client.predict(local_file_path, api_name='/upload_api')\r\n            pbar.update(100)\r\n        return server_file_path\r\n\r\n    def add_document_and_ocr(self, server_file_path: str, loaders: list) -> dict:\r\n        \"\"\"Adds the document to the server with OCR processing and shows progress.\"\"\"\r\n        with tqdm(total=100, desc=f\"Processing {os.path.basename(server_file_path)}\", unit='%', ncols=80) as pbar:\r\n            res = self.client.predict(\r\n                server_file_path, \"UserData\", True, 512, True, *loaders, api_name='/add_file_api'\r\n            )\r\n            pbar.update(100)\r\n        return res\r\n\r\n    def query_document(self, langchain_mode: str, instruction: str) -> str:\r\n        \"\"\"Queries the document based on given instructions and returns the response.\"\"\"\r\n        kwargs = dict(langchain_mode=langchain_mode, instruction=instruction)\r\n        res = self.client.predict(str(kwargs), api_name='/submit_nochat_api')\r\n        return ast.literal_eval(res)['response']\r\n\r\n    def process_file(self, file_path: str):\r\n        \"\"\"Processes a single file with progress display.\"\"\"\r\n        loaders = [\r\n            ['Caption', 'CaptionBlip2', 'Pix2Struct', 'OCR', 'DocTR'],\r\n            ['PyMuPDF', 'Unstructured', 'PyPDF', 'TryHTML', 'OCR'],\r\n            None, None\r\n        ]\r\n        print(f\"Processing single file: {file_path}\")\r\n        server_file_path = self.upload_document(file_path)\r\n        self.add_document_and_ocr(server_file_path, loaders)\r\n\r\n    def query_uploaded_documents(self, instruction: str):\r\n        \"\"\"Queries the already uploaded documents.\"\"\"\r\n        response = self.query_document(\"UserData\", instruction)\r\n        print(\"Query response:\", response)\r\n\r\n\r\nclass NewFileHandler(FileSystemEventHandler):\r\n    \"\"\"Event handler that triggers when a new file is added to the folder.\"\"\"\r\n    def __init__(self, uploader: DocumentUploader):\r\n        self.uploader = uploader\r\n\r\n    def on_created(self, event):\r\n        \"\"\"Triggered when a new file is created in the watched folder.\"\"\"\r\n        if not event.is_directory and event.src_path.endswith(\".pdf\"):\r\n            print(f\"New file detected: {event.src_path}\")\r\n            self.uploader.process_file(event.src_path)\r\n\r\n\r\ndef monitor_folder(folder_path: str, uploader: DocumentUploader):\r\n    \"\"\"Monitors the folder and triggers the uploader when new files are added.\"\"\"\r\n    event_handler = NewFileHandler(uploader)\r\n    observer = Observer()\r\n    observer.schedule(event_handler, folder_path, recursive=False)\r\n    observer.start()\r\n    print(f\"Monitoring folder: {folder_path}\")\r\n\r\n    try:\r\n        while True:\r\n            time.sleep(1)  # Keep the script running\r\n    except KeyboardInterrupt:\r\n        observer.stop()\r\n\r\n    observer.join()\r\n\r\n\r\n# Usage example\r\nhost_url = \"http://xx.xx.x.xx:7860/\" \r\nfolder_path = \"data\"\r\n\r\nuploader = DocumentUploader(host_url)\r\n\r\n# Start monitoring the folder\r\nmonitor_folder(folder_path, uploader)\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, maybe the API and UI are using different options by default for which handlers (e.g. doctr, unstructured, OCR, vision, etc.) are used.  Good to compare logs for each.\r\n\r\nNote that the API and UI use the same code and use gradio's unified way of generating API from UI itself."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1851,
    "title": "Clean install, nothing boots when i do 'python generate.py'",
    "author": "Domsmasher1",
    "state": "open",
    "created_at": "2024-09-23T10:40:11Z",
    "updated_at": "2024-10-24T23:11:07Z",
    "labels": [],
    "body": "When i try to run the start script i get this error message\r\n\r\n```\r\n(h2ogpt) C:\\Users\\domin\\Documents\\aiGen\\h2ogpt>python generate.py\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nWindows fatal exception: code 0xc0000139\r\n\r\nCurrent thread 0x000007e4 (most recent call first):\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 1176 in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 571 in module_from_spec\r\n  File \"<frozen importlib._bootstrap>\", line 674 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\modules\\linear.py\", line 4 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\base.py\", line 16 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\mpt.py\", line 1 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\__init__.py\", line 1 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\__init__.py\", line 2 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\mapping.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\auto.py\", line 32 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\__init__.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\trainer.py\", line 215 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1050 in _gcd_import\r\n  ...\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1603, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\trainer.py\", line 215, in <module>\r\n    from peft import PeftModel\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\__init__.py\", line 22, in <module>\r\n    from .auto import (\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\auto.py\", line 32, in <module>\r\n    from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\mapping.py\", line 22, in <module>\r\n    from peft.tuners.xlora.model import XLoraModel\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21, in <module>\r\n    from .lora import LoraConfig, LoraModel, LoftQConfig, LoraRuntimeConfig\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20, in <module>\r\n    from .model import LoraModel\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50, in <module>\r\n    from .awq import dispatch_awq\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\__init__.py\", line 2, in <module>\r\n    from awq.models.auto import AutoAWQForCausalLM\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\__init__.py\", line 1, in <module>\r\n    from .mpt import MptAWQForCausalLM\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\mpt.py\", line 1, in <module>\r\n    from .base import BaseAWQForCausalLM\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\base.py\", line 16, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\modules\\linear.py\", line 4, in <module>\r\n    import awq_inference_engine  # with CUDA kernels\r\nImportError: DLL load failed while importing awq_inference_engine: The specified procedure could not be found.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\src\\utils.py\", line 79, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\src\\gen.py\", line 2055, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\src\\gpt_langchain.py\", line 550, in get_embedding\r\n    embedding = HuggingFaceBgeEmbeddings(model_name=hf_embedding_model,\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py\", line 287, in __init__\r\n    import sentence_transformers\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\sentence_transformers\\__init__.py\", line 17, in <module>\r\n    from sentence_transformers.trainer import SentenceTransformerTrainer\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\sentence_transformers\\trainer.py\", line 13, in <module>\r\n    from transformers import EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1593, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1605, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\nDLL load failed while importing awq_inference_engine: The specified procedure could not be found.\r\n```\r\n\r\nI have gone though and followed eveything here https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md, and eveything else ran without issue. \r\n\r\nAny ideas?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Same issues as here:\r\n\r\nhttps://github.com/h2oai/h2ogpt/issues/1835\r\nhttps://github.com/h2oai/h2ogpt/issues/1561\r\n\r\nMaybe try loading model from UI, the other person said.\r\n\r\nIf anyone with windows can help figure it out would be great."
      },
      {
        "user": "Domsmasher1",
        "body": "The issue is i dont even get to a stage where the ui boots, it crashes before that stage, unlike the other two which seem to get to the ui stage"
      },
      {
        "user": "pseudotensor",
        "body": "Try what was done for ooba:\r\n\r\nhttps://github.com/oobabooga/text-generation-webui/issues/4253#issuecomment-1932400867\r\n\r\n```\r\npip uninstall autoawq\r\ngit clone https://github.com/casper-hansen/AutoAWQ\r\ncd AutoAWQ\r\npip install -e .\r\n```"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1848,
    "title": "Issue with Concurrent Query Processing and Document Upload",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-09-18T05:50:05Z",
    "updated_at": "2024-10-24T23:09:51Z",
    "labels": [],
    "body": "I have implemented a solution using vLLM on an A100 server to support multiple users. However, I have encountered an issue:\r\n\r\nWhile one user's query is being processed, other users are unable to upload documents into the `UserData` or MyData collections. The document upload process gets stuck at the processing stage without any errors appearing in the terminal or UI. Additionally, the document is not uploaded successfully.\r\n\r\nCan you suggest ways to decouple the query processing, document upload, and user interface programs so they can run independently of each other?\r\n\r\nAlternatively, can we build or use prebuilt separate APIs to manage program in the backend?\r\nPlease provide suggestions or potential solutions.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "They should all be independent unless you changed CONCURRENCY_COUNT to be 1.  This is tested normally.  The backend has no issues with this at all."
      },
      {
        "user": "pseudotensor",
        "body": "Once you have that working, I can explain how to make it even more efficient using the function_server."
      },
      {
        "user": "llmwesee",
        "body": "this is the command for running h2ogpt with login.\r\n`python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct  --score_model=None --langchain_mode='UserData' --user_path=user_path --auth='' --use_auth_token=True --visible_visible_models=False --max_seq_len=8192 --max_max_new_tokens=4096 --max_new_tokens=4096 --min_new_tokens=256\r\n`\r\ncan you show me some examples for having h2ogpt as fully backend server running with full functionality from query processing to document uploading for multiple users concurrently & independently . I want to integrated it's backend with react or next.js as frontend with having full functionality like as h2ogpt and having a datalake for all related document things "
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1678,
    "title": "Source Link opened in the same tab",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-06-11T06:24:53Z",
    "updated_at": "2024-10-22T04:53:58Z",
    "labels": [],
    "body": "After got the answer and go to the source and hit the source link then it opened in the same tab instead of new tab. By looking the code, The link should open in the new tab always instead of same tab because we are targeting the blank. Then is it a Bug or something else?\r\n\r\n```\r\ndef get_url(x, from_str=False, short_name=False, font_size=2):\r\n    if not from_str:\r\n        source = x.metadata['source']\r\n    else:\r\n        source = x\r\n    if short_name:\r\n        source_name = get_short_name(source)\r\n    else:\r\n        source_name = source\r\n    if source.startswith('http://') or source.startswith('https://'):\r\n        return \"\"\"<font size=\"%s\"><a href=\"%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n            font_size, source, source_name)\r\n    elif '<a href=' not in source:\r\n        return \"\"\"<font size=\"%s\"><a href=\"file/%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n            font_size, source, source_name)\r\n    else:\r\n        # already filled\r\n        return source\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Sorry for delay.\r\n\r\nI've never seen it open in same tab.  For me a PDF link leads to downloading of the PDF.\r\n\r\nFor images, it opens in new tab always for me.\r\n\r\nJust tried it again, seems all ok.\r\n\r\nCan you give a specific document and question where there is an issue?"
      },
      {
        "user": "llmwesee",
        "body": "I'm attaching the both PDF and also the screenshot for the reference. \r\n\r\n[Screencast from 28-06-24 09:45:42 AM IST.webm](https://github.com/h2oai/h2ogpt/assets/137979399/256c5ded-3f4d-4f68-a2dd-ef19b7cd1d31)\r\n\r\n[2210.14699v2.pdf](https://github.com/user-attachments/files/16024081/2210.14699v2.pdf)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "I see.  It must be because for me it downloads the PDF in Ubuntu, but for you it is instead opening the PDF up.  And you are using some modified code to get the correct page number, which is great."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1876,
    "title": "[Question] RAG retrieval method used in H2OGPT",
    "author": "chengchu88",
    "state": "closed",
    "created_at": "2024-10-16T18:40:09Z",
    "updated_at": "2024-10-16T20:59:20Z",
    "labels": [],
    "body": "Hello sir,\r\nI am assuming the H2O GPT uses langchain for RAG (document Q/A), is that correct?\r\nIf you could, would you kindly point me to the section/code where the retrieval is implemented, with the instruction input at the 'Ask or Ingest' as query? I have been looking at the gradio_runner and gpt_langchain but could not figure out where it is.\r\nThank you!",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1862,
    "title": "Where is the thumbs up/down results?",
    "author": "yokomizo-tech",
    "state": "closed",
    "created_at": "2024-10-01T05:45:21Z",
    "updated_at": "2024-10-01T05:57:35Z",
    "labels": [
      "type/question"
    ],
    "body": "I find the results from the thumbs up/down buttons in chat tab useful; however, auth.json does not seem to store them. Are they stored somewhere else?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "https://github.com/h2oai/h2ogpt/blob/70ef4f859762bac1543b8557611bf28c1629176e/gradio_utils/prompt_form.py#L141\r\n\r\nSo reviews.csv if reviews_file is not set on CLI."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1849,
    "title": "H2ogpt openai endpoints",
    "author": "CommitAndPray",
    "state": "closed",
    "created_at": "2024-09-18T10:25:19Z",
    "updated_at": "2024-09-30T15:38:09Z",
    "labels": [],
    "body": "Hi, can someone explain how I can test these endpoints? I keep getting the same error as shown in the screenshot. If I need to get a key, how can I obtain it? \r\n![image](https://github.com/user-attachments/assets/4cd7cc00-0406-46d4-95d2-e52a94dfe336)\r\n![image](https://github.com/user-attachments/assets/af26629a-4896-4ec3-ac1f-1028136da639)\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "OpenAI endpoints are on port 5000 by default."
      },
      {
        "user": "CommitAndPray",
        "body": "Thank you for responding. Do I need an API key to test the endpoints, or can I just set the api_key to empty"
      },
      {
        "user": "pseudotensor",
        "body": "EMPTY is default.   You can set the key, like I showed in the FAQ for how to setup gpt.h2o.ai\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/0e1597644dfee5a2b368d753d84086d966647bd4/docs/FAQ.md#L408"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1845,
    "title": "Accessing and Uploading Files in MyData Using upload_api and add_file_api",
    "author": "llmwesee",
    "state": "closed",
    "created_at": "2024-09-13T11:23:16Z",
    "updated_at": "2024-09-30T06:26:20Z",
    "labels": [],
    "body": "**Problem Description:**\r\nI am uploading files to the `MyData `folder through the UI. The **sources_dir** already contains text files related to the LLM, `UserData`, and `MyData` which has information related to the files name. However, I am facing difficulty accessing the `MyData` folder and its files similarly to how I can access the` user_path` that contains documents uploaded to `UserData` through the UI.\r\n\r\n**Expected Behavior:**\r\nI would like to:\r\nAccess the` MyData` folder and retrieve the files stored in it, just like I can with the `user_path` for `UserData`.\r\nUpload files to both `MyData` and `UserData` using the existing `upload_api` and` add_file_api`.\r\n\r\n**Current Behavior:**\r\nI am able to upload files to `UserData `via the UI, but I cannot access or interact with `MyData `in a similar manner.\r\nUnclear on how to use the APIs to upload files directly to `MyData` and `UserData`.\r\n\r\n**Request:**\r\nCould you provide a solution or example code demonstrating how to:\r\n\r\nAccess and retrieve files from the `MyData `folder.\r\nUpload files to both `MyData` and `UserData` using `upload_api` and `add_file_api`.\r\n\r\n**Sample Code Request:**\r\nI would appreciate a code example illustrating the following use cases:\r\n\r\nUploading files to `MyData` and `UserData` using `upload_api` and `add_file_api`.\r\nAccessing and listing files in the `MyData` directory similarly to how it’s done with the `user_path`.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Personal scratch collections like MyData are not persisted unless you use enable the auth db so that users can login.\r\n\r\nI have a FAQ for the item you are asking about I think: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#per-user-database"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1819,
    "title": "upload document via API",
    "author": "vitalyshalumov",
    "state": "closed",
    "created_at": "2024-08-28T11:54:21Z",
    "updated_at": "2024-09-30T06:23:41Z",
    "labels": [
      "type/question"
    ],
    "body": "Hello,\r\nHow can I upload a document via API and what formats does it support?\r\n\r\nThank you.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "See run_client_chat_stream_langchain_steps3 in tests/test_client_calls.py for example client code that does many things including upload using `upload_api` endpoint"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1835,
    "title": "Get a lot of Errors in Webinterface",
    "author": "DrNokkel",
    "state": "closed",
    "created_at": "2024-09-10T09:42:56Z",
    "updated_at": "2024-09-28T22:51:08Z",
    "labels": [],
    "body": "I got these Error Signs and cant do anything on the webinterface.\r\n\r\nBefore I started to ge along with the webinterface i tested it in the cli which worked nicely.\r\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --cli=True\r\n\r\nIt`s my first time to work with such project I hope someone can help me\r\n\r\n![image](https://github.com/user-attachments/assets/1c35bc6e-ed36-4d71-ab73-e79510ba8a15)\r\n\r\n\r\n(h2ogpt) C:\\Windows\\System32\\h2ogpt>python generate.py --share=False --gradio_offline_level=1 --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --load_8bit=True\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nMust install langchain for transcription, disabling\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\ngit failed to run: [WinError 2] The system cannot find the file specified\r\nMust install langchain for preloading embedding model, disabling\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nStarting get_model: h2oai/h2ogpt-4096-llama2-7b-chat\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\ndevice_map: {'': 0}\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.81s/it]\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nBegin auto-detect HF cache text generation models\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models\r\nEnd auto-detect llama.cpp models\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStarted Gradio Server and/or GUI: server_name: localhost port: None\r\nUse local URL: http://localhost:7860/\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_lock\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_names\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nverbose disabled\r\nOpenAI API URL: http://0.0.0.0:5000\r\nOpenAI API key: EMPTY\r\nINFO:     127.0.0.1:49761 - \"GET / HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:49761 - \"GET /info HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:49761 - \"GET /theme.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:49762 - \"GET /heartbeat/po69u1hn4b8 HTTP/1.1\" 200 OK\r\nClient Connected: po69u1hn4b8\r\nINFO:     127.0.0.1:49761 - \"POST /queue/join HTTP/1.1\" 500 Internal Server Error\r\nException in ASGI application\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 210, in __init__\r\n    core_schema = _getattr_no_parents(type, '__pydantic_core_schema__')\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 98, in _getattr_no_parents\r\n    raise AttributeError(attribute)\r\nAttributeError: __pydantic_core_schema__\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 401, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\gradio\\route_utils.py\", line 695, in __call__\r\n    await self.simple_response(scope, receive, send, request_headers=headers)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\gradio\\route_utils.py\", line 711, in simple_response\r\n    await self.app(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\routing.py\", line 291, in app\r\n    solved_result = await solve_dependencies(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 639, in solve_dependencies\r\n    ) = await request_body_to_args(  # body_params checked above\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 813, in request_body_to_args\r\n    fields_to_extract = get_model_fields(first_field.type_)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\_compat.py\", line 284, in get_model_fields\r\n    ModelField(field_info=field_info, name=name)\r\n  File \"<string>\", line 6, in __init__\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\_compat.py\", line 109, in __post_init__\r\n    self._type_adapter: TypeAdapter[Any] = TypeAdapter(\r\n                                           ^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 212, in __init__\r\n    core_schema = _get_schema(type, config_wrapper, parent_depth=_parent_depth + 1)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 81, in _get_schema\r\n    schema = gen.generate_schema(type_)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 499, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 734, in _generate_schema_inner\r\n    return self._annotated_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1749, in _annotated_schema\r\n    schema = self._apply_annotations(source_type, annotations)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1817, in _apply_annotations\r\n    schema = get_inner_schema(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 82, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1899, in new_handler\r\n    schema = metadata_get_schema(source, get_inner_schema)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1895, in <lambda>\r\n    lambda source, handler: handler(source)\r\n                            ^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 82, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1798, in inner_handler\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 755, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 837, in match_type\r\n    return self._match_generic_type(obj, origin)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 861, in _match_generic_type\r\n    return self._union_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1149, in _union_schema\r\n    choices.append(self.generate_schema(arg))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 499, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 755, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 841, in match_type\r\n    return self._unknown_type_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 402, in _unknown_type_schema\r\n    raise PydanticSchemaGenerationError(\r\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n\r\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\r\n\r\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type\r\nINFO:     127.0.0.1:49763 - \"POST /queue/join HTTP/1.1\" 500 Internal Server Error\r\nException in ASGI application\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 210, in __init__\r\n    core_schema = _getattr_no_parents(type, '__pydantic_core_schema__')\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 98, in _getattr_no_parents\r\n    raise AttributeError(attribute)\r\nAttributeError: __pydantic_core_schema__\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 401, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\gradio\\route_utils.py\", line 695, in __call__\r\n    await self.simple_response(scope, receive, send, request_headers=headers)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\gradio\\route_utils.py\", line 711, in simple_response\r\n    await self.app(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\routing.py\", line 291, in app\r\n    solved_result = await solve_dependencies(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 639, in solve_dependencies\r\n    ) = await request_body_to_args(  # body_params checked above\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 813, in request_body_to_args\r\n    fields_to_extract = get_model_fields(first_field.type_)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\_compat.py\", line 284, in get_model_fields\r\n    ModelField(field_info=field_info, name=name)\r\n  File \"<string>\", line 6, in __init__\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\_compat.py\", line 109, in __post_init__\r\n    self._type_adapter: TypeAdapter[Any] = TypeAdapter(\r\n                                           ^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 212, in __init__\r\n    core_schema = _get_schema(type, config_wrapper, parent_depth=_parent_depth + 1)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 81, in _get_schema\r\n    schema = gen.generate_schema(type_)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 499, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 734, in _generate_schema_inner\r\n    return self._annotated_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1749, in _annotated_schema\r\n    schema = self._apply_annotations(source_type, annotations)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1817, in _apply_annotations\r\n    schema = get_inner_schema(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 82, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1899, in new_handler\r\n    schema = metadata_get_schema(source, get_inner_schema)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1895, in <lambda>\r\n    lambda source, handler: handler(source)\r\n                            ^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 82, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1798, in inner_handler\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 755, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 837, in match_type\r\n    return self._match_generic_type(obj, origin)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 861, in _match_generic_type\r\n    return self._union_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1149, in _union_schema\r\n    choices.append(self.generate_schema(arg))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 499, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 755, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 841, in match_type\r\n    return self._unknown_type_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 402, in _unknown_type_schema\r\n    raise PydanticSchemaGenerationError(\r\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n\r\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\r\n\r\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Sounds like the installation is messed up.  How did you install?  Did you use the predefined script in a clean env?"
      },
      {
        "user": "DrNokkel",
        "body": "Yes i followed step by step the windows installation. After that i tried to fix it myself . I will try a new installation. "
      },
      {
        "user": "DrNokkel",
        "body": "(h2ogpt) C:\\Windows\\System32\\h2ogpt>python generate.py --share=False --gradio_offline_level=1 --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --load_8bit=True\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nNo GPUs detected\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n  warn_deprecated(\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1603, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\trainer.py\", line 215, in <module>\r\n    from peft import PeftModel\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\__init__.py\", line 22, in <module>\r\n    from .auto import (\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\auto.py\", line 32, in <module>\r\n    from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\mapping.py\", line 22, in <module>\r\n    from peft.tuners.xlora.model import XLoraModel\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21, in <module>\r\n    from .lora import LoraConfig, LoraModel, LoftQConfig, LoraRuntimeConfig\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20, in <module>\r\n    from .model import LoraModel\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50, in <module>\r\n    from .awq import dispatch_awq\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\__init__.py\", line 2, in <module>\r\n    from awq.models.auto import AutoAWQForCausalLM\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\__init__.py\", line 1, in <module>\r\n    from .mpt import MptAWQForCausalLM\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\mpt.py\", line 1, in <module>\r\n    from .base import BaseAWQForCausalLM\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\base.py\", line 16, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\modules\\linear.py\", line 4, in <module>\r\n    import awq_inference_engine  # with CUDA kernels\r\nImportError: DLL load failed while importing awq_inference_engine: The specified module could not be found.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\utils.py\", line 78, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gen.py\", line 2044, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 562, in get_embedding\r\n    embedding = HuggingFaceEmbeddings(model_name=hf_embedding_model, model_kwargs=model_kwargs)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 203, in warn_if_direct_instance\r\n    return wrapped(self, *args, **kwargs)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py\", line 71, in __init__\r\n    import sentence_transformers\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\sentence_transformers\\__init__.py\", line 15, in <module>\r\n    from sentence_transformers.trainer import SentenceTransformerTrainer\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\sentence_transformers\\trainer.py\", line 10, in <module>\r\n    from transformers import EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1593, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1605, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\nDLL load failed while importing awq_inference_engine: The specified module could not be found.\r\n\r\nAfter a fresh installation i got this error.\r\nLast time i tried to fix it by myself until the Webinterface errors"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1834,
    "title": "Unable to get the login screen",
    "author": "rohitnanda1443",
    "state": "closed",
    "created_at": "2024-09-10T05:56:52Z",
    "updated_at": "2024-09-28T22:49:50Z",
    "labels": [],
    "body": "Hi I am running a fresh install of H2O-GPT: \r\n\r\nRunning script is: python generate.py --inference_server=\"vllm:0.0.0.0:5001\" --guest_name=\"\"  --enable_tts=False --enable_stt=False --base_model=mistralai/Mistral-7B-Instruct-v0.3 --enable_transcriptions=False & \r\n\r\nEven if we run without an inference server we get the same issue. Has there been any change in the gradio server files? \r\n\r\n![error](https://github.com/user-attachments/assets/c858df92-24de-44f5-9edc-777db9682b82)\r\n\r\n\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, nothing seems wrong when I run a similar command.  I recommend checking the dev console in chrome browser for errors and posting or fixing."
      },
      {
        "user": "pseudotensor",
        "body": "In some cases in past, I've noticed that the browser gets messed up, and one has to reset its cache.\r\n\r\n![image](https://github.com/user-attachments/assets/f58c6117-8b46-442c-9bae-ce9ba38cfd86)\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "Tried clearing the cache out and retried. Got the same issue.  \r\n\r\nInstallation steps:\r\n\r\n1. Used: Vast.ai to get a server (Ubuntu with Cuda 12.1)\r\n2. Installed H2O-GPT fallowing the Linux install steps\r\n3. Installed Ngrok \r\n4. started H20 gradio server \r\n5. started Ngrok and got a web URL. \r\n\r\nThis used to work previously. (Has there been any changes in the Gradio libraries?) \r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1812,
    "title": "H2O-GPT on AMD GPUs (ROCm)",
    "author": "rohitnanda1443",
    "state": "open",
    "created_at": "2024-08-24T20:12:03Z",
    "updated_at": "2024-09-23T14:51:28Z",
    "labels": [],
    "body": "Hi, How can we run H20-GPT on AMD-GPUs using the AMD ROCm libraries.\r\n\r\nOne can easily run an inference server on Ollama using ROCm thereby H2O-GPT needs to use this Ollama server for inferencing. \r\n\r\nProblem: H2o-GPT install fails as it keeps finding CUDA during install. Some guidance here on editing the install script for ROCm would be helpful,\r\n\r\nMethod: \r\n1) LLM runs on an inference server using ROCm\r\n2) H2o-GPT sends LLM requests to the inference server ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Can you share what you mean by it finds CUDA during install and fails?  Maybe logs etc.?\r\n\r\nI adjusted one block In docs/linux_install.sh CUDA is mentioned.\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "**It should not be uninstalling ROCm-Torch** \r\n\r\n\r\n`` Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n      /tmp/pip-install-jav98t1i/flash-attn_c0c8ed92b3c147bfa04d7e6ab7c98f49/setup.py:95: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\r\n        warnings.warn(\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/tmp/pip-install-jav98t1i/flash-attn_c0c8ed92b3c147bfa04d7e6ab7c98f49/setup.py\", line 179, in <module>\r\n          CUDAExtension(\r\n        File \"/home/rohit/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1074, in CUDAExtension\r\n          library_dirs += library_paths(cuda=True)\r\n        File \"/home/rohit/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1201, in library_paths\r\n          if (not os.path.exists(_join_cuda_home(lib_dir)) and\r\n        File \"/home/rohit/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2407, in _join_cuda_home\r\n          raise OSError('CUDA_HOME environment variable is not set. '\r\n      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root. \r\n      \r\n      \r\n      torch.__version__  = 2.2.1+cu121\r\n      \r\n      \r\n      [end of output]\r\n  \r\n  \r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details. \r\n\r\nAttempting uninstall: torch\r\n    Found existing installation: torch 2.5.0.dev20240822+rocm6.1\r\n    Uninstalling torch-2.5.0.dev20240822+rocm6.1:\r\n      Successfully uninstalled torch-2.5.0.dev20240822+rocm6.1\r\n  Attempting uninstall: sse_starlette\r\n    Found existing installation: sse-starlette 0.10.3\r\n    Uninstalling sse-starlette-0.10.3:\r\n      Successfully uninstalled sse-starlette-0.10.3\r\n  Attempting uninstall: torchvision\r\n    Found existing installation: torchvision 0.20.0.dev20240823+rocm6.1\r\n    Uninstalling torchvision-0.20.0.dev20240823+rocm6.1:\r\n      Successfully uninstalled torchvision-0.20.0.dev20240823+rocm6.1\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ntts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\r\ntts 0.22.0 requires pandas<2.0,>=1.4, but you have pandas 2.2.2 which is incompatible.\r\nawscli 1.34.5 requires docutils<0.17,>=0.10, but you have docutils 0.21.2 which is incompatible.\r\nfiftyone 0.25.0 requires sse-starlette<1,>=0.10.3, but you have sse-starlette 2.1.3 which is incompatible.\r\ntorchaudio 2.4.0.dev20240823+rocm6.1 requires torch==2.5.0.dev20240822, but you have torch 2.2.1 which is incompatible.\r\nvllm 0.5.5+rocm614 requires pydantic>=2.8, but you have pydantic 2.7.0 which is incompatible.\r\nSuccessfully installed docutils-0.21.2 pandas-2.2.2 pydantic-2.7.0 pydantic-core-2.18.1 pypandoc_binary-1.13 sse_starlette-2.1.3 torch-2.2.1 `torchvision-0.17.1` ``\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "Do we have an ROCm Docker image? \r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1844,
    "title": "h2ogpt with open WebUI",
    "author": "InesBenAmor99",
    "state": "closed",
    "created_at": "2024-09-13T10:11:45Z",
    "updated_at": "2024-09-16T19:37:26Z",
    "labels": [
      "type/question"
    ],
    "body": "Hello, I would like to integrate H2O GPT with Open WebUI. Will it be possible to chat with all the documents that are already uploaded in collections created from the H2O GPT interface and stored in the vector database, or will it just be a simple chat without accessing the documents already uploaded in the background?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "The documents won't work across.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#open-web-ui\r\n\r\nWe use it here: https://gpt-docs.h2o.ai\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "FYI I recommend the non-docker way so it can use h2oGPT OSS changes to the package so it works optimally."
      },
      {
        "user": "InesBenAmor99",
        "body": "I don't actually understand the need for [https://gpt-docs.h2o.ai/](https://gpt-docs.h2o.ai/ ) when it's just an implementation of the LLM model, not H2O GPT functionalities. Can you explain the difference between these two options?"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1842,
    "title": "BUG: Broken agent flow",
    "author": "fatihozturkh2o",
    "state": "closed",
    "created_at": "2024-09-13T09:00:43Z",
    "updated_at": "2024-09-13T09:23:48Z",
    "labels": [
      "agents/bug"
    ],
    "body": "For agents, running into the following error now:\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/fatih/h2ogpt/openai_server/autogen_streaming.py\", line 50, in run_autogen_in_proc\r\n    ret_dict = func(**kwargs)\r\n  File \"/home/fatih/h2ogpt/openai_server/agent_utils.py\", line 113, in run_agent\r\n    ret_dict = run_agent_func(**kwargs)\r\n  File \"/home/fatih/h2ogpt/openai_server/autogen_2agent_backend.py\", line 145, in run_autogen_2agent\r\n    chat_result = code_executor_agent.initiate_chat(**chat_kwargs)\r\n  File \"/home/fatih/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py\", line 1095, in initiate_chat\r\n    self.send(msg2send, recipient, request_reply=True, silent=silent)\r\n  File \"/home/fatih/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py\", line 733, in send\r\n    message = self._process_message_before_send(message, recipient, ConversableAgent._is_silent(self, silent))\r\n  File \"/home/fatih/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py\", line 278, in _is_silent\r\n    return agent.silent if agent.silent is not None else silent\r\nAttributeError: 'H2OConversableAgent' object has no attribute 'silent'\r\n```\r\n\r\nseems related to this commit: https://github.com/h2oai/h2ogpt/commit/e3114a1ccc6b60d8b1e60c16855abedabfa2c533\r\n\r\nRepro:\r\nprompt = any prompt\r\nagent_type = auto",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Must be due to version differences.  My version of ConversableAgent has no silent parameter.\r\n\r\n\r\na9b97158d1d5f3a8fbb47246e28327e7609bc903"
      },
      {
        "user": "fatihozturkh2o",
        "body": "That explains, I have 0.2.35 (because of > 2.33.0) . Let me also downgrade to that then."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1841,
    "title": "BUG: Broken # execution: false logic for agents",
    "author": "fatihozturkh2o",
    "state": "closed",
    "created_at": "2024-09-12T13:32:30Z",
    "updated_at": "2024-09-13T02:46:19Z",
    "labels": [],
    "body": "It seems when `# execution: false` is in the codeblock, code executor tries to execute the code and returns the output: \r\n```\r\nexitcode: -2 (execution failed)\r\nCode output:\r\n```\r\nand since this is not an empty string, there is no logic in `terminate_message_func` function to terminate the chat either, hence the conversation goes on as a loop.\r\n\r\nI think we need to be able to skip execution of such code blocks in H2OLocalCommandLineCodeExecutor.\r\n\r\nrepro prompt: `Write a code to show an image in matplotlib. Do not execute any code`\r\nmodel: claude-3-5-sonnet-20240620\r\nagent_type: auto\r\n#### How it looks  (since it was a loop, forcefully killed the process before hitting the conversation limit)\r\nhttps://github.com/user-attachments/assets/88a6c31a-0c27-4e78-a459-c03e9acb415a\r\n\r\n\r\n\r\n",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1838,
    "title": "Fix cost for multi-agent",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2024-09-11T23:19:17Z",
    "updated_at": "2024-09-13T00:24:21Z",
    "labels": [],
    "body": "https://github.com/h2oai/h2ogpt/pull/1836",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1715,
    "title": "Missing instructions for make_db in docker",
    "author": "tomerjr",
    "state": "closed",
    "created_at": "2024-06-30T11:27:29Z",
    "updated_at": "2024-09-10T20:18:15Z",
    "labels": [
      "type/question"
    ],
    "body": "Hi, would like a clarification as to how to use make_db with specific path as data source and with the ability to create a collection name.\r\n\r\nI've tried to use it like the normal instructions with --collection_name   and   --user_path   but it did not work and i did not find an answer to this in other issues or the doc files.\r\n\r\nfor example i tried:\r\n\r\nexport GRADIO_SERVER_PORT=7860\r\nexport OPENAI_SERVER_PORT=5000\r\nsudo docker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=1g \\\r\n       -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\r\n       -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache/huggingface/hub/:/workspace/.cache/huggingface/hub \\\r\n       -v \"${HOME}\"/.config:/workspace/.config/ \\\r\n       -v \"${HOME}\"/.triton:/workspace/.triton/  \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       -v \"${HOME}\"/users:/workspace/users \\\r\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n       -v \"${HOME}\"/h2ogpt_auth:/workspace/h2ogpt_auth \\\r\n       -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/src/make_db.py \\\r\n       --collection_name=Duck\r\n\r\nThank you and have a good day.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's already documented here near bottom of this section:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#database-creation"
      },
      {
        "user": "tomerjr",
        "body": "so when using docker i must use weaviate in order to change the user_path and the collection_name?\r\nfrom the doc its not stated how to run it locally with these arguments and with docker otherwise. @pseudotensor "
      },
      {
        "user": "pseudotensor",
        "body": "The bottom of the section has this:\r\n```\r\nmkdir -p ~/.cache\r\nmkdir -p ~/save\r\nmkdir -p ~/user_path\r\nmkdir -p ~/db_dir_UserData\r\ndocker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=2g \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/src/make_db.py --verbose --use_unstructured_pdf=False --enable_pdf_ocr=False --hf_embedding_model=BAAI/bge-small-en-v1.5 --cut_distance=10000\r\n```\r\n\r\nIt doesn't use weaviate.  Can you explain what you mean?"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1796,
    "title": "upload docs",
    "author": "InesBenAmor99",
    "state": "open",
    "created_at": "2024-08-12T15:50:01Z",
    "updated_at": "2024-09-10T20:16:04Z",
    "labels": [],
    "body": "What could be the problem if, when uploading a document (simple unscanned PDF), the process gets stuck at the processing stage, without displaying any errors, and without the document being uploaded? I've also tested uploading to the online version, but that doesn't work either. ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Are you able to provide an example document that has issues?  I expect it's getting stuck in using unstructured OCR or something that is very slow."
      },
      {
        "user": "llmwesee",
        "body": "I'm also facing the same issue. I already uploaded around 720 documents in the UserData. But now whenever I trying to upload documents in collections like 'UserData'  or 'MyData'  the process gets stuck at the processing stage, without displaying any errors, and without the document being uploaded?"
      },
      {
        "user": "pseudotensor",
        "body": "Can you see what the processes are by looking at `ps -auxwf` and `top` (e.g. hit c and show in wide view)?  Any processes still going that seem stuck?\r\n\r\nI have over 3000 tests that run over a 24 hour period, and not noticing anything wrong."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1028,
    "title": "include grclient.py in the OpenAI compliant API to make easy access to basic doc operations",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2023-11-01T04:57:19Z",
    "updated_at": "2024-09-04T03:07:10Z",
    "labels": [],
    "body": "https://h2oai.slack.com/archives/C04ND1ZCXGD/p1698811222904429\r\n\r\n\r\n```\r\nExample LLM, QA, Summarization, Extraction using open-source client code from url.\r\nIn bash:\r\nconda create -n gradioclient -y\r\nconda activate gradioclient\r\nconda install python=3.10 -y\r\npip install gradio_client==0.6.1\r\n\r\n# Download Gradio Wrapper code if GradioClient class used, not needed for native Gradio Client\r\n# No wheel for now\r\nwget https://raw.githubusercontent.com/h2oai/h2ogpt/main/gradio_utils/grclient.py\r\nmkdir -p gradio_utils\r\nmv grclient.py gradio_utils\r\n```\r\nThen in python:\r\n```\r\nimport os\r\nfrom gradio_utils.grclient import GradioClient\r\n\r\nh2ogpt_key = os.getenv('H2OGPT_KEY') or os.getenv('H2OGPT_H2OGPT_KEY')\r\n# if you have API key for public instance:\r\nclient = GradioClient(\"https://gpt.h2o.ai\", h2ogpt_key=h2ogpt_key)\r\n\r\n# LLM\r\nprint(client.question(\"Who are you?\"))\r\n\r\nurl = \"https://cdn.openai.com/papers/whisper.pdf\"\r\n\r\n# Q/A\r\nprint(client.query(\"What is whisper?\", url=url))\r\n# summarization (map_reduce over all pages if top_k_docs=-1)\r\nprint(client.summarize(\"What is whisper?\", url=url, top_k_docs=3))\r\n# extraction (map per page)\r\nprint(client.extract(\"Give bullet for all key points\", url=url, top_k_docs=3))\r\nWill output stuff like this: https://github.com/h2oai/h2ogpt/blob/main/tests/test_client_readme.py#L34-L57\r\n```",
    "comments": [
      {
        "user": "this",
        "body": "Now we use the `openai` Python package for DAI H2OGPT integration  https://github.com/h2oai/h2oai/pull/33328. Hence this issue is no longer valid."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1809,
    "title": "Please tell me how to summarise local document using local API",
    "author": "anushaharish538",
    "state": "closed",
    "created_at": "2024-08-20T14:03:47Z",
    "updated_at": "2024-08-27T05:20:28Z",
    "labels": [
      "type/question"
    ],
    "body": "Please tell me how to summarise local document using local API",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Check out the client code for the tests in tests/test_client_calls.py::test_client_summarization_from_url or test_client_summarization_from_text\r\n\r\nIt's very simple."
      },
      {
        "user": "anushaharish538",
        "body": "This is for urls correct ?. tests/test_client_calls.py::test_client_summarization_from_url or test_client_summarization_from_text. I need for document summarisation.\r\n\r\nRegards,\r\nAnusha Harish"
      },
      {
        "user": "pseudotensor",
        "body": "There are 3 test codes, the other one is test_client_summarization that takes files like PDFs.  Is that what you want?"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1810,
    "title": "TypeError Can only concatenate str (not \"bool\") to str",
    "author": "btriw",
    "state": "closed",
    "created_at": "2024-08-23T10:21:29Z",
    "updated_at": "2024-08-26T16:39:47Z",
    "labels": [],
    "body": "Hello i tried to connect h2ogpt with gradio so i can get the function from h2opgt, like this:\r\n\r\n![image](https://github.com/user-attachments/assets/e16e1e43-6e74-4ed9-93b2-3c4c6e5e253b)\r\n\r\nthis is the code that use the vicuna_client:\r\n\r\n![image](https://github.com/user-attachments/assets/781241f3-b2d4-4fd4-b440-c6786ae7a552)\r\n\r\nit can be used as expected, but when i try to ask throught h2opgt it getting error like in the Title above, TypeError: can only concatenate str (not \"bool\") to str\r\n\r\n![image](https://github.com/user-attachments/assets/779a154c-74f4-44f9-9381-f4e47048c858)\r\n\r\ndoes anyone know what happen? i try to re-clone the repo but the probelm still exist\r\nhope anyone can answer this :)",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "What version of gradio do you have?  h2oGPT works with gradio 3.50.2 or gradio from requirements.txt:\r\n```\r\ngradio @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl\r\ngradio_client @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl\r\n```\r\n\r\nIt appears you have some other version."
      },
      {
        "user": "btriw",
        "body": "Hello @pseudotensor sorry for my late reply and thanks for the reply :) \r\nafter installed the recommended version its worked again, thanks :D"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1808,
    "title": "Relocate \"Delete Selected Sources from DB\" Button for Safety",
    "author": "yokomizo-tech",
    "state": "closed",
    "created_at": "2024-08-20T05:26:20Z",
    "updated_at": "2024-08-25T21:40:19Z",
    "labels": [],
    "body": "The \"Delete Selected Sources from DB\" button on the Document Selection tab appears to be risky. Could the functionality for document deletion be moved to a separate tab, such as a \"Document Deletion\" tab, to prevent accidental deletions and enhance user safety?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's a good idea, to at least make it an option."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1799,
    "title": "API Support",
    "author": "anushaharish538",
    "state": "closed",
    "created_at": "2024-08-13T16:48:52Z",
    "updated_at": "2024-08-20T12:38:36Z",
    "labels": [
      "type/question"
    ],
    "body": "Hi,\r\n\r\nCan I chat with files using api by giving file url or website url.\r\n\r\nRegards,\r\nAnusha",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Ya the full gradio-based API exists for uploading, managing, querying docs.  And the openAI API can be used for querying docs.\r\n\r\nE.g. https://github.com/h2oai/h2ogpt/blob/e7c04e57b21dd412f4babd10aed8d3a7b686e446/tests/test_client_calls.py#L2972\r\n\r\nThis test shows many client calls for various tasks that are tested."
      },
      {
        "user": "anushaharish538",
        "body": "I need for summarise the docs"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1667,
    "title": "Is there a plan to incorporate a Knowledge Graph RAG Query Engine?",
    "author": "vitalyshalumov",
    "state": "closed",
    "created_at": "2024-06-05T12:29:22Z",
    "updated_at": "2024-08-16T09:20:47Z",
    "labels": [
      "type/question"
    ],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Yes"
      },
      {
        "user": "FellowTraveler",
        "body": "What's the status on this?"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1766,
    "title": "Integrating H2OGPT in multitenance application.",
    "author": "Eliyas",
    "state": "closed",
    "created_at": "2024-07-23T21:09:59Z",
    "updated_at": "2024-08-15T23:30:09Z",
    "labels": [
      "type/question"
    ],
    "body": "We have a multi-tenant application and are integrating H2OGPT to serve various clients. Each client may have hundreds of users.\r\n\r\n**1. Our goal is to store and ingest user, client, and permission-based documents efficiently within H2OGPT. Is there a method to manage and search only in documents permitted for each user without duplicating them?**\r\nI have explored using collections in H2OGPT, where each user would have a separate collection. However, duplicating documents across multiple collections is not ideal for us.\r\nAdministrators of each client should have access to all documents, while other users should only access documents based on their permissions.\r\n\r\n**2. How to add custom metadata to a document?**\r\nI am thinking of filtering documents based on custom metadata like roles, permission, and client names. I saw that chromaDB has a metadata filter available in the query.\r\n```\r\nresults = collection.query(\r\n   query_texts=[\"This is a query document\"],\r\n   n_results=2,\r\n   where={\"metadata_field\": \"is_equal_to_this\"}\r\n)\r\n\r\n```\r\nBut not sure how to add custom metadata into a document and pass metadata filter in summary or query API in H2OGPT.\r\n\r\n**3. Do we need to restart the AI server to create a new collection and DB?**\r\n\r\n**For creating user collection now I am doing this.**\r\n`python src/make_db.py --user_path=clients/user1 --collection_name=user1 --langchain_type=personal --hf_embedding_model=hkunlp/instructor-large --persist_directory=users/test/db_dir_user1`\r\n\r\n**After**\r\n`python generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --tokenizer_base_model=zephyr/zephyr-7B-beta --prompt_type=zephyr --max_seq_len=4096 --langchain_mode='UserData' --langchain_modes=['UserData', 'img', 'live', 'user1'] --langchain_mode_paths=\"{'UserData':'user_path','img':'clients/img','live':'clients/live','user1':'clients/user1'}\"  --langchain_mode_types='{'UserData':'personal','img':'personal','live':'personal','user1':'personal'}' --save_dir=saveDir --verbose=True --system_prompt='auto'`\r\n\r\nDo we need to add and execute the script to define user collections in **langchain_modes**, **langchain_mode_paths**, and **langchain_mode_types** every time we restart the server, or is it a one-time setup? If we have 100 users, do we need to include details for each user's collection in the script?\r\n\r\n",
    "comments": [
      {
        "user": "llmwesee",
        "body": "I'm also curious about the implementation details?"
      },
      {
        "user": "pseudotensor",
        "body": "@Eliyas Sorry for the delay.\r\n\r\n1a. The simplest way to implement an efficient document ingestion is via caching of the embedding for a given input.  That is relatively easy.\r\n\r\n1b. Permission based access is automatic if using personal collections and auth access to the user.\r\n\r\n2. h2oGPT automatically adds meta data to this and its searchable as AND or OR logical operation, but there is no way to add additional meta data.  However, if the document was parsed and had metadata (e.g. PDF) and you add metadata_in_context='all' or pass a list of keys to metadata_in_context, then we will use that.  So the issue just passes to the PDF having the correct metadata and you choosing which to use.\r\n\r\n3. No need to restart.  If created outside server with make_db, then any user can add that collection by name and see it.  E.g. for specific users, one would follow this: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#personal-collections-with-make_db"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1768,
    "title": "Error when using AWQ model with 'use_gpu_id=False'",
    "author": "bw-Deejee",
    "state": "closed",
    "created_at": "2024-07-24T13:38:14Z",
    "updated_at": "2024-08-15T23:21:13Z",
    "labels": [],
    "body": "Using the newest version of the repo.\r\nFollowing command works:\r\n\r\n`python generate.py --base_model=casperhansen/llama-3-70b-instruct-awq --load_awq=model`\r\n\r\nfollowing command doesn't:\r\n\r\n`python generate.py --base_model=casperhansen/llama-3-70b-instruct-awq --load_awq=model --use_gpu_id=False`\r\n\r\nerror message:\r\n`... TypeError: functools.partial(<bound method AutoAWQForCausalLM.from_quantized of <class 'awq.models.auto.AutoAWQForCausalLM'>>, fuse_layers=True) got multiple values for keyword argument 'use_safetensors'`\r\n\r\nSetting `--use_safetensors=True` doesnt help.\r\n\r\nIs there any way to solve this?\r\n",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1779,
    "title": "embedding model Cohere/Cohere-embed-multilingual-v3.0 not supported.",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-08-02T08:15:41Z",
    "updated_at": "2024-08-15T22:41:34Z",
    "labels": [
      "resolution/wontfix"
    ],
    "body": "Command line executed:\r\n```shell\r\nexport max_input_tokens=8192;\r\nexport max_total_input_tokens=16384;\r\nexport chunk_size=2048;\r\nTOKENIZERS_PARALLELISM=true python generate.py \\\r\n    --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --prompt_type=llama2 \\\r\n    --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 \\\r\n    --user_path=/opt/myDocuments/all/ --langchain_mode='UserData' --max_quality=True \\\r\n    --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True \\\r\n    --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True \\\r\n    --hf_embedding_model=\"Cohere/Cohere-embed-multilingual-v3.0\" \\\r\n    --memory_restriction_level=0 --score_model=None --verbose=True --debug=True \\\r\n    --show_examples=True --compile_model=True \\\r\n    --share=True \\\r\n    --max_input_tokens=$max_input_tokens --max_total_input_tokens=$max_total_input_tokens --chunk_size=$chunk_size;\r\n ```\r\nError output:\r\n```shell\r\nWARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name Cohere/Cohere-embed-multilingual-v3.0. Creating a new one with mean pooling.\r\nTraceback (most recent call last):\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/gen.py\", line 2015, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/gpt_langchain.py\", line 559, in get_embedding\r\n    embedding = HuggingFaceEmbeddings(model_name=hf_embedding_model, model_kwargs=model_kwargs)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 203, in warn_if_direct_instance\r\n    return wrapped(self, *args, **kwargs)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 79, in __init__\r\n    self.client = sentence_transformers.SentenceTransformer(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 299, in __init__\r\n    modules = self._load_auto_model(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 1324, in _load_auto_model\r\n    transformer_model = Transformer(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py\", line 53, in __init__\r\n    config = AutoConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1004, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized model in Cohere/Cohere-embed-multilingual-v3.0. Should have a `model_type` key in its config.json, ....\r\n```\r\nAs it can be seen there is no support for embedding model Cohere/Cohere-embed-multilingual-v3.0 from huggingface.\r\n\r\nSome idea would be appreciated, thanks in advance.\r\n\r\n------------------------\r\n\r\nSO: Ubuntu 22.04.4 LTS\r\nCommit: 10571004 (last at the moment this script was executed)",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "That's true, it's not a local model. The https://huggingface.co/Cohere/Cohere-embed-multilingual-v3.0 only contains the tokenizer.  This is a paid model for Cohere via their API, I wouldn't easily support and probably little value in it."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1792,
    "title": "running h2ogpt on Macbook M1 error ",
    "author": "a-ml",
    "state": "closed",
    "created_at": "2024-08-09T17:01:55Z",
    "updated_at": "2024-08-12T14:49:51Z",
    "labels": [],
    "body": "Hello,\r\n\r\nI've installed H2O on Macbook M1, when installing the dependencies I'm receiving the error bellow.\r\n\r\n```\r\nIgnoring torch: markers 'sys_platform != \"darwin\" and platform_machine != \"arm64\"' don't match your environment\r\nCollecting InstructorEmbedding@ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 23))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (20 kB)\r\nCollecting sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 26))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (125 kB)\r\nCollecting llava@ https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 41))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl (87 kB)\r\nIgnoring jq: markers 'platform_machine == \"x86_64\"' don't match your environment\r\nERROR: Cannot install torch==2.2.1 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    The user requested torch==2.2.1\r\n    The user requested (constraint) torch==2.3.1\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip to attempt to solve the dependency conflict\r\n\r\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\r\n```\r\nI don't know if this error is impediment to run the application which I'm trying with the command bellow:\r\n\r\n`python generate.py --base_model=llama --model_path_llama=https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf?download=true --tokenizer_base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --max_seq_len=8192`\r\n\r\nWhen trying to run I've this error:\r\n\r\n```\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model llama\r\nTraceback (most recent call last):\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/src/utils.py\", line 77, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/src/gen.py\", line 2013, in main\r\n    from gpt_langchain import get_embedding\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/src/gpt_langchain.py\", line 48, in <module>\r\n    from langchain_core.language_models.llms import aget_prompts, aupdate_cache\r\nImportError: cannot import name 'aget_prompts' from 'langchain_core.language_models.llms' (/Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py)\r\n```\r\n\r\n\r\nAny hint?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I have these constraints:\r\n\r\n![image](https://github.com/user-attachments/assets/da17c6c1-d5f4-4f6d-9a09-de86aa9669a7)\r\n\r\n\r\nI'm unclear why the constraint works (says has to be 2.3.1 for MAC) but the requirements.txt (that uses same constraint) thinks it needs to use 2.2.1.\r\n\r\n\r\nCan you give more complete logs of the install?"
      },
      {
        "user": "a-ml",
        "body": "I've deleted the env and I'm trying recreate it again... I'll update"
      },
      {
        "user": "a-ml",
        "body": "I've created a brand-new conda environment, tried to follow the instructions and I got the same error.\r\n\r\nHere complete install logs\r\n\r\n```\r\n❯ conda activate h2ogpt\r\n❯ pip uninstall -y pandoc pypandoc pypandoc-binary\r\nWARNING: Skipping pandoc as it is not installed.\r\nWARNING: Skipping pypandoc as it is not installed.\r\nWARNING: Skipping pypandoc-binary as it is not installed.\r\n❯ pip install --upgrade pip\r\nRequirement already satisfied: pip in /Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages (24.0)\r\nCollecting pip\r\n  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\r\nUsing cached pip-24.2-py3-none-any.whl (1.8 MB)\r\nInstalling collected packages: pip\r\n  Attempting uninstall: pip\r\n    Found existing installation: pip 24.0\r\n    Uninstalling pip-24.0:\r\n      Successfully uninstalled pip-24.0\r\nSuccessfully installed pip-24.2\r\n❯ python -m pip install --upgrade setuptools\r\nRequirement already satisfied: setuptools in /Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages (72.1.0)\r\n❯ pip install -r requirements.txt --extra-index https://download.pytorch.org/whl/cpu -c reqs_optional/reqs_constraints.txt\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\r\nCollecting gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl (from -r requirements.txt (line 8))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl (17.3 MB)\r\nCollecting gradio_client@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl (from -r requirements.txt (line 9))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl (313 kB)\r\nIgnoring torch: markers 'sys_platform != \"darwin\" and platform_machine != \"arm64\"' don't match your environment\r\nIgnoring bitsandbytes: markers 'sys_platform != \"darwin\" and platform_machine != \"arm64\"' don't match your environment\r\nIgnoring pypandoc_binary: markers 'platform_machine == \"x86_64\"' don't match your environment\r\nIgnoring pypandoc_binary: markers 'platform_system == \"Windows\"' don't match your environment\r\nIgnoring python-magic-bin: markers 'platform_system == \"Windows\"' don't match your environment\r\nCollecting InstructorEmbedding@ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (from -r requirements.txt (line 74))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (20 kB)\r\nCollecting sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (from -r requirements.txt (line 75))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (125 kB)\r\nCollecting gunicorn (from -r requirements.txt (line 12))\r\n  Using cached gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\r\nCollecting fastapi-utils (from -r requirements.txt (line 13))\r\n  Using cached fastapi_utils-0.7.0-py3-none-any.whl.metadata (5.4 kB)\r\nCollecting sse_starlette>=1.8.2 (from -r requirements.txt (line 14))\r\n  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\r\nCollecting huggingface_hub>=0.23.3 (from -r requirements.txt (line 16))\r\n  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\r\nCollecting appdirs>=1.4.4 (from -r requirements.txt (line 17))\r\n  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\nCollecting fire>=0.5.0 (from -r requirements.txt (line 18))\r\n  Downloading fire-0.6.0.tar.gz (88 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting docutils>=0.20.1 (from -r requirements.txt (line 19))\r\n  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\r\nCollecting torch==2.3.1 (from -r requirements.txt (line 21))\r\n  Using cached https://download.pytorch.org/whl/cpu/torch-2.3.1-cp310-none-macosx_11_0_arm64.whl (61.0 MB)\r\nCollecting evaluate>=0.4.0 (from -r requirements.txt (line 22))\r\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\r\nCollecting rouge_score>=0.1.2 (from -r requirements.txt (line 23))\r\n  Using cached rouge_score-0.1.2-py3-none-any.whl\r\nCollecting sacrebleu>=2.3.1 (from -r requirements.txt (line 24))\r\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\r\nCollecting scikit-learn>=1.2.2 (from -r requirements.txt (line 25))\r\n  Downloading scikit_learn-1.5.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (12 kB)\r\nCollecting numpy<2.0,>=1.23.4 (from -r requirements.txt (line 29))\r\n  Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\r\nCollecting pandas>=2.0.2 (from -r requirements.txt (line 30))\r\n  Downloading pandas-2.2.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (19 kB)\r\nCollecting matplotlib>=3.7.1 (from -r requirements.txt (line 31))\r\n  Downloading matplotlib-3.9.1.post1-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\r\nCollecting loralib>=0.1.2 (from -r requirements.txt (line 34))\r\n  Using cached loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\r\nCollecting bitsandbytes==0.42.0 (from -r requirements.txt (line 37))\r\n  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\r\nCollecting accelerate>=0.30.1 (from -r requirements.txt (line 38))\r\n  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\r\nCollecting peft>=0.7.0 (from -r requirements.txt (line 39))\r\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting transformers>=4.43.2 (from -r requirements.txt (line 40))\r\n  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\r\nCollecting jinja2>=3.1.0 (from -r requirements.txt (line 41))\r\n  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting tokenizers>=0.19.0 (from -r requirements.txt (line 42))\r\n  Using cached tokenizers-0.20.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\nCollecting hf_transfer>=0.1.6 (from -r requirements.txt (line 43))\r\n  Downloading hf_transfer-0.1.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.7 kB)\r\nCollecting optimum>=1.17.1 (from -r requirements.txt (line 44))\r\n  Using cached optimum-1.21.3-py3-none-any.whl.metadata (19 kB)\r\nCollecting datasets>=2.18.0 (from -r requirements.txt (line 45))\r\n  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\r\nCollecting sentencepiece>=0.2.0 (from -r requirements.txt (line 46))\r\n  Using cached sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\r\nCollecting APScheduler>=3.10.1 (from -r requirements.txt (line 48))\r\n  Downloading APScheduler-3.10.4-py3-none-any.whl.metadata (5.7 kB)\r\nCollecting pynvml>=11.5.0 (from -r requirements.txt (line 51))\r\n  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\r\nCollecting psutil>=5.9.5 (from -r requirements.txt (line 52))\r\n  Using cached psutil-6.0.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (21 kB)\r\nCollecting boto3>=1.26.101 (from -r requirements.txt (line 53))\r\n  Downloading boto3-1.34.158-py3-none-any.whl.metadata (6.6 kB)\r\nCollecting botocore>=1.29.101 (from -r requirements.txt (line 54))\r\n  Downloading botocore-1.34.158-py3-none-any.whl.metadata (5.7 kB)\r\nCollecting beautifulsoup4>=4.12.2 (from -r requirements.txt (line 55))\r\n  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\r\nCollecting markdown>=3.4.3 (from -r requirements.txt (line 56))\r\n  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\r\nCollecting pytest>=7.2.2 (from -r requirements.txt (line 59))\r\n  Downloading pytest-8.3.2-py3-none-any.whl.metadata (7.5 kB)\r\nCollecting pytest-xdist>=3.2.1 (from -r requirements.txt (line 60))\r\n  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\r\nCollecting nltk>=3.8.1 (from -r requirements.txt (line 61))\r\n  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\r\nCollecting textstat>=0.7.3 (from -r requirements.txt (line 62))\r\n  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\r\nCollecting pypandoc>=1.11 (from -r requirements.txt (line 64))\r\n  Using cached pypandoc-1.13-py3-none-any.whl.metadata (16 kB)\r\nCollecting openpyxl>=3.1.2 (from -r requirements.txt (line 68))\r\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\r\nCollecting lm_dataformat>=0.0.20 (from -r requirements.txt (line 69))\r\n  Downloading lm_dataformat-0.0.20-py3-none-any.whl.metadata (1.2 kB)\r\nCollecting bioc>=2.0 (from -r requirements.txt (line 70))\r\n  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting sentence_transformers>=3.0.1 (from -r requirements.txt (line 73))\r\n  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\r\nCollecting einops>=0.6.1 (from -r requirements.txt (line 78))\r\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\nCollecting python-dotenv>=1.0.0 (from -r requirements.txt (line 81))\r\n  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\r\nCollecting json_repair>=0.21.0 (from -r requirements.txt (line 83))\r\n  Using cached json_repair-0.27.0-py3-none-any.whl.metadata (7.9 kB)\r\nCollecting text-generation>=0.7.0 (from -r requirements.txt (line 85))\r\n  Using cached text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\r\nCollecting tiktoken>=0.5.2 (from -r requirements.txt (line 88))\r\n  Using cached tiktoken-0.7.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\r\nCollecting openai>=1.40.1 (from -r requirements.txt (line 91))\r\n  Using cached openai-1.40.2-py3-none-any.whl.metadata (22 kB)\r\nCollecting requests>=2.31.0 (from -r requirements.txt (line 93))\r\n  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting httpx>=0.24.1 (from -r requirements.txt (line 94))\r\n  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\r\nCollecting urllib3>=1.26.16 (from -r requirements.txt (line 95))\r\n  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\r\nCollecting filelock>=3.12.2 (from -r requirements.txt (line 96))\r\n  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\r\nCollecting joblib>=1.3.1 (from -r requirements.txt (line 97))\r\n  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\r\nCollecting tqdm>=4.65.0 (from -r requirements.txt (line 98))\r\n  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\r\nCollecting tabulate>=0.9.0 (from -r requirements.txt (line 99))\r\n  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\r\nCollecting packaging>=23.1 (from -r requirements.txt (line 100))\r\n  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\nCollecting uvicorn[standard] (from -r requirements.txt (line 11))\r\n  Downloading uvicorn-0.30.5-py3-none-any.whl.metadata (6.6 kB)\r\nCollecting typing-extensions>=4.8.0 (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting sympy (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\r\nCollecting networkx (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\r\nCollecting fsspec (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\r\nCollecting scipy (from bitsandbytes==0.42.0->-r requirements.txt (line 37))\r\n  Downloading scipy-1.14.0-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\r\nCollecting aiofiles<24.0,>=22.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\r\nCollecting altair<6.0,>=4.2.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\r\nCollecting fastapi (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\r\nCollecting ffmpy (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\r\nCollecting importlib-resources<7.0,>=1.3 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\r\nCollecting markupsafe~=2.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\r\nCollecting orjson~=3.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached orjson-3.10.6-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\r\nCollecting pillow<11.0,>=8.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.2 kB)\r\nCollecting pydantic>=2.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\r\nCollecting pydub (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\nCollecting python-multipart>=0.0.9 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\r\nCollecting pyyaml<7.0,>=5.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\r\nCollecting ruff>=0.2.2 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading ruff-0.5.7-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\r\nCollecting semantic-version~=2.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\nCollecting tomlkit==0.12.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\r\nCollecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\r\nCollecting websockets<12.0,>=10.0 (from gradio_client@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl->-r requirements.txt (line 9))\r\n  Downloading websockets-11.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\r\nCollecting click>=7.0 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting h11>=0.8 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\r\nCollecting httptools>=0.5.0 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Using cached httptools-0.6.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.6 kB)\r\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Using cached uvloop-0.19.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (4.9 kB)\r\nCollecting watchfiles>=0.13 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Downloading watchfiles-0.23.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.9 kB)\r\nCollecting psutil>=5.9.5 (from -r requirements.txt (line 52))\r\n  Using cached psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl.metadata (21 kB)\r\nCollecting starlette (from sse_starlette>=1.8.2->-r requirements.txt (line 14))\r\n  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\r\nCollecting anyio (from sse_starlette>=1.8.2->-r requirements.txt (line 14))\r\n  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting six (from fire>=0.5.0->-r requirements.txt (line 18))\r\n  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\r\nCollecting termcolor (from fire>=0.5.0->-r requirements.txt (line 18))\r\n  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\r\nCollecting dill (from evaluate>=0.4.0->-r requirements.txt (line 22))\r\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\nCollecting xxhash (from evaluate>=0.4.0->-r requirements.txt (line 22))\r\n  Using cached xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\r\nCollecting multiprocess (from evaluate>=0.4.0->-r requirements.txt (line 22))\r\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\nCollecting absl-py (from rouge_score>=0.1.2->-r requirements.txt (line 23))\r\n  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\r\nCollecting portalocker (from sacrebleu>=2.3.1->-r requirements.txt (line 24))\r\n  Using cached portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\r\nCollecting regex (from sacrebleu>=2.3.1->-r requirements.txt (line 24))\r\n  Downloading regex-2024.7.24-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\r\nCollecting colorama (from sacrebleu>=2.3.1->-r requirements.txt (line 24))\r\n  Using cached https://download.pytorch.org/whl/colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\nCollecting lxml (from sacrebleu>=2.3.1->-r requirements.txt (line 24))\r\n  Downloading lxml-5.2.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.4 kB)\r\nCollecting threadpoolctl>=3.1.0 (from scikit-learn>=1.2.2->-r requirements.txt (line 25))\r\n  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting python-dateutil>=2.8.2 (from pandas>=2.0.2->-r requirements.txt (line 30))\r\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\r\nCollecting pytz>=2020.1 (from pandas>=2.0.2->-r requirements.txt (line 30))\r\n  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\r\nCollecting tzdata>=2022.7 (from pandas>=2.0.2->-r requirements.txt (line 30))\r\n  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\nCollecting contourpy>=1.0.1 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Downloading contourpy-1.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.8 kB)\r\nCollecting cycler>=0.10 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\nCollecting fonttools>=4.22.0 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Downloading fonttools-4.53.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (162 kB)\r\nCollecting kiwisolver>=1.3.1 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Using cached kiwisolver-1.4.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\r\nCollecting pyparsing>=2.3.1 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\r\nCollecting safetensors>=0.3.1 (from accelerate>=0.30.1->-r requirements.txt (line 38))\r\n  Downloading safetensors-0.4.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\r\nCollecting tokenizers>=0.19.0 (from -r requirements.txt (line 42))\r\n  Using cached tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\nCollecting coloredlogs (from optimum>=1.17.1->-r requirements.txt (line 44))\r\n  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\nCollecting transformers>=4.43.2 (from -r requirements.txt (line 40))\r\n  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\r\nCollecting pyarrow>=15.0.0 (from datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached pyarrow-17.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.3 kB)\r\nCollecting pyarrow-hotfix (from datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\nCollecting fsspec (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\r\nCollecting aiohttp (from datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached aiohttp-3.10.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.5 kB)\r\nCollecting tzlocal!=3.*,>=2.0 (from APScheduler>=3.10.1->-r requirements.txt (line 48))\r\n  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\r\nCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.26.101->-r requirements.txt (line 53))\r\n  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\r\nCollecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.26.101->-r requirements.txt (line 53))\r\n  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\r\nCollecting soupsieve>1.2 (from beautifulsoup4>=4.12.2->-r requirements.txt (line 55))\r\n  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\r\nCollecting iniconfig (from pytest>=7.2.2->-r requirements.txt (line 59))\r\n  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting pluggy<2,>=1.5 (from pytest>=7.2.2->-r requirements.txt (line 59))\r\n  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\r\nCollecting exceptiongroup>=1.0.0rc8 (from pytest>=7.2.2->-r requirements.txt (line 59))\r\n  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\r\nCollecting tomli>=1 (from pytest>=7.2.2->-r requirements.txt (line 59))\r\n  Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\r\nCollecting execnet>=2.1 (from pytest-xdist>=3.2.1->-r requirements.txt (line 60))\r\n  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\r\nCollecting pyphen (from textstat>=0.7.3->-r requirements.txt (line 62))\r\n  Downloading pyphen-0.16.0-py3-none-any.whl.metadata (3.2 kB)\r\nRequirement already satisfied: setuptools in /Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages (from textstat>=0.7.3->-r requirements.txt (line 62)) (72.1.0)\r\nCollecting et-xmlfile (from openpyxl>=3.1.2->-r requirements.txt (line 68))\r\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\r\nCollecting jsonlines (from lm_dataformat>=0.0.20->-r requirements.txt (line 69))\r\n  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting ujson (from lm_dataformat>=0.0.20->-r requirements.txt (line 69))\r\n  Downloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.3 kB)\r\nCollecting zstandard (from lm_dataformat>=0.0.20->-r requirements.txt (line 69))\r\n  Downloading zstandard-0.23.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.0 kB)\r\nCollecting intervaltree (from bioc>=2.0->-r requirements.txt (line 70))\r\n  Using cached intervaltree-3.1.0-py2.py3-none-any.whl\r\nCollecting docopt (from bioc>=2.0->-r requirements.txt (line 70))\r\n  Downloading docopt-0.6.2.tar.gz (25 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting torchvision (from sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl->-r requirements.txt (line 75))\r\n  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.19.0-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\r\nCollecting distro<2,>=1.7.0 (from openai>=1.40.1->-r requirements.txt (line 91))\r\n  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\nCollecting jiter<1,>=0.4.0 (from openai>=1.40.1->-r requirements.txt (line 91))\r\n  Using cached jiter-0.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.6 kB)\r\nCollecting sniffio (from openai>=1.40.1->-r requirements.txt (line 91))\r\n  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\r\nCollecting charset-normalizer<4,>=2 (from requests>=2.31.0->-r requirements.txt (line 93))\r\n  Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (33 kB)\r\nCollecting idna<4,>=2.5 (from requests>=2.31.0->-r requirements.txt (line 93))\r\n  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\r\nCollecting certifi>=2017.4.17 (from requests>=2.31.0->-r requirements.txt (line 93))\r\n  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\r\nCollecting httpcore==1.* (from httpx>=0.24.1->-r requirements.txt (line 94))\r\n  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\r\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached aiohappyeyeballs-2.3.5-py3-none-any.whl.metadata (5.8 kB)\r\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\r\nCollecting attrs>=17.3.0 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\r\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\r\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Downloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\r\nCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (31 kB)\r\nCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\r\nCollecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\r\nCollecting toolz (from altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\r\nCollecting starlette (from sse_starlette>=1.8.2->-r requirements.txt (line 14))\r\n  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\r\nCollecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\r\nCollecting pydantic-core==2.18.1 (from pydantic>=2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached pydantic_core-2.18.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.5 kB)\r\nCollecting protobuf (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum>=1.17.1->-r requirements.txt (line 44))\r\n  Downloading protobuf-5.27.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\r\nCollecting shellingham>=1.3.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\r\nCollecting rich>=10.11.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\r\nWARNING: typer 0.12.3 does not provide the extra 'all'\r\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum>=1.17.1->-r requirements.txt (line 44))\r\n  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\nCollecting sortedcontainers<3.0,>=2.0 (from intervaltree->bioc>=2.0->-r requirements.txt (line 70))\r\n  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\r\nCollecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.3.1->-r requirements.txt (line 21))\r\n  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\r\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\r\nCollecting torchvision (from sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl->-r requirements.txt (line 75))\r\n  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.18.1-cp310-cp310-macosx_11_0_arm64.whl (1.6 MB)\r\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\r\nCollecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading rpds_py-0.20.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\r\nCollecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\nCollecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\r\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\nUsing cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\r\nUsing cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\r\nUsing cached gunicorn-22.0.0-py3-none-any.whl (84 kB)\r\nUsing cached fastapi_utils-0.7.0-py3-none-any.whl (18 kB)\r\nDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\r\nUsing cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\r\nUsing cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\nDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 587.4/587.4 kB 3.5 MB/s eta 0:00:00\r\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\r\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\r\nDownloading scikit_learn-1.5.1-cp310-cp310-macosx_12_0_arm64.whl (11.0 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/11.0 MB 2.9 MB/s eta 0:00:00\r\nDownloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.0/14.0 MB 3.0 MB/s eta 0:00:00\r\nDownloading pandas-2.2.2-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 3.1 MB/s eta 0:00:00\r\nDownloading matplotlib-3.9.1.post1-cp310-cp310-macosx_11_0_arm64.whl (7.8 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 2.3 MB/s eta 0:00:00\r\nUsing cached loralib-0.1.2-py3-none-any.whl (10 kB)\r\nUsing cached accelerate-0.33.0-py3-none-any.whl (315 kB)\r\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\r\nUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\r\nUsing cached tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\r\nDownloading hf_transfer-0.1.8-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 2.2 MB/s eta 0:00:00\r\nUsing cached optimum-1.21.3-py3-none-any.whl (421 kB)\r\nUsing cached transformers-4.43.4-py3-none-any.whl (9.4 MB)\r\nUsing cached datasets-2.20.0-py3-none-any.whl (547 kB)\r\nUsing cached sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\r\nDownloading APScheduler-3.10.4-py3-none-any.whl (59 kB)\r\nDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\r\nUsing cached psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl (249 kB)\r\nDownloading boto3-1.34.158-py3-none-any.whl (139 kB)\r\nDownloading botocore-1.34.158-py3-none-any.whl (12.5 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/12.5 MB 2.5 MB/s eta 0:00:00\r\nUsing cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\r\nDownloading Markdown-3.6-py3-none-any.whl (105 kB)\r\nDownloading pytest-8.3.2-py3-none-any.whl (341 kB)\r\nDownloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\r\nUsing cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\nDownloading textstat-0.7.4-py3-none-any.whl (105 kB)\r\nUsing cached pypandoc-1.13-py3-none-any.whl (21 kB)\r\nDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\r\nUsing cached lm_dataformat-0.0.20-py3-none-any.whl (5.8 kB)\r\nDownloading bioc-2.1-py3-none-any.whl (33 kB)\r\nUsing cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\r\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\r\nUsing cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\r\nUsing cached json_repair-0.27.0-py3-none-any.whl (12 kB)\r\nUsing cached text_generation-0.7.0-py3-none-any.whl (12 kB)\r\nUsing cached tiktoken-0.7.0-cp310-cp310-macosx_11_0_arm64.whl (906 kB)\r\nUsing cached openai-1.40.2-py3-none-any.whl (360 kB)\r\nUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\r\nUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\r\nUsing cached httpcore-1.0.5-py3-none-any.whl (77 kB)\r\nUsing cached urllib3-2.2.2-py3-none-any.whl (121 kB)\r\nUsing cached filelock-3.15.4-py3-none-any.whl (16 kB)\r\nUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\r\nUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\r\nUsing cached tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nUsing cached packaging-24.1-py3-none-any.whl (53 kB)\r\nUsing cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\r\nUsing cached aiohttp-3.10.2-cp310-cp310-macosx_11_0_arm64.whl (387 kB)\r\nUsing cached altair-5.3.0-py3-none-any.whl (857 kB)\r\nUsing cached anyio-4.4.0-py3-none-any.whl (86 kB)\r\nUsing cached certifi-2024.7.4-py3-none-any.whl (162 kB)\r\nUsing cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)\r\nUsing cached click-8.1.7-py3-none-any.whl (97 kB)\r\nDownloading contourpy-1.2.1-cp310-cp310-macosx_11_0_arm64.whl (244 kB)\r\nUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\nDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\nUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\r\nDownloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\r\nDownloading execnet-2.1.1-py3-none-any.whl (40 kB)\r\nUsing cached fastapi-0.112.0-py3-none-any.whl (93 kB)\r\nDownloading fonttools-4.53.1-cp310-cp310-macosx_11_0_arm64.whl (2.2 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 2.6 MB/s eta 0:00:00\r\nUsing cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\r\nUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\r\nUsing cached httptools-0.6.1-cp310-cp310-macosx_10_9_universal2.whl (149 kB)\r\nUsing cached idna-3.7-py3-none-any.whl (66 kB)\r\nUsing cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\r\nUsing cached jiter-0.5.0-cp310-cp310-macosx_11_0_arm64.whl (299 kB)\r\nUsing cached jmespath-1.0.1-py3-none-any.whl (20 kB)\r\nUsing cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\r\nUsing cached kiwisolver-1.4.5-cp310-cp310-macosx_11_0_arm64.whl (66 kB)\r\nDownloading lxml-5.2.2-cp310-cp310-macosx_10_9_universal2.whl (8.1 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.1/8.1 MB 2.2 MB/s eta 0:00:00\r\nUsing cached orjson-3.10.6-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (250 kB)\r\nUsing cached pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl (3.4 MB)\r\nUsing cached pluggy-1.5.0-py3-none-any.whl (20 kB)\r\nUsing cached pyarrow-17.0.0-cp310-cp310-macosx_11_0_arm64.whl (27.2 MB)\r\nUsing cached pydantic-2.7.0-py3-none-any.whl (407 kB)\r\nUsing cached pydantic_core-2.18.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\r\nUsing cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\r\nUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\r\nUsing cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\r\nUsing cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\r\nDownloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\r\nDownloading regex-2024.7.24-cp310-cp310-macosx_11_0_arm64.whl (278 kB)\r\nDownloading ruff-0.5.7-py3-none-macosx_11_0_arm64.whl (8.3 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.3/8.3 MB 1.1 MB/s eta 0:00:00\r\nUsing cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\r\nDownloading safetensors-0.4.4-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\r\nDownloading scipy-1.14.0-cp310-cp310-macosx_14_0_arm64.whl (23.1 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.1/23.1 MB 762.5 kB/s eta 0:00:00\r\nUsing cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\nUsing cached six-1.16.0-py2.py3-none-any.whl (11 kB)\r\nUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\r\nUsing cached soupsieve-2.5-py3-none-any.whl (36 kB)\r\nUsing cached starlette-0.37.2-py3-none-any.whl (71 kB)\r\nDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\r\nUsing cached tomli-2.0.1-py3-none-any.whl (12 kB)\r\nUsing cached typer-0.12.3-py3-none-any.whl (47 kB)\r\nUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\nUsing cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\r\nUsing cached tzlocal-5.2-py3-none-any.whl (17 kB)\r\nDownloading uvicorn-0.30.5-py3-none-any.whl (62 kB)\r\nUsing cached uvloop-0.19.0-cp310-cp310-macosx_10_9_universal2.whl (1.4 MB)\r\nDownloading watchfiles-0.23.0-cp310-cp310-macosx_11_0_arm64.whl (369 kB)\r\nUsing cached websockets-11.0.3-cp310-cp310-macosx_11_0_arm64.whl (121 kB)\r\nUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\r\nUsing cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\nUsing cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\r\nDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\r\nUsing cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\nDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\r\nDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 909.3 kB/s eta 0:00:00\r\nUsing cached portalocker-2.10.1-py3-none-any.whl (18 kB)\r\nUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\nUsing cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\r\nDownloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 918.1 kB/s eta 0:00:00\r\nDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 921.7 kB/s eta 0:00:00\r\nUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\r\nDownloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl (51 kB)\r\nUsing cached xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\r\nDownloading zstandard-0.23.0-cp310-cp310-macosx_11_0_arm64.whl (633 kB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 633.7/633.7 kB 861.3 kB/s eta 0:00:00\r\nUsing cached aiohappyeyeballs-2.3.5-py3-none-any.whl (12 kB)\r\nUsing cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\nUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\nUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\r\nDownloading attrs-24.2.0-py3-none-any.whl (63 kB)\r\nUsing cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\r\nUsing cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\nUsing cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\r\nDownloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\r\nUsing cached rich-13.7.1-py3-none-any.whl (240 kB)\r\nUsing cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\r\nUsing cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\nUsing cached yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\r\nDownloading protobuf-5.27.3-cp38-abi3-macosx_10_9_universal2.whl (412 kB)\r\nUsing cached toolz-0.12.1-py3-none-any.whl (56 kB)\r\nUsing cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\r\nUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\nUsing cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\r\nUsing cached referencing-0.35.1-py3-none-any.whl (26 kB)\r\nDownloading rpds_py-0.20.0-cp310-cp310-macosx_11_0_arm64.whl (311 kB)\r\nUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\nBuilding wheels for collected packages: fire, docopt\r\n  Building wheel for fire (setup.py) ... done\r\n  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=7185a126050d4222c7b567a05444f5ee59c01d506a821f71e871b44a7d189e18\r\n  Stored in directory: /Users/kapenge/Library/Caches/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\r\n  Building wheel for docopt (setup.py) ... done\r\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=be75b6164fc47568d338658282162b0ee172034cd5ecb69285c9e7a743845da3\r\n  Stored in directory: /Users/kapenge/Library/Caches/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\r\nSuccessfully built fire docopt\r\nInstalling collected packages: sortedcontainers, sentencepiece, pytz, pydub, mpmath, InstructorEmbedding, docopt, appdirs, zstandard, xxhash, websockets, uvloop, urllib3, ujson, tzlocal, tzdata, typing-extensions, tqdm, toolz, tomlkit, tomli, threadpoolctl, termcolor, tabulate, sympy, soupsieve, sniffio, six, shellingham, semantic-version, safetensors, ruff, rpds-py, regex, pyyaml, python-multipart, python-dotenv, pyphen, pyparsing, pypandoc, pynvml, pygments, pyarrow-hotfix, psutil, protobuf, portalocker, pluggy, pillow, packaging, orjson, numpy, networkx, multidict, mdurl, markupsafe, markdown, lxml, loralib, kiwisolver, json_repair, joblib, jmespath, jiter, intervaltree, iniconfig, importlib-resources, idna, humanfriendly, httptools, hf_transfer, h11, fsspec, frozenlist, fonttools, filelock, ffmpy, execnet, exceptiongroup, et-xmlfile, einops, docutils, distro, dill, cycler, colorama, click, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, absl-py, yarl, uvicorn, textstat, scipy, sacrebleu, requests, referencing, python-dateutil, pytest, pydantic-core, pyarrow, openpyxl, nltk, multiprocess, markdown-it-py, jsonlines, jinja2, httpcore, gunicorn, fire, contourpy, coloredlogs, beautifulsoup4, APScheduler, anyio, aiosignal, watchfiles, torch, tiktoken, starlette, scikit-learn, rouge_score, rich, pytest-xdist, pydantic, pandas, matplotlib, lm_dataformat, jsonschema-specifications, huggingface_hub, httpx, botocore, bitsandbytes, bioc, aiohttp, typer, torchvision, tokenizers, text-generation, sse_starlette, s3transfer, openai, jsonschema, gradio_client, fastapi, accelerate, transformers, fastapi-utils, datasets, boto3, altair, sentence_transformers_old, sentence_transformers, peft, gradio, evaluate, optimum\r\nSuccessfully installed APScheduler-3.10.4 InstructorEmbedding-1.0.1 absl-py-2.1.0 accelerate-0.33.0 aiofiles-23.2.1 aiohappyeyeballs-2.3.5 aiohttp-3.10.2 aiosignal-1.3.1 altair-5.3.0 annotated-types-0.7.0 anyio-4.4.0 appdirs-1.4.4 async-timeout-4.0.3 attrs-24.2.0 beautifulsoup4-4.12.3 bioc-2.1 bitsandbytes-0.42.0 boto3-1.34.158 botocore-1.34.158 certifi-2024.7.4 charset-normalizer-3.3.2 click-8.1.7 colorama-0.4.6 coloredlogs-15.0.1 contourpy-1.2.1 cycler-0.12.1 datasets-2.20.0 dill-0.3.8 distro-1.9.0 docopt-0.6.2 docutils-0.21.2 einops-0.8.0 et-xmlfile-1.1.0 evaluate-0.4.2 exceptiongroup-1.2.2 execnet-2.1.1 fastapi-0.112.0 fastapi-utils-0.7.0 ffmpy-0.4.0 filelock-3.15.4 fire-0.6.0 fonttools-4.53.1 frozenlist-1.4.1 fsspec-2024.5.0 gradio-4.26.0 gradio_client-0.15.1 gunicorn-22.0.0 h11-0.14.0 hf_transfer-0.1.8 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface_hub-0.24.5 humanfriendly-10.0 idna-3.7 importlib-resources-6.4.0 iniconfig-2.0.0 intervaltree-3.1.0 jinja2-3.1.4 jiter-0.5.0 jmespath-1.0.1 joblib-1.4.2 json_repair-0.27.0 jsonlines-4.0.0 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 kiwisolver-1.4.5 lm_dataformat-0.0.20 loralib-0.1.2 lxml-5.2.2 markdown-3.6 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.1.post1 mdurl-0.1.2 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.3 nltk-3.8.1 numpy-1.26.4 openai-1.40.2 openpyxl-3.1.5 optimum-1.21.3 orjson-3.10.6 packaging-24.1 pandas-2.2.2 peft-0.12.0 pillow-10.4.0 pluggy-1.5.0 portalocker-2.10.1 protobuf-5.27.3 psutil-5.9.8 pyarrow-17.0.0 pyarrow-hotfix-0.6 pydantic-2.7.0 pydantic-core-2.18.1 pydub-0.25.1 pygments-2.18.0 pynvml-11.5.3 pypandoc-1.13 pyparsing-3.1.2 pyphen-0.16.0 pytest-8.3.2 pytest-xdist-3.6.1 python-dateutil-2.9.0.post0 python-dotenv-1.0.1 python-multipart-0.0.9 pytz-2024.1 pyyaml-6.0.2 referencing-0.35.1 regex-2024.7.24 requests-2.32.3 rich-13.7.1 rouge_score-0.1.2 rpds-py-0.20.0 ruff-0.5.7 s3transfer-0.10.2 sacrebleu-2.4.2 safetensors-0.4.4 scikit-learn-1.5.1 scipy-1.14.0 semantic-version-2.10.0 sentence_transformers-3.0.1 sentence_transformers_old-2.2.2 sentencepiece-0.2.0 shellingham-1.5.4 six-1.16.0 sniffio-1.3.1 sortedcontainers-2.4.0 soupsieve-2.5 sse_starlette-2.1.3 starlette-0.37.2 sympy-1.13.1 tabulate-0.9.0 termcolor-2.4.0 text-generation-0.7.0 textstat-0.7.4 threadpoolctl-3.5.0 tiktoken-0.7.0 tokenizers-0.19.1 tomli-2.0.1 tomlkit-0.12.0 toolz-0.12.1 torch-2.3.1 torchvision-0.18.1 tqdm-4.66.5 transformers-4.43.4 typer-0.12.3 typing-extensions-4.12.2 tzdata-2024.1 tzlocal-5.2 ujson-5.10.0 urllib3-2.2.2 uvicorn-0.30.5 uvloop-0.19.0 watchfiles-0.23.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4 zstandard-0.23.0\r\n❯\r\n❯\r\n❯ pip install -r reqs_optional/requirements_optional_langchain.txt -c reqs_optional/reqs_constraints.txt\r\nIgnoring torch: markers 'sys_platform != \"darwin\" and platform_machine != \"arm64\"' don't match your environment\r\nCollecting InstructorEmbedding@ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 23))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (20 kB)\r\nCollecting sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 26))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (125 kB)\r\nCollecting llava@ https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 41))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl (87 kB)\r\nIgnoring jq: markers 'platform_machine == \"x86_64\"' don't match your environment\r\nERROR: Cannot install torch==2.2.1 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    The user requested torch==2.2.1\r\n    The user requested (constraint) torch==2.3.1\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip to attempt to solve the dependency conflict\r\n\r\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\r\n```"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1788,
    "title": "Chat history silently fails to download",
    "author": "antoninadert",
    "state": "closed",
    "created_at": "2024-08-08T09:21:21Z",
    "updated_at": "2024-08-09T18:02:16Z",
    "labels": [],
    "body": "No matter the chat history count, when I click \"Exports chats to Download\", it gives an empty file of 2.0b\r\n\r\nSee screenshot\r\n\r\n<img width=\"1108\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0489c5bf-2086-43a2-a35d-1f1941f455ff\">\r\n\r\n\r\nThere is no error in the command line. \r\n\r\nIs this a known issue ?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It only saves the \"chat saved\" items after clicking save.  The current chat view isn't saved except in database for the user."
      },
      {
        "user": "pseudotensor",
        "body": "![image](https://github.com/user-attachments/assets/91820c73-a27d-4d87-916e-1d818f1a105f)\r\n\r\n\r\n\r\nMake sure to click \"save\" among buttons to save it so it would be part of download.\r\n\r\n![image](https://github.com/user-attachments/assets/d875a7f1-82a8-473f-81cf-a8c204f5ce99)\r\n\r\n\r\nI would prefer it saved all things, but the database takes care of saving things for that general purpose."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1787,
    "title": "TypeError: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-08-08T08:16:29Z",
    "updated_at": "2024-08-09T07:24:09Z",
    "labels": [],
    "body": "This is the command line lauched:\r\n```shell\r\npython generate.py \\\r\n    --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --prompt_type=llama2 \\\r\n    --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 \\\r\n    --user_path=/opt/myDocuments/arsys.es/all/ --langchain_mode='UserData' --max_quality=True \\\r\n    --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True \\\r\n    --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True \\\r\n    --hf_embedding_model=\"hkunlp/instructor-base\" \\\r\n    --memory_restriction_level=0 --score_model=None --verbose=True --debug=True\r\n```\r\n---\r\nThis is the error found:\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/gen.py\", line 2015, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/gpt_langchain.py\", line 556, in get_embedding\r\n    embedding = HuggingFaceInstructEmbeddings(model_name=hf_embedding_model,\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 167, in __init__\r\n    self.client = INSTRUCTOR(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 287, in __init__\r\n    modules = self._load_sbert_model(\r\nTypeError: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'\r\n```\r\n---\r\nSystem:\r\n- SO: Ubuntu 22.04 LTS utterly updated.\r\n- Commit: 7435b4bc (the last one at the moment this issue is written)\r\n- Environment utterly updated and clean after execution of script ```bash docs/linux_install_full.sh```\r\n---\r\nObservations:\r\n- This only happens with embedding models based on BERT\r\n- Embedding models as \"intfloat/multilingual-e5-small\" does not report any problem.\r\n---\r\nThanks for everything.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It seems the deps are not installed that should be.\r\n\r\nIn requierments_optional_langchain.txt it has:\r\n```\r\nsentence_transformers>=3.0.1\r\nInstructorEmbedding @ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl\r\nsentence_transformers_old @ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl\r\n```\r\n\r\nand for me I see no failure because inside `$HOME/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/InstructorEmbedding/instructor.py` it references only:\r\n```\r\nfrom sentence_transformers_old import SentenceTransformer\r\nfrom sentence_transformers_old.models import Transformer\r\n```\r\n\r\nCan you check your file and see what it shows?  I presume not the \"old\" ones but normal.  So explains failure.\r\n\r\nHowever, doesn't explain why you have wrong packages.  I just redid install and see these good packages used.\r\n\r\n\r\nYou should be able to do just:\r\n```\r\nfrom InstructorEmbedding import INSTRUCTOR\r\n```\r\nin python and it shouldn't fail.  So not related to h2oGPT itself, just those two packages.\r\n\r\nIn clean docker I also see this works fine."
      },
      {
        "user": "pseudotensor",
        "body": "677bf0817d3e342ffc32a31a94cf63cc05096660\r\n794ec254460a0c38a2e3ae3e4437f5dc0f695a09"
      },
      {
        "user": "pseudotensor",
        "body": "Building new image with above fix to see if order helps.  I saw in jenkins that earlier requirements.txt file triggered instructorembedding install, so needs to be early and not as late as langchain one."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1772,
    "title": "Implement custom function call.",
    "author": "anushaharish538",
    "state": "closed",
    "created_at": "2024-07-26T13:32:01Z",
    "updated_at": "2024-08-09T05:34:02Z",
    "labels": [],
    "body": "Hi,\r\n\r\nCan you please tell me how to implement custom function calling like. https://platform.openai.com/docs/guides/function-calling\r\n\r\n\r\nThank You\r\nAnusha",
    "comments": [
      {
        "user": "anushaharish538",
        "body": "hi @pseudotensor Can you please tell me . Please I'm using open_ai"
      },
      {
        "user": "pseudotensor",
        "body": "Hi, function calling is now part of h2oGPT OpenAI API, with 'auto' tool_choice, but it's basic at moment."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1784,
    "title": "Running Llama 3.1 on Mac OS with m2 chip has errors",
    "author": "antoninadert",
    "state": "open",
    "created_at": "2024-08-05T08:22:16Z",
    "updated_at": "2024-08-08T10:51:14Z",
    "labels": [],
    "body": "I tried to run h2ogpt with this command : \r\n\r\n`python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --use_auth_token=...`\r\n\r\nand it triggered errors\r\n\r\n```The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\r\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nthread exception: Traceback (most recent call last):\r\n  File \"/Users/.../h2ogpt/src/utils.py\", line 524, in run\r\n    self._return = self._target(*self._args, **self._kwargs)\r\n  File \"/Users/.../h2ogpt/src/gen.py\", line 4288, in generate_with_exceptions\r\n    func(*args, **kwargs)\r\n  File \"/Users/.../miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/Users/.../miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1727, in generate\r\n    model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\r\n  File \"/Users/.../miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 493, in _prepare_attention_mask_for_generation\r\n    raise ValueError(\r\nValueError: Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.```\r\n\r\nIt worked fine when I tried to run other older models like llama2 for example.\r\n\r\nDo you know what could be the source of this issue ?\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Looks like transformers bug: https://github.com/huggingface/transformers/issues/31744\r\n\r\nBut need new transformers for llama 3.1.\r\n\r\nMaybe stick to GGUF?"
      },
      {
        "user": "antoninadert",
        "body": "It worked fine with GGUF, I had to install different package version than what is recommended though. I will propose a pull request to be able to start with llama 3.1"
      },
      {
        "user": "antoninadert",
        "body": "See https://github.com/h2oai/h2ogpt/pull/1789 which is how I made llama 3.1 gguf work on Mac M2"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1774,
    "title": "intfloat_multilingual-e5-large-instruct and file sentence_xlnet_config.json",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-07-30T06:58:29Z",
    "updated_at": "2024-08-02T08:09:44Z",
    "labels": [],
    "body": "I have executed the following command line:\r\n```shell\r\npython generate.py \\\r\n    --base_model=mistralai/Mistral-Nemo-Instruct-2407 --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 \\\r\n    --user_path=/opt/myDocuments/all/ --langchain_mode='UserData' --max_quality=True \\\r\n    --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True \\\r\n    --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True \\\r\n    --hf_embedding_model=\"intfloat/multilingual-e5-large-instruct\" \\\r\n    --memory_restriction_level=0 --score_model=None --verbose=True --debug=True \\\r\n    --show_examples=True --compile_model=True \\\r\n    --share=True\r\n``` \r\nand I get the following error:\r\n```shell\r\nFileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/torch/sentence_transformers/intfloat_multilingual-e5-large-instruct/sentence_xlnet_config.json'\r\n```\r\n\r\nI have used the repository with the last commit at the time I write this issue:\r\n- Commit: 47e14672\r\n- SO: Ubuntu 20.04\r\n\r\nAs you can see the problem point out that problem is from the embedding model \"intfloat/multilingual-e5-large-instruct\", with the embedding model \"\"intfloat/multilingual-e5-large\" I did not get that kind of error.\r\n\r\nThanks in advance for help.",
    "comments": [
      {
        "user": "llmwesee",
        "body": "I'm also facing the same thing when running docker offline"
      },
      {
        "user": "pseudotensor",
        "body": "If I'm online, I see the same error.\r\n\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py     --base_model=llama --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192     --user_path=user_path --langchain_mode='UserData' --max_quality=True     --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True     --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True     --hf_embedding_model=\"intfloat/multilingual-e5-large-instruct\"     --memory_restriction_level=0 --score_model=None --verbose=True --debug=True     --show_examples=True --compile_model=True     --share=True\r\n```\r\ngives:\r\n```\r\nCommand: generate.py --base_model=llama --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 --user_path=user_path --langchain_mode=UserData --max_quality=True --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True --hf_embedding_model=intfloat/multilingual-e5-large-instruct --memory_restriction_level=0 --score_model=None --verbose=True --debug=True --show_examples=True --compile_model=True --share=True\r\nHash: 47e14672da4fc926aef7eeccd09d1119f112db3c\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n  warn_deprecated(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\r\n  warnings.warn(warning_message, FutureWarning)\r\n.gitattributes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.57k/1.57k [00:00<00:00, 4.43MB/s]\r\n1_Pooling/config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 271/271 [00:00<00:00, 3.06MB/s]\r\nREADME.md: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140k/140k [00:00<00:00, 40.4MB/s]\r\nconfig.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 690/690 [00:00<00:00, 1.72MB/s]\r\nconfig_sentence_transformers.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:00<00:00, 368kB/s]\r\nmodel.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1.12G/1.12G [00:11<00:00, 93.5MB/s]\r\nsentencepiece.bpe.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.07M/5.07M [00:00<00:00, 27.0MB/s]\r\nspecial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 964/964 [00:00<00:00, 2.79MB/s]\r\ntokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17.1M/17.1M [00:00<00:00, 37.0MB/s]\r\ntokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 3.37MB/s]\r\nmodules.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 886kB/s]\r\nWARNING:sentence_transformers.SentenceTransformer:You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.2.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jon/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/jon/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/jon/h2ogpt/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/jon/h2ogpt/src/gen.py\", line 1996, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 558, in get_embedding\r\n    embedding = HuggingFaceEmbeddings(model_name=hf_embedding_model, model_kwargs=model_kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 203, in warn_if_direct_instance\r\n    return wrapped(self, *args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 79, in __init__\r\n    self.client = sentence_transformers.SentenceTransformer(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 95, in __init__\r\n    modules = self._load_sbert_model(model_path)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 840, in _load_sbert_model\r\n    module = module_class.load(os.path.join(model_path, module_config['path']))\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py\", line 135, in load\r\n    with open(sbert_config_path) as fIn:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/jon/.cache/torch/sentence_transformers/intfloat_multilingual-e5-large-instruct/sentence_xlnet_config.json'\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```\r\n\r\nI'm guessing sentence-transforemrs==2.2.2 is too old for this model.  However, that version is required for langchain's HF embedding wrapper code to work.\r\n\r\nOtherwise I get an error from sentence transformers complaining about how Langchain is misusing it:\r\n\r\n```\r\nCommand: generate.py --base_model=llama --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 --user_path=user_path --langchain_mode=UserData --max_quality=True --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True --hf_embedding_model=intfloat/multilingual-e5-large-instruct --memory_restriction_level=0 --score_model=None --verbose=True --debug=True --show_examples=True --compile_model=True --share=True\r\nHash: 47e14672da4fc926aef7eeccd09d1119f112db3c\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n  warn_deprecated(\r\nmodules.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 603kB/s]\r\nconfig_sentence_transformers.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:00<00:00, 387kB/s]\r\nREADME.md: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140k/140k [00:00<00:00, 6.15MB/s]\r\nconfig.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 690/690 [00:00<00:00, 2.11MB/s]\r\nmodel.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1.12G/1.12G [00:10<00:00, 103MB/s]\r\ntokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 3.64MB/s]\r\nsentencepiece.bpe.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.07M/5.07M [00:00<00:00, 5.64MB/s]\r\ntokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17.1M/17.1M [00:01<00:00, 14.5MB/s]\r\nspecial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 964/964 [00:00<00:00, 2.83MB/s]\r\n1_Pooling/config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 271/271 [00:00<00:00, 2.50MB/s]\r\nPrep: persist_directory=db_dir_UserData exists, user_path=user_path passed, adding any changed or new documents\r\nDO Loading db: UserData\r\nTraceback (most recent call last):\r\n  File \"/home/jon/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/jon/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/jon/h2ogpt/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/jon/h2ogpt/src/gen.py\", line 2121, in main\r\n    db = prep_langchain(persist_directory1,\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 5701, in prep_langchain\r\n    db, num_new_sources, new_sources_metadata = make_db(**langchain_kwargs)\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 5962, in make_db\r\n    return _make_db(**langchain_kwargs)\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 6187, in _make_db\r\n    get_existing_db(db, persist_directory, load_db_if_exists, db_type,\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 5858, in get_existing_db\r\n    embedding = get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 553, in get_embedding\r\n    embedding = HuggingFaceInstructEmbeddings(model_name=hf_embedding_model,\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 167, in __init__\r\n    self.client = INSTRUCTOR(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 287, in __init__\r\n    modules = self._load_sbert_model(\r\nTypeError: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "However, if you have no databases with instructor models, you can do:\r\n```\r\npip install sentence-transformers --upgrade\r\n```\r\n\r\nAnd it'll work fine.\r\n\r\nThen the above commands work.\r\n\r\ni.e. somehow langchain instructor class is broken for new sentence-transformers."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1777,
    "title": "Doc file splitting into multiple image files",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-07-31T05:14:26Z",
    "updated_at": "2024-07-31T16:01:08Z",
    "labels": [],
    "body": "When uploading .doc or .docx files, the following warnings are displayed:\r\n `No acceptable contours found `\r\n `Contour is not a quadrilateral `\r\n`lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\r\n  warn_deprecated(`\r\n\r\nAfter uploading, a single .doc file splits into around 59 documents in .png or .jpeg formats. These are shown in the Doc Counts sidebar and also form the metadata as illustrated in the attached screenshot.\r\n\r\n![Screenshot from 2024-07-31 10-02-10](https://github.com/user-attachments/assets/df2b99eb-cc89-48be-96a8-92bbad5161d5)\r\n\r\nThe main issue is the inability of the .doc file to be parsed as a single document, unlike .pdf files. Instead, it splits into multiple .png or .jpeg files, leading to hallucination during querying.\r\n\r\nPlease address the parsing issue and provide a solution to handle .doc or .docx files correctly without splitting into multiple image files.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "We extract images so that they can be processed separately for image question-answer, but I understand if there are many images it might get messy.\r\n\r\nI added an ENV so you can control with `H2OGPT_DOCX_EXTRACT_IMAGES`.  set it to \"0\" to avoid this step.\r\n\r\nDocker with this feature will be in new build in few hours.\r\n\r\n\r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1770,
    "title": "Llama 3.1 support",
    "author": "bw-Deejee",
    "state": "closed",
    "created_at": "2024-07-25T10:19:49Z",
    "updated_at": "2024-07-25T21:15:14Z",
    "labels": [],
    "body": "Are llama3.1 models currently supported?\r\n\r\nI pulled recent repo however, didn't go through the full installation requirements and the following doesn't work:\r\n\r\n`python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --use_auth_token=...`\r\n\r\nError message:\r\n`ValueError: 'rope_scaling' must be a dictionary with two fields, 'type' and 'factor', got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}`",
    "comments": [
      {
        "user": "bw-Deejee",
        "body": "Nevermind. I looked deeper in to repo changes and it looks like i just had to rerun:\r\n`pip install -r requirements.txt -c reqs_optional/reqs_constraints.txt`\r\nin the newest repo version and now it works."
      },
      {
        "user": "pseudotensor",
        "body": "Ya new transformers was required. Thanks."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1767,
    "title": "Docker Environment Variables / Arguments & Llama3.1 Support",
    "author": "plitc",
    "state": "closed",
    "created_at": "2024-07-24T07:58:28Z",
    "updated_at": "2024-07-24T22:04:29Z",
    "labels": [],
    "body": "Hello H2OGPT Team,\r\n\r\ncan you please list a Docker example that exactly reflects the functionality of https://gpt.h2o.ai/?\r\n\r\nWe currently cannot load the new Llama3.1 and with the option: prompt_type=llama the model only spits out confusing stuff.\r\n\r\n`root@ai-ubuntu22gpu-gpt:/opt# cat run-latest4`\r\n`#!/bin/sh`\r\n`export HUGGING_FACE_HUB_TOKEN=...PRIVATE...`\r\n`export CUDA_VISIBLE_DEVICES=\"0,1,2,3\"`\r\n`docker run \\`\r\n`       --gpus='\"device=0,1,2,3\"' \\`\r\n`       --runtime=nvidia \\`\r\n`       --shm-size=16g \\`\r\n`       -p 7860:7860 \\`\r\n`       --rm --init \\`\r\n`       -v /etc/passwd:/etc/passwd:ro \\`\r\n`       -v /etc/group:/etc/group:ro \\`\r\n`       -u 1000:1000 \\`\r\n`       -v /opt/h2o_gpt_data/.cache:/workspace/.cache \\`\r\n`       -v /opt/h2o_gpt_data/save:/workspace/save \\`\r\n`       -v /opt/h2o_gpt_data/db_dir_UserData:/workspace/db_dir_UserData \\`\r\n`       -v /opt/h2o_gpt_data/tmp:/tmp \\`\r\n`       -v /opt/CEPHFS_DATA:/workspace/user_path/CACHE \\`\r\n`       -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\`\r\n`       -e TOKENIZERS_PARALLELISM=false \\`\r\n`       -e ADMIN_PASS=...PRIVATE... \\`\r\n`       -e CONCURRENCY_COUNT=1 \\`\r\n`       -e API_OPEN=1 \\`\r\n`       -e ALLOW_API=1 \\`\r\n`       gcr.io/vorvan/h2oai/h2ogpt-runtime:latest /workspace/generate.py \\`\r\n`          --score_model=None \\`\r\n`          --max_output_seq_len=8192 \\`\r\n`          --max_seq_len=8192 \\`\r\n`          --prompt_type=llama3 \\`\r\n`          --use_safetensors=True \\`\r\n`          --base_model=meta-llama/Meta-Llama-3.1-8B \\`\r\n`          --tokenizer-path=meta-llama/Meta-Llama-3.1-8B \\`\r\n`          --save_dir='/workspace/save/' \\`\r\n`          --use_auth_token=$HUGGING_FACE_HUB_TOKEN \\`\r\n`          --use_gpu_id=False \\`\r\n`          --allow_upload_to_user_data=False \\`\r\n`          --allow_upload_to_my_data=True \\`\r\n`          --enable_ocr='off' \\`\r\n`          --enable_pdf_ocr='off' \\`\r\n`          --langchain_mode='UserData' \\`\r\n`          --langchain_modes=\"['LLM', 'UserData', 'MyData']\" \\`\r\n`          --user_path=/workspace/user_path \\`\r\n`          --db_type=chroma \\`\r\n`          --visible_h2ogpt_header=False \\`\r\n`          --visible_doc_selection_tab=False \\`\r\n`          --visible_doc_view_tab=False \\`\r\n`          --visible_chat_history_tab=False \\`\r\n`          --visible_expert_tab=False \\`\r\n`          --visible_models_tab=False \\`\r\n`          --visible_system_tab=False \\`\r\n`          --visible_tos_tab=False \\`\r\n`          --visible_hosts_tab=False \\`\r\n`          --answer_with_sources=False \\`\r\n`        --visible_side_bar=True \\`\r\n`          --visible_login_tab=False \\`\r\n`          --visible_visible_models=False \\`\r\n`        --visible_submit_buttons=True \\`\r\n`        --visible_chat_tab=True \\`\r\n`          --visible_h2ogpt_links=False \\`\r\n`          --visible_h2ogpt_qrcode=False \\`\r\n`          --visible_h2ogpt_logo=False \\`\r\n`          --visible_chatbot_label=False`\r\n` # EOF`\r\n`root@ai-ubuntu22gpu-gpt:/opt#`\r\n\r\nbeste regards\r\nDaniel Plominski",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Try not passing prompt_type. It is not required."
      },
      {
        "user": "plitc",
        "body": "without prompt_type =  the model only spits out confusing stuff\r\n\r\nLogoutput\r\n\r\n`INFO:     172.17.0.1:40054 - \"GET /queue/data?session_hash=e0apa8hwqu HTTP/1.1\" 200 OK\r\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\r\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nINFO:     172.17.0.1:43008 - \"GET / HTTP/1.1\" 200 OK\r\n`"
      },
      {
        "user": "plitc",
        "body": "[pseudotensor](https://github.com/pseudotensor)\r\n\r\ncan you please list a Docker example that exactly reflects the functionality of https://gpt.h2o.ai/?\r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1743,
    "title": "Support private CA",
    "author": "tomkraljevic",
    "state": "closed",
    "created_at": "2024-07-16T17:22:43Z",
    "updated_at": "2024-07-23T17:16:43Z",
    "labels": [],
    "body": "# What happens currently\r\n\r\nWhen the models that h2ogpt are pointing to are https and signed with a private CA the connection attempt errors out with an untrusted SSL certificate error.\r\n\r\n# What I want to happen\r\n\r\n1.  the helm chart should support a caCertificates section like other components from h2o.ai\r\n2. the deployment user supplies one or more PEM-format certificates in caCertificates\r\n3. the user-supplied caCertificates should be unioned with the set of root certificates that come by default with the pod\r\n4. this unioned list of certificates should be put in a place where the underlying software will find it\r\n5. the h2ogpt client honors the private CA, and the remote server is considered trusted, and the connection succeeds\r\n\r\n## Some implementation details\r\n\r\n- currently, the python code in h2ogpt uses httpx to make connections to models in the model_lock list\r\n- httpx documentation says that it uses certifi.  however by trial and error, i discovered that in this pod https uses `/etc/ssl/cert.pem`\r\n- certifi seems like it's ignored.  certifi.where() does not point to /etc/ssl/cert.pem\r\n",
    "comments": [
      {
        "user": "tomkraljevic",
        "body": "Methodogy I used to experimentally jam in certificates by hand, to see which file the current code/pod was really picking up the certs from.\r\n\r\n```\r\n\r\n\r\n1.  create the configmap:\r\n\r\nroot@ip-10-0-1-175:/home/ubuntu/tomk-1.5.1-07-15# head tomk-cacert-config \r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  namespace: h2ogpt\r\n  name: tomk\r\ndata:\r\n  cert.pem: |\r\n    # Local box\r\n    -----BEGIN CERTIFICATE-----\r\n    MIIDDjCCAfagAwIBAgIRANerbMOq4u7UvTHYe6Phnw0wDQYJKoZIhvcNAQELBQAw\r\n....\r\n\r\n\r\n2.  hack the h2ogpt deployment to add a volume and volumeMount:\r\n\r\n\r\n\r\n        volumeMounts:\r\n        - mountPath: /etc/ssl/cert.pem\r\n          name: tomk\r\n          subPath: cert.pem\r\n\r\n\r\n\r\n      volumes:\r\n      - configMap:\r\n          name: tomk\r\n        name: tomk\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "To clarify, h2oGPT just uses OpenAI API pypi package for connecting to vllm etc.  Nothing related to these issues would be involving any other part of h2oGPT."
      },
      {
        "user": "tomkraljevic",
        "body": "a suggestion:\r\n\r\nhttpx has env vars.  maybe an init container could cat /etc/ssl/cert.pem with the provided caCertificates, write them to a new location, and set the SSL_CERT_FILE env var so they get picked up.\r\n\r\nthis would prevent the need for any code changes in the image.\r\n\r\nhttps://www.python-httpx.org/environment_variables/\r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1750,
    "title": "Running model on multiple GPUs? How to do it? Does h2ogpt allows it do easily?",
    "author": "martinenkoEduard",
    "state": "closed",
    "created_at": "2024-07-17T08:12:28Z",
    "updated_at": "2024-07-17T17:36:07Z",
    "labels": [
      "type/question"
    ],
    "body": "Running model on multiple GPUs? How to do it?\r\nCan you show simple example?\r\n\r\nWhat are the restrictions? Should be GPUs identical?\r\nOr is it possible for instance to have\r\none RTX 3070 and one 3080? What about memory sharing?\r\n\r\nDo mistal and llama support these features?\r\n\r\nAlso can you share a rig configuration with multiple GPUs for local LLM deployment?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I recommend using vLLM, ollama, etc. as inference servers with h2oGPT to get most benefit if isolation and concurrency.\r\n\r\nBut you can use GGUF models, which automatically spread to multiple GPUs or however many are set as visible (CUDA_VISIBLE_DEVICES).\r\n\r\nFor HF models, just set `use_gpu_id=False` to spread it over multiple GPUs.\r\n\r\nLastly, there are many auxiliary models like embedding, TTS, STT, image generation, etc.  Those are controlled with embedding_gpu_id, caption_gpu_id, doctr_gpu_id, asr_gpu_id, stt_gpu_id, tts_gpu_id, image_gpu_ids (for each generation model added)."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1742,
    "title": "Parallelize image calls instead of multiple images, probably better results in some cases",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-16T01:19:17Z",
    "updated_at": "2024-07-16T01:19:22Z",
    "labels": [
      "type/feature"
    ],
    "body": null,
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1692,
    "title": "Can we query Qdrant directly without needing sources?",
    "author": "andrewyuau",
    "state": "closed",
    "created_at": "2024-06-18T02:50:12Z",
    "updated_at": "2024-07-16T01:10:28Z",
    "labels": [],
    "body": "I followed [instructions](https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#using-qdrant) to spin up Qdrant using Docker and passed the QDRANT_URL and QDRANT_API_KEY. \r\n\r\nI'm trying to get h2ogpt to query Qdrant directly by not having any files in /workspace/user_path and deleting the directory /workspace/db_dir_UserData. However, I am getting an error message \"Did not generate db for UserData since no sources\" which originates from line 6258 in [gpt_langchain.py](https://github.com/h2oai/h2ogpt/blob/main/src/gpt_langchain.py). \r\n\r\nIs it possible to use my own Qdrant without needing sources?",
    "comments": [
      {
        "user": "andrewyuau",
        "body": "My current workaround is to have one entry in /workspace/user_path and h2ogpt automatically adds that entry into my Qdrant DB. I then delete that entry manually from Qdrant DB and then h2ogpt can query the rest of the entries in my own Qdrant DB. "
      },
      {
        "user": "pseudotensor",
        "body": "@Anush008 may be able to help."
      },
      {
        "user": "Anush008",
        "body": "Hey @andrewyuau, from the codebase, I see it should only be a log message and not an error. You should probably be able to continue to using it."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1741,
    "title": "RAG via open-webui",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-15T23:28:43Z",
    "updated_at": "2024-07-15T23:28:46Z",
    "labels": [],
    "body": "- [ ] vector db match h2oGPT\r\n- [ ] -e OPENAI_API_USER='user:password'\r\n\r\ncont of https://github.com/h2oai/h2ogpt/pull/1663",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 248,
    "title": "add continue button",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2023-06-07T22:05:55Z",
    "updated_at": "2024-07-15T07:24:01Z",
    "labels": [],
    "body": "to continue generation",
    "comments": [
      {
        "user": "DJJones66",
        "body": "Glad this was not just me, I seriously thought I was missing something or an option that needed to be selected.  \r\n"
      },
      {
        "user": "pseudotensor",
        "body": "@DJJones66 you can always choose\r\n```\r\npython generate.py --max_new_tokens=1024\r\n```\r\nOr you can choose in expert panel this.\r\n\r\nOr something large if you want.\r\n\r\nBut I agree \"continue\" is a better option, in case the model goes wild one won't have to wait.  Our \"stop\" button can't actually stop generation itself, just what appears in UI."
      },
      {
        "user": "DJJones66",
        "body": "Can't use the token increase as an answer more because then you kind of get into an arms race.. when is it too much.  The continue button is just what has become the normally acceptable answer to the problem.  (Thank ChatGPT for that, rather than teaching everyone.. Here is the limit, it is a wall)"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1626,
    "title": "Enchance h2oGPT UI to have librechat like features.",
    "author": "hemenkapadia",
    "state": "closed",
    "created_at": "2024-05-17T05:56:22Z",
    "updated_at": "2024-07-15T06:55:00Z",
    "labels": [
      "type/feature",
      "reporter/proserve"
    ],
    "body": "Enhancement request coming from a customer for h2oGPT UI\r\n\r\n1. Enhance UI to give a more OpenAI-UI-like experience\r\n2. Option to choose/change model in between a chat session without losing context\r\n3. Persistent chat history with the ability to share it using a URL\r\n4. Admin capability to monitor usage / chats etc\r\n5. Integration with OIDC for authentication\r\n\r\nThey suggested Librechat https://www.librechat.ai/ meets their requirements and want something similar natively in the platform. \r\n\r\ncc @somanathghalimath @shivam5992 @genrichards ",
    "comments": [
      {
        "user": "hemenkapadia",
        "body": "Discussion on the topic at https://h2oai.slack.com/archives/C05MUTAHKU5/p1715925510510869\r\n- Suggest using OpenWebUI over librechat - https://h2oai.slack.com/archives/C05MUTAHKU5/p1715925510510869\r\n- https://openwebui.com/\r\n- https://github.com/open-webui/open-webui"
      },
      {
        "user": "pseudotensor",
        "body": "h2oGPT backend can be used for Open Web UI:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#open-web-ui\r\n\r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1740,
    "title": "report generation",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-15T05:17:36Z",
    "updated_at": "2024-07-15T05:17:36Z",
    "labels": [],
    "body": "https://github.com/mshumer/gpt-author/blob/main/Claude_Author.ipynb",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1739,
    "title": "action model integration",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-15T05:17:15Z",
    "updated_at": "2024-07-15T05:17:15Z",
    "labels": [],
    "body": "https://github.com/lavague-ai/LaVague?tab=readme-ov-file",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1738,
    "title": "integrate stuff like LARS for PDF citations highlighting",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-15T05:16:08Z",
    "updated_at": "2024-07-15T05:16:08Z",
    "labels": [],
    "body": "https://www.reddit.com/r/LocalLLaMA/comments/1db98el/rag_for_documents_with_advanced_source_citations/?share_id=jsvIr2Ctx8_P04IOgaOHV&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\r\n\r\nhttps://github.com/abgulati/LARS/tree/v1.4\r\nhttps://www.youtube.com/watch?v=Mam1i86n8sU&t=1s",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1736,
    "title": "add ingestion for OpenAI API",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-13T04:43:55Z",
    "updated_at": "2024-07-13T06:48:52Z",
    "labels": [
      "type/feature"
    ],
    "body": "https://docs.privategpt.dev/api-reference/api-reference/context-chunks/chunks-retrieval\r\n\r\n\r\nhttps://platform.openai.com/docs/api-reference/files/create\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "https://github.com/h2oai/h2ogpt/issues/1563#issuecomment-2081301661"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1737,
    "title": "for image batching, do parallel jobs instead of many images per batch, some models just not good enough to handle multiple images",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-13T04:52:12Z",
    "updated_at": "2024-07-13T04:56:51Z",
    "labels": [
      "type/feature"
    ],
    "body": "```\r\nimport cv2\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url='http://<ip>:80/v1')\r\nmodel=\"OpenGVLab/InternVL2-26B\"\r\n#client = OpenAI(base_url='http://<ip>:80/v1')\r\n#model = 'OpenGVLab/InternVL-Chat-V1-5'\r\n\r\nprompt = \"\"\"<response_instructions>\r\n- Act as a keen observer with a sharp eye for detail.\r\n- Analyze the content within the images.\r\n- Provide insights based on your observations.\r\n- Avoid making up facts.\r\n- Finally, according to our chat history, above documents, above figure captions, or given images, generate a well-structured response.\r\n</response_instructions>\r\nWhat tower do you see in the image?\r\n\"\"\"\r\n\r\nfrom PIL import Image\r\nimport base64\r\nimport requests\r\nfrom io import BytesIO\r\n\r\n\r\n# The encoding function I linked previously - but we actually don't use this function in the API server\r\ndef encode_image_base64(image: Image.Image, format: str = 'JPEG') -> str:\r\n    \"\"\"encode image to base64 format.\"\"\"\r\n\r\n    buffered = BytesIO()\r\n    if format == 'JPEG':\r\n        image = image.convert('RGB')\r\n    image.save(buffered, format)\r\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\r\n\r\n\r\n# This is what we use in the API server to load the base64 string to image\r\ndef load_image_from_base64(image: str):\r\n    \"\"\"Load image from base64 format.\"\"\"\r\n    return Image.open(BytesIO(base64.b64decode(image)))\r\n\r\n\r\nimage1 = '/tmp/image_file_764ae7bd-6b02-4ffb-b9d6-83e754c30952.jpeg'\r\nimage2 = '/tmp/image_file_1bfb88ea-a545-4b1f-a31f-051dbb90a378.jpeg'\r\nimage3 = '/tmp/image_file_ac5589e7-92a3-470f-a933-40d6bad38052.jpeg'\r\n\r\n#from PIL import Image\r\n\r\n\r\ndef remove_padding(image_path, output_path, background_color=(255, 255, 255)):\r\n    # Read the image\r\n    image = cv2.imread(image_path)\r\n\r\n    # Convert the image to grayscale\r\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n    # Apply a binary threshold to get a binary image\r\n    _, binary = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\r\n\r\n    # Invert the binary image\r\n    inverted_binary = cv2.bitwise_not(binary)\r\n\r\n    # Find contours\r\n    contours, _ = cv2.findContours(inverted_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n    # Get the bounding box of the largest contour\r\n    x, y, w, h = cv2.boundingRect(contours[0])\r\n    for contour in contours:\r\n        x1, y1, w1, h1 = cv2.boundingRect(contour)\r\n        if w1 * h1 > w * h:\r\n            x, y, w, h = x1, y1, w1, h1\r\n\r\n    # Crop the image to the bounding box\r\n    cropped_image = image[y:y+h, x:x+w]\r\n\r\n    # Save the cropped image\r\n    cv2.imwrite(output_path, cropped_image)\r\n\r\n\r\n# Example usage\r\nif False:\r\n    ext = 'b.jpg'\r\n    remove_padding(image1, image1 + ext)\r\n    remove_padding(image2, image2 + ext)\r\n    remove_padding(image3, image3 + ext)\r\nelse:\r\n    ext = ''\r\n\r\nimage1_64 = base64.b64encode(open(image1 + ext, 'rb').read()).decode('utf-8')\r\nimage2_64 = base64.b64encode(open(image2 + ext, 'rb').read()).decode('utf-8')\r\nimage3_64 = base64.b64encode(open(image3 + ext, 'rb').read()).decode('utf-8')\r\n\r\nsystem_prompt = \"You are h2oGPTe, an expert question-answering AI system created by H2O.ai that performs like GPT-4 by OpenAI.\"\r\n\r\nmessages = [\r\n    #{'role': 'system', 'content': system_prompt},\r\n    {\r\n        'role': 'user',\r\n        'content': [\r\n            {'type': 'image_url',\r\n             'image_url': {\r\n                'url': 'data:image/jpeg;base64,' + image1_64,\r\n                }\r\n             },\r\n            {'type': 'image_url',\r\n             'image_url': {\r\n                'url': 'data:image/jpeg;base64,' + image2_64,\r\n                }\r\n             },\r\n            {'type': 'image_url',\r\n             'image_url': {\r\n                'url': 'data:image/jpeg;base64,' + image3_64,\r\n                }\r\n             },\r\n            {'type': 'text', 'text': prompt},\r\n        ],\r\n    }\r\n]\r\n\r\nresponse = client.chat.completions.create(\r\n    model=model,\r\n    messages=messages,\r\n    max_tokens=300,\r\n    temperature=0.0,\r\n)\r\n\r\nprint(response.choices[0])\r\n```\r\n\r\ngives:\r\n```\r\nThe image does not show a tower. Instead, it shows two separate items:\\n\\n1. A receipt from a shopping store.\\n2. A cake with a message congratulating Kate and Duke on their upcoming arrival.\\n\\nIf you have any specific questions about these items, please let me know!\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "[\r\n![image_file_ac5589e7-92a3-470f-a933-40d6bad38052](https://github.com/user-attachments/assets/0a16067a-a71a-41fa-8080-9bd2e39ead1b)\r\n![image_file_1bfb88ea-a545-4b1f-a31f-051dbb90a378](https://github.com/user-attachments/assets/bfd8293e-47f6-4759-b6b9-cd83d5899e3b)\r\n![image_file_764ae7bd-6b02-4ffb-b9d6-83e754c30952](https://github.com/user-attachments/assets/6a8df125-47c7-4dcd-be19-5c38aed4fd83)\r\n](url)"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1734,
    "title": "Running RAG optimized models (nvidia/Llama3-ChatQA-1.5-70B)",
    "author": "bw-Deejee",
    "state": "closed",
    "created_at": "2024-07-12T08:52:18Z",
    "updated_at": "2024-07-13T01:38:22Z",
    "labels": [
      "area/qa"
    ],
    "body": "Two questions:\r\n\r\n1. Do you have any recommendation regarding best models for RAG applications?\r\n\r\n2. Is it possible to run docQA optimized models like nvidia/Llama3-ChatQA-1.5-8B,\r\nwhich seems to have two different promtp formats:\r\n\r\nwhen context is available\r\n```\r\nSystem: {System}\r\n\r\n{Context}\r\n\r\nUser: {Question}\r\n\r\nAssistant: {Response}\r\n\r\nUser: {Question}\r\n\r\nAssistant:\r\n```\r\n\r\nwhen context is not available\r\n```\r\nSystem: {System}\r\n\r\nUser: {Question}\r\n\r\nAssistant: {Response}\r\n\r\nUser: {Question}\r\n\r\nAssistant:\r\n```\r\n\r\nI'd be interested on how to run this properly.\r\nI tested RAG with the normal Llama3 8b and it always mentioned chunk numbers in the response and overall didnt respond very cleanly.\r\n\r\nBest regards",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "That model has a chat template in the model HF repo, so it works OOTB.  You don't need to do anything else but something like:\r\n\r\n```\r\npython generate.py --base_model=nvidia/Llama3-ChatQA-1.5-8B\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "![image](https://github.com/user-attachments/assets/b9c4d5e1-927c-4e6e-a970-776e222b6448)\r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1587,
    "title": "Question: correct prompts template for llama3-instruct",
    "author": "slavag",
    "state": "closed",
    "created_at": "2024-04-28T09:19:14Z",
    "updated_at": "2024-07-12T08:48:48Z",
    "labels": [],
    "body": "Hi,\r\nWhat is correct prompts template for llama3-instruct, that I can choose from the existing to be able to work with model ?\r\n\r\nThanks",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It works OOTB in newer h2ogpt.  Uses chat template with end of turn stopping added."
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks, can you please elaborate more, as I'm not fully understand :) Sorry"
      },
      {
        "user": "pseudotensor",
        "body": "e.g. this is sufficient:\r\n```\r\npython generate.py --base_model=meta-llama/Meta-Llama-3-8B-Instruct\r\n```\r\n\r\nIt uses the chat template that is in the HF model repo.  This doesn't have a system prompt, so we add that as a pre-conversation."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1718,
    "title": "system_prompt ignored for response_format JSON -- use text_context_list instead?",
    "author": "vanboom",
    "state": "closed",
    "created_at": "2024-07-01T21:35:00Z",
    "updated_at": "2024-07-08T21:37:41Z",
    "labels": [
      "type/question"
    ],
    "body": "I am not 100% sure how to express this issue but I have found a strange behavior after some testing attempts to get a JSON output using `response_format='json_object'` or `json_code`.   I am querying the model using the API.   I set up this simple example during testing to help explore the issue.\r\n\r\nThe goal:  give the model some context in the `system` message and then ask some questions. \r\n**The issue:**  The model does not consider the given context if `response_format=json_object` is configured as an ARG\r\n\r\n```\r\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --langchain_mode=LLM --response_format=json_object\r\n```\r\n\r\n### Test 1 - working context\r\n```\r\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --langchain_mode=LLM\r\n```\r\n\r\n```\r\n[\r\n    [0] {\r\n           :role => \"system\",\r\n        :content => \"      Reference the following context to answer questions:\\n      *****\\n      Kitty likes BBQ and golf.\\n      *****\\n\"\r\n    },\r\n    [1] {\r\n           :role => \"user\",\r\n        :content => \"      What does Kitty like?  Explain your answer.\\n\"\r\n    }\r\n]\r\n\r\n\r\n\"Kitty likes BBQ and golf, as stated in the given context. BBQ refers to barbecue, which is a method of cooking food slowly over low heat, usually involving smoking. Golf is a sport that involves hitting a small ball into a series of holes using a variety of clubs. Therefore, Kitty has a preference for both BBQ and golf.\"\r\n```\r\n\r\n### Test 2 - hallucinated answer with no consideration of the context\r\n\r\n```\r\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --langchain_mode=LLM --response_format=json_object\r\n```\r\n\r\n```\r\n[\r\n    [0] {\r\n           :role => \"system\",\r\n        :content => \"      Reference the following context to answer questions:\\n      *****\\n      Kitty likes BBQ and golf.\\n      *****\\n\"\r\n    },\r\n    [1] {\r\n           :role => \"user\",\r\n        :content => \"      What does Kitty like?  Explain your answer.\\n\"\r\n    }\r\n]\r\n\r\nRESPONSE:  \"{\\\"Kitty\\\": {\\\"likes\\\": [\\\"catnip\\\", \\\"laser pointers\\\", \\\"playtime\\\", \\\"treats\\\", \\\"cuddles\\\"]}}\"\r\n\r\n```\r\n\r\nIn the web app, it works perfectly even with `--response_format='json_code'` enabled on the command line. \r\n![image](https://github.com/h2oai/h2ogpt/assets/251706/88a7de7a-633b-4b1a-a244-b6112bc338bf)\r\n\r\n\r\nAny ideas why the API would behave this way differently from the Gradio app?  I am not sure how to set `response_format` to JSON in the API calls so possibly there is a method that would work at that level.  Thanks!!",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, we purposefully nuke the system_prompt when doing json mode.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/6d514a1b9d498f714fd3f06570a816ac169ad962/src/gen.py#L2930\r\n\r\nAs the comment says, we have seen issues with it messing up even normal models, if the system_prompt contains conflicting persona relative to JSON making.\r\n\r\nBut you can comment-out that line."
      },
      {
        "user": "pseudotensor",
        "body": "I think even better than commenting out that line is to pass your lines of information as `text_context_list` list that would be considered same as if it were a document being referenced."
      },
      {
        "user": "pseudotensor",
        "body": "As yet another alternative, if you need to input instructions, just change these prompts via API:\r\n\r\n```\r\n            \"json_object_prompt\",\r\n            \"json_object_prompt_simpler\",\r\n            \"json_code_prompt\",\r\n            \"json_code_prompt_if_no_schema\",\r\n            \"json_schema_instruction\",\r\n\r\n```"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1722,
    "title": "Metrics for the generated answer",
    "author": "vitalyshalumov",
    "state": "closed",
    "created_at": "2024-07-07T07:19:55Z",
    "updated_at": "2024-07-08T21:34:09Z",
    "labels": [
      "resolution/wontfix"
    ],
    "body": "Are there any metrics that I can get on the generated answer such as faithfulness?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "We considered adding to h2oGPT this kind of stuff, but stopped.  So the answer is not at moment."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1713,
    "title": "Getting \"an unexpected keyword argument 'cache_folder'\" during import",
    "author": "inmanityus",
    "state": "closed",
    "created_at": "2024-06-25T20:04:57Z",
    "updated_at": "2024-07-03T21:42:54Z",
    "labels": [],
    "body": "I have the following code into which I am passing in a JSON document.  It keeps throwing the same error.  I checked the JSOn and it is valid - what am I doing wrong?\r\n\r\nError:  TypeError: SentenceTransformer.__init__() got an unexpected keyword argument 'cache_folder'\r\n\r\n\r\nI am using Chroma through LangChain.\r\n\r\n    ```\r\n    db_directory = os.path.join(user_directory, database_name + \".db\")\r\n    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\r\n    # Cosine will keep the similarity scores between zero and one\r\n    chroma_db = Chroma(persist_directory=db_directory, collection_name=collection_name, \r\n    embedding_function=embedding_function,\r\n                           collection_metadata={\"hnsw:space\": \"cosine\"}, relevance_score_fn=lambda distance: 1.0 - distance / 2)\r\n    json_splitter = RecursiveJsonSplitter(max_chunk_size=2000)\r\n    docs = json_splitter.create_documents(json_splitter.split_json(json_object))\r\n    if doc_ids is None:\r\n        doc_ids = [str(uuid.uuid4()) for i in range(1, len(docs) + 1)]\r\n    else:\r\n        # We look to see if the document exists:\r\n        result = chroma_db.get(doc_ids)\r\n        if result is not None and len(result) > 0:\r\n            # This is an update:\r\n            chroma_db.update_documents(doc_ids, docs)\r\n            return doc_ids\r\n    chroma_db.from_documents(docs, embedding_function, ids=doc_ids)\r\n    return doc_ids\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You are probably using the wrong/unsupported version of that package.  It should be `sentence-transformers==2.2.2\r\n`"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1683,
    "title": "Chunk should open on the same page from it has been taken",
    "author": "glenbhermon",
    "state": "open",
    "created_at": "2024-06-13T06:38:27Z",
    "updated_at": "2024-07-03T21:42:04Z",
    "labels": [
      "resolution/wontfix"
    ],
    "body": "when click on the link in the Source then the file is open in the same tab and the pdf by default goes on the first page. is there any possible way when we click on the link from the chunk has been taken from should open in the same page and the same paragraph in the source file.",
    "comments": [
      {
        "user": "srimanreddy4",
        "body": "In utils.py, check for the function \"get_url \", which is line 826, there you can make the following changes\r\n\r\n if not from_str:\r\n    source = x.metadata['source']+f\"#page={x.metadata['page']}\" //Line  828\r\n \r\nelse(of short name condition) : \r\n  source_name = os.path.basename(source.split('#')[0]) // Line 834\r\n\r\n\r\nDo these changes, then you are good to go with what you want\r\nand regarding pdf opening on same page, you can add onclick=\"\" in the final link"
      },
      {
        "user": "llmwesee",
        "body": "Modified the get_url function:\r\n\r\n```\r\ndef get_url(x, from_str=False, short_name=False, font_size=2):\r\n    if not from_str:\r\n        source = x.metadata['source']\r\n        if 'page' in x.metadata:\r\n            source += f\"#page={x.metadata['page']}\"\r\n    else:\r\n        source = x\r\n    if short_name:\r\n        source_name = get_short_name(source)\r\n    else:\r\n        #source_name = source\r\n        source_name = os.path.basename(source.split('#')[0])\r\n    if source.startswith('http://') or source.startswith('https://'):\r\n        # return \"\"\"<font size=\"%s\"><a href=\"%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n        #     font_size, source, source_name)\r\n        return \"\"\"<font size=\"%s\"><a href=\"%s\" target=\"_blank\" rel=\"noopener noreferrer\" onclick=\"\">%s</a></font>\"\"\" % (\r\n            font_size, source, source_name)\r\n    elif '<a href=' not in source:\r\n        # return \"\"\"<font size=\"%s\"><a href=\"file/%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n        #     font_size, source, source_name)\r\n        return \"\"\"<font size=\"%s\"><a href=\"file/%s\" target=\"_blank\" rel=\"noopener noreferrer\" onclick=\"\">%s</a></font>\"\"\" % (\r\n            font_size, source, source_name)\r\n    else:\r\n        # already filled\r\n        return source\r\n```\r\n\r\nBut the problem is that all chunks not opened in the desired page location when clicking on the link. sometimes chunks open on the irrelevant page location from where it haven't parse. \r\n\r\nAnd the this kind of metadata it always start with first page.\r\nBegin Document: Metadata: chunk_id = 13\r\nBegin Document:\r\nMetadata:\r\nchunk_id = 13\r\ndate = 2024-06-27 12:17:36.255558\r\ninput_type = .pdf\r\nsource = /tmp/gradio/611348b8d08a3f166d04ca265917a4ee6bc1af52/ GEAR AND EQUIPMENTS.pdf\r\n\r\nDocument Contents:\r\n\"\"\"\r\nI also don't understand why do every chunks not parsed along with page number. \r\nFor reference I also attaching the metadata screenshot:-\r\n\r\n![Screenshot from 2024-06-27 12-47-24](https://github.com/h2oai/h2ogpt/assets/137979399/e8675806-ce72-4db7-b241-610d0527e42f)\r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1702,
    "title": "Offline mode is still attempting to fetch from HF",
    "author": "machinelearning2014",
    "state": "closed",
    "created_at": "2024-06-19T11:00:32Z",
    "updated_at": "2024-07-03T21:34:24Z",
    "labels": [],
    "body": "I am running in Offline mode with this command:\r\n\r\nHF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python generate.py --base_model=mistral --model_path_llama=mistral-7b-instruct-v0.2.Q2_K.gguf --prompt_type=mistral --cli=True\r\n\r\nI have already downloaded the following models manually from HF:\r\n\r\n/home/letro/.cache/huggingface/hub/models--hkunlp--instructor-large\r\n/home/letro/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2\r\n/home/letro/h2ogpt/llamacpp_path/mistral-7b-instruct-v0.2.Q2_K.gguf\r\n\r\nWhen I run the command above I get following:\r\n\r\n(h2ogpt) letro@jupyterhub-gpu-alpha-bw:~/h2ogpt$ TRANSFORMERS_OFFLINE=1 python generate.py --base_model=mistral --model_path_llama=mistral-7b-instruct-v0.2.Q2_K.gguf --prompt_type=mistral --cli=True\r\n/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nUsing Model mistral\r\nfatal: ambiguous argument 'HEAD': unknown revision or path not in the working tree.\r\nUse '--' to separate paths from revisions, like this:\r\n'git <command> [<revision>...] -- [<file>...]'\r\nWARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nTraceback (most recent call last):\r\n  File \"/home/letro/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/letro/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/letro/h2ogpt/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/letro/h2ogpt/src/gen.py\", line 2044, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/home/letro/h2ogpt/src/gpt_langchain.py\", line 551, in get_embedding\r\n    embedding = HuggingFaceInstructEmbeddings(model_name=hf_embedding_model,\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 164, in __init__\r\n    self.client = INSTRUCTOR(\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 87, in __init__\r\n    snapshot_download(model_name_or_path,\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/util.py\", line 442, in snapshot_download\r\n    model_info = _api.model_info(repo_id=repo_id, revision=revision, token=token)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2300, in model_info\r\n    r = get_session().get(path, headers=headers, timeout=timeout, params=params)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 602, in get\r\n    return self.request(\"GET\", url, **kwargs)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 77, in send\r\n    raise OfflineModeIsEnabled(\r\nhuggingface_hub.errors.OfflineModeIsEnabled: Cannot reach https://huggingface.co/api/models/hkunlp/instructor-large: offline mode is enabled. To disable it, please unset the `HF_HUB_OFFLINE` environment variable.\r\n\r\nQuestion: why is the command still trying to fetch from HF? and why is it not seeing these already in local:\r\n/home/letro/.cache/huggingface/hub/models--hkunlp--instructor-large\r\n/home/letro/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2\r\n",
    "comments": [
      {
        "user": "machinelearning2014",
        "body": "I have now downloaded the embedding models at:\r\n\r\n/home/letro/.cache/torch/hkunlp/hkunlp_instructor-large\r\n/home/letro/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2\r\n\r\nand running: \r\nTRANSFORMERS_OFFLINE=1 python generate.py --base_model=mistral-7b-instruct-v0.2.Q2_K.gguf --prompt_type=mistral --max_seq_len=4096 --cli=True\r\n\r\nBut still it is still does not recognise the models in .cache so attempts to download from https://huggingface.co/api/models/hkunlp/instructor-large, which I want to avoid, as I am in corporate firewall"
      },
      {
        "user": "llmwesee",
        "body": "I'm also facing the same issue. "
      },
      {
        "user": "pseudotensor",
        "body": "I see same thing but only for the instructor model and only if using `HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1`.  If I don't set those it's fine.  This is a known bug in the instructor code.\r\n\r\nIf one unsets those, then \"mistral\" model is trying to be obtained from HF, but this is just an error in the command line.  If one is offline one needs to ensure the HF smart attempts are not used by using:\r\n\r\n```\r\npython generate.py --base_model=llama --model_path_llama=mistral-7b-instruct-v0.2.Q5_K_M.gguf --prompt_type=mistral --cli=True\r\n```\r\n\r\nthis works just fine.\r\n\r\nThis is what is documented here:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#tldr"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1707,
    "title": "Error installing through full linux bash script",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-06-24T11:00:45Z",
    "updated_at": "2024-07-03T21:15:34Z",
    "labels": [],
    "body": "SO: Ubuntu 22.04\r\nCommit: 9a7c07b5 (last at the moment of this issue)\r\nCuda: 12.1\r\nNvidia: 535.183.01\r\n\r\nI got the following error trying to install through:\r\n\r\n```shell\r\nbash docs/linux_install_full.sh\r\n```\r\nif someone can help with the error:\r\n\r\n```shell\r\nBuilding wheels for collected packages: llama-cpp-python\r\n  Building wheel for llama-cpp-python (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [93 lines of output]\r\n      *** scikit-build-core 0.9.6 using CMake 3.27.1 (wheel)\r\n      *** Configuring CMake...\r\n      2024-06-24 12:54:28,896 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/root/miniconda3/envs/h2ogpt/lib, ldlibrary=libpython3.10.a, multiarch=x86_64-linux-gnu, masd=None\r\n      loading initial cache file /tmp/tmpcjgjaztp/build/CMakeInit.txt\r\n      -- The C compiler identification is GNU 11.4.0\r\n      -- The CXX compiler identification is GNU 11.4.0\r\n      -- Detecting C compiler ABI info\r\n      -- Detecting C compiler ABI info - done\r\n      -- Check for working C compiler: /usr/bin/cc - skipped\r\n      -- Detecting C compile features\r\n      -- Detecting C compile features - done\r\n      -- Detecting CXX compiler ABI info\r\n      -- Detecting CXX compiler ABI info - done\r\n      -- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n      -- Detecting CXX compile features\r\n      -- Detecting CXX compile features - done\r\n      -- Found Git: /usr/bin/git (found version \"2.34.1\")\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n      -- Found Threads: TRUE\r\n      CMake Warning at vendor/llama.cpp/CMakeLists.txt:387 (message):\r\n        LLAMA_CUBLAS is deprecated and will be removed in the future.\r\n\r\n        Use LLAMA_CUDA instead\r\n\r\n\r\n      -- Unable to find cublas_v2.h in either \"/usr/local/cuda/include\" or \"/usr/math_libs/include\"\r\n      -- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.1.66\")\r\n      -- CUDA found\r\n      -- The CUDA compiler identification is NVIDIA 12.1.66\r\n      -- Detecting CUDA compiler ABI info\r\n      -- Detecting CUDA compiler ABI info - done\r\n      -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\r\n      -- Detecting CUDA compile features\r\n      -- Detecting CUDA compile features - done\r\n      -- Using CUDA architectures: all\r\n      -- CUDA host compiler is GNU 11.4.0\r\n\r\n      -- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.\r\n      -- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n      -- x86 detected\r\n      CMake Warning (dev) at CMakeLists.txt:26 (install):\r\n        Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\r\n      This warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n      CMake Warning (dev) at CMakeLists.txt:35 (install):\r\n        Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\r\n      This warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n      -- Configuring done (5.7s)\r\n      CMake Error at vendor/llama.cpp/CMakeLists.txt:1225 (target_link_libraries):\r\n        Target \"ggml\" links to:\r\n\r\n          CUDA::cublas\r\n\r\n        but the target was not found.  Possible reasons include:\r\n\r\n          * There is a typo in the target name.\r\n          * A find_package call is missing for an IMPORTED target.\r\n          * An ALIAS target is missing.\r\n\r\n\r\n\r\n      CMake Error at vendor/llama.cpp/CMakeLists.txt:1232 (target_link_libraries):\r\n        Target \"ggml_shared\" links to:\r\n\r\n          CUDA::cublas\r\n\r\n        but the target was not found.  Possible reasons include:\r\n\r\n          * There is a typo in the target name.\r\n          * A find_package call is missing for an IMPORTED target.\r\n          * An ALIAS target is missing.\r\n\r\n\r\n\r\n      CMake Error at vendor/llama.cpp/CMakeLists.txt:1249 (target_link_libraries):\r\n        Target \"llama\" links to:\r\n\r\n          CUDA::cublas\r\n\r\n        but the target was not found.  Possible reasons include:\r\n\r\n          * There is a typo in the target name.\r\n          * A find_package call is missing for an IMPORTED target.\r\n          * An ALIAS target is missing.\r\n\r\n\r\n\r\n      -- Generating done (0.0s)\r\n      CMake Generate step failed.  Build files cannot be regenerated correctly.\r\n\r\n      *** CMake configuration failed\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for llama-cpp-python\r\nFailed to build llama-cpp-python\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\r\n``` ",
    "comments": [
      {
        "user": "juerware",
        "body": "I think I have resolved the problem following the steps:\r\n\r\n1. Follow this url documentation: https://developer.nvidia.com/cuda-12-1-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_local\r\n2. ```apt -y install nvidia-cuda-toolkit``` # be careful because this is version 11.5 not 12.1 because 11.5 is default package in the ubuntu distribution mentioned before. This package is necessary for getting installed header files for compilation through nvcc command.\r\n3. Execute command to view version of installed packages:\r\n```\r\n# dpkg -l | grep -iP '(cuda|nvidia)' | grep -i toolkit\r\nii  cuda-toolkit-12-1-config-common             12.1.55-1                               all          Common config package for CUDA Toolkit 12.1.\r\nii  cuda-toolkit-12-config-common               12.1.55-1                               all          Common config package for CUDA Toolkit 12.\r\nii  cuda-toolkit-config-common                  12.1.55-1                               all          Common config package for CUDA Toolkit.\r\nii  nvidia-cuda-toolkit                         11.5.1-1ubuntu1                         amd64        NVIDIA CUDA development toolkit\r\nii  nvidia-cuda-toolkit-doc                     11.5.1-1ubuntu1                         all          NVIDIA CUDA and OpenCL documentation\r\n```\r\n4. Executing final repository command ```bash docs/linux_install_full.sh```\r\n\r\nAs you can see with these steps it is getting up but .... I think it has to be reviewed in order to automate the right final process and coordinate cuda versions"
      },
      {
        "user": "pseudotensor",
        "body": "The steps say to install cuda 12.1 toolkit first.  Yes, depending upon your drivers etc. this may be more involved as old drivers would need to be updated."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1536,
    "title": "One Click Installers for MacOS not working on MacMini M2",
    "author": "VillaesterModerneMedien",
    "state": "closed",
    "created_at": "2024-04-08T09:14:01Z",
    "updated_at": "2024-07-03T21:13:33Z",
    "labels": [],
    "body": "Hello,\r\n\r\nI am currently trying to install H2O GPT on my MacMini and am not getting anywhere with the One Click Installers.\r\n\r\n---\r\nMar 07, 2024\r\n\r\n[h2ogpt-osx-m1-cpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Mar2024/h2ogpt-osx-m1-cpu)\r\n[h2ogpt-osx-m1-gpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Mar2024/h2ogpt-osx-m1-gpu)\r\nNov 08, 2023\r\n\r\n[h2ogpt-osx-m1-cpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-cpu)\r\n[h2ogpt-osx-m1-gpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-gpu)\r\n---\r\n\r\n\r\nI have started the file for installation, but after a short time (it looks as if it is being installed), the file (e.g. h2ogpt-osx-m1-gpu) is simply opened in a text editor and then it doesn't go any further.\r\nI have enabled the installation of the extension in the system settings before...\r\n\r\nIs it because the install is actually for an M1 Mac?\r\nDoes anyone have a similar problem?\r\nI can't install H2O GPT at the moment...\r\n\r\nSystem Specs:\r\nMac mini 2023\r\nApple M2 Pro\r\n32 GB\r\nmacOS: Sonoma 14.4.1\r\n\r\nAny help would be appreciated, thank you very much.\r\n\r\n",
    "comments": [
      {
        "user": "verbiate",
        "body": "Same issue. The file has no extension, and attempts to rename with an appended \".dmg\" or \".app\" have been fruitless."
      },
      {
        "user": "pseudotensor",
        "body": "I've asked @Mathanraj-Sharma for help."
      },
      {
        "user": "USMCM1A1",
        "body": "Same--would love for this to work."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1686,
    "title": "h2ogpt tries to download model from hugging face when using local inference server",
    "author": "hapatrick",
    "state": "closed",
    "created_at": "2024-06-14T20:42:12Z",
    "updated_at": "2024-07-03T21:13:05Z",
    "labels": [],
    "body": "I am trying to follow the instructions at https://github.com/h2oai/h2ogpt/blob/main/docs/README_InferenceServers.md to run h2ogpt with a local inference server -- specifically, vLLM serving Mixtral8x22B using the OpenAI API.\r\n\r\nWhen I run generate.py, it tries to download the mixtral model from HuggingFace. My understanding is it should not need to do this because it's going to be using the model served by my inference server!\r\n\r\nHere is the command I'm trying to use:\r\npython3.10 /workspace/generate.py --inference_server=\"vllm:http://vllm-mixtral8x22b:8000/v1/\" --prompt_type=openai --base_model=mixtral\r\n\r\nWhat am I doing wrong here? \r\n",
    "comments": [
      {
        "user": "srimanreddy4",
        "body": "I have done it recently with mixtral, it worked for me, compared to my command, in the base command I mentioned the whole model name that I was using, I have looked up the documentation, they did the same too, why don't you try that out once"
      },
      {
        "user": "pseudotensor",
        "body": "@hapatrick It should only be downloading the tokenizer, not the whole model.  You should use \"vllm_chat\" instead of \"vllm\" for the inference_server prefix and then avoid passing --prompt_type.  prompt_type openai will use no LLM prompting with vllm's text generation API and not work right."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1705,
    "title": "Use auto ingest with openai Api ",
    "author": "rmisire",
    "state": "closed",
    "created_at": "2024-06-20T09:14:25Z",
    "updated_at": "2024-07-03T21:10:00Z",
    "labels": [],
    "body": "I would like to know if it is possible to automatically ingest files in a specific collection without going through the ui. After several searches I found nothing in the documentation. I looked at http://localhost:5001/ but nothing conclusive\r\n\r\nThank you for your feedback",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "The gradio API is extensive and it can be done that way yes.  An example test is the `test_client_chat_stream_langchain_steps3` test.\r\n\r\nFor openai I need to to this issue:\r\n\r\n\r\nhttps://github.com/h2oai/h2ogpt/issues/1563#issuecomment-2081301661"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1691,
    "title": "Gradio Client - Repeated WebSocket Connection Rejection (HTTP 403)",
    "author": "bibo7086",
    "state": "closed",
    "created_at": "2024-06-17T19:25:18Z",
    "updated_at": "2024-07-03T21:05:47Z",
    "labels": [],
    "body": "I encountered an issue while using the Gradio client to connect to a Gradio application running at `http://localhost:7860/`. The client initially reports successful loading of the API, but attempts to establish a WebSocket connection are rejected by the server with an HTTP 403 (Forbidden) status code.\r\n\r\n**Error Message:**\r\n\r\n```\r\nLoaded as API: http://localhost:7860/ ✔\r\nGR job failed: server rejected WebSocket connection: HTTP 403\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/gradio_client/client.py\", line 798, in _inner\r\n    predictions = _predict(*data)\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/gradio_client/client.py\", line 827, in _predict\r\n    result = utils.synchronize_async(self._ws_fn, data, hash_data, helper)\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/gradio_client/utils.py\", line 540, in synchronize_async\r\n    return fsspec.asyn.sync(fsspec.asyn.get_loop(), func, *args, **kwargs)  # type: ignore\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/fsspec/asyn.py\", line 103, in sync\r\n    raise return_result\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/fsspec/asyn.py\", line 56, in _runner\r\n    result[0] = await coro\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/gradio_client/client.py\", line 1055, in _ws_fn\r\n    async with websockets.connect(  # type: ignore\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/websockets/legacy/client.py\", line 637, in __aenter__\r\n    return await self\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/websockets/legacy/client.py\", line 655, in __await_impl_timeout__\r\n    return await self.__await_impl__()\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/websockets/legacy/client.py\", line 662, in __await_impl__\r\n    await protocol.handshake(\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/websockets/legacy/client.py\", line 329, in handshake\r\n    raise InvalidStatusCode(status_code, response_headers)\r\n\r\nLoaded as API: http://localhost:7860/ ✔\r\nGR job failed again: server rejected WebSocket connection: HTTP 403\r\n    ... (same stack trace as above)\r\n```\r\n\r\n**Steps to Reproduce:**\r\n\r\n1. python generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096\r\n2. Run the Gradio client code. \r\n```\r\nimport time\r\nimport os\r\nimport sys\r\n\r\nfrom gradio_utils.grclient import GradioClient\r\n\r\n# self-contained example used for readme, to be copied to README_CLIENT.md if changed, setting local_server = True at first\r\n# The grclient.py file can be copied from h2ogpt repo and used with local gradio_client for example use\r\n\r\nif local_server:\r\n    client = GradioClient(\"http://0.0.0.0:7860\")\r\nelse:\r\n    h2ogpt_key = os.getenv('H2OGPT_KEY') or os.getenv('H2OGPT_H2OGPT_KEY')\r\n    if h2ogpt_key is None:\r\n        sys.exit(\"API key not found. Exiting.\")\r\n    # if you have API key for public instance:\r\n    client = GradioClient(\"https://gpt.h2o.ai\", h2ogpt_key=h2ogpt_key)\r\n\r\n\r\nprint(client.question(\"Who are you?\"))\r\n```\r\n\r\n* **Environment:**\r\n    * Operating System: Ubuntu 22.04.4 LTS\r\n    * gradio: 4.26.0\r\n    * gradio_client:  0.6.1\r\n    \r\n    \r\n * **Note:**\r\nI suspect this is due to internal network restrictions and using 127.0.0.1 should ideally work. Unfortunately it doesn't. \r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You need to use gradio 4.26.0 and gradio_client 0.15.1 as is our default.\r\n\r\nAlso, in some cases, 0.0.0.0 is required as 127.0.0.1 is isolated to a local network."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1720,
    "title": "databricks",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-02T22:57:01Z",
    "updated_at": "2024-07-02T22:57:01Z",
    "labels": [],
    "body": "https://python.langchain.com/v0.2/docs/integrations/llms/databricks/#wrapping-a-serving-endpoint-custom-model\r\nhttps://seattledataguy.substack.com/p/building-llms-on-databricks",
    "comments": [],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1719,
    "title": "TypeError: 'ResponseFormat' object is not subscriptable",
    "author": "vanboom",
    "state": "closed",
    "created_at": "2024-07-01T22:25:59Z",
    "updated_at": "2024-07-01T22:28:02Z",
    "labels": [],
    "body": "In  the OpenAI server API, setting the `response_format` per the API docs causes a TypeError exception.   It appears that the code is attempting to treat the response_format as a hash instead of struct. \r\n\r\n```\r\n  File \"/data/h2ogpt/openai_server/backend.py\", line 156, in get_response\r\n    gen_kwargs['response_format'] = gen_kwargs.get('response_format')['type']\r\n```\r\n\r\nWhen the options are parsed, the response_format is a `ResponseFormat` struct, so accessing via `.type` works where `['type']` causes the TypeError.   \r\n\r\nHere is the diff working vs. non-working.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/251c72bd70dd5b1ebd76046d9187e13f343597cb/openai_server/backend.py#L195\r\n```\r\ndiff --git a/openai_server/backend.py b/openai_server/backend.py\r\nindex fc77019a..7b41a18f 100644\r\n--- a/openai_server/backend.py\r\n+++ b/openai_server/backend.py\r\n@@ -152,7 +152,9 @@ def get_response(instruction, gen_kwargs, verbose=False, chunk_response=True, st\r\n     if gen_kwargs.get('response_format'):\r\n         # pydantic ensures type and key\r\n         # transcribe to h2oGPT way of just value\r\n-        gen_kwargs['response_format'] = gen_kwargs.get('response_format')['type']\r\n+        print(gen_kwargs)\r\n+        #gen_kwargs['response_format'] = gen_kwargs.get('response_format')['type']\r\n+        gen_kwargs['response_format'] = gen_kwargs.get('response_format').type\r\n \r\n     kwargs.update(**gen_kwargs)\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "vanboom",
        "body": "This appears to be fixed in a newer version than what I was using when I reported this.  "
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1529,
    "title": "Can you add langsmith/wandb for tracing and ragas for evaluation metrics?",
    "author": "vitalyshalumov",
    "state": "open",
    "created_at": "2024-04-04T06:49:36Z",
    "updated_at": "2024-06-30T15:03:40Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I have some code in WIP for verifiers that include such things, but not done.  RAGAS is ok, but it's a bit loose compared to specific checking of actual specific faqs like done here: https://github.com/h2oai/enterprise-h2ogpte/tree/main/rag_benchmark"
      },
      {
        "user": "vitalyshalumov",
        "body": "Thank you,\r\nMy problem is not on a test set - it is on a per query metric:\r\nI want to let the user know the quality of the answer by showing him the  metrics:\r\n\r\n[Faithfulness]\r\n[Answer relevancy]\r\n[Context recall]\r\n[Context precision]\r\n[Context relevancy]\r\n[Context entity recall]\r\n\r\nhttps://docs.ragas.io/en/stable/concepts/metrics/index.html"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1684,
    "title": "can't add personal data db/collection to auth.json",
    "author": "rxng",
    "state": "closed",
    "created_at": "2024-06-13T07:55:51Z",
    "updated_at": "2024-06-28T04:10:54Z",
    "labels": [],
    "body": "According to the instructions, we can add a make_db.py database to auth.json , but does not specify exactly how to do this. \r\n```\r\nTo make a new one for the user, fill `user_path_jon` with documents (can be soft or hard linked to avoid dups across multiple users), do:\r\n```bash\r\npython src/make_db.py --user_path=gptdocsdb/jon--collection_name=JonData --langchain_type=personal --hf_embedding_model=hkunlp/instructor-large --persist_directory=users/jon/db_dir_JonData\r\n```\r\n\r\nThen you'll have:\r\n```text\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ ls -alrt users/jon/db_dir_JonData/\r\ntotal 264\r\ndrwx------ 13 jon jon   4096 Apr 16 12:28 ../\r\ndrwx------  2 jon jon   4096 Apr 16 12:28 d7ccacb6-93fe-4380-9340-b7f5edffb655/\r\n-rw-------  1 jon jon 249856 Apr 16 12:28 chroma.sqlite3\r\n-rw-------  1 jon jon     41 Apr 16 12:28 embed_info\r\ndrwx------  3 jon jon   4096 Apr 16 12:28 ./\r\n```\r\n\r\nYou can add that database to the `auth.json` for their entry if using `auth.json` type file, and they will see when they login.\r\n```\r\n\r\nh2ogpt is being run like so and everything works well except it does not load the correct collection for the user \r\n`python generate.py --base_model=mistral-7b-instruct-v0.2.Q8_0.gguf --score_model=None --prompt_type=instruct --auth_access=closed --auth=auth.json --guest_name='' --auth_freeze`\r\n\r\nI have tried the following by adding db parameters but it does not work. \r\n```\r\n{\r\n  \"jon\": {\r\n    \"password\": \"jon1306\",\r\n    \"userid\": \"acb8fef1a77d122b5e12b261202ada7a\",\r\n    \"selection_docs_state\": {\r\n      \"langchain_modes\": [\r\n        \"JonData\",\r\n        \"LLM\",\r\n        \"Disabled\"\r\n      ],\r\n      \"langchain_mode_types\": {\r\n        \"JonData\": \"personal\"\r\n      }\r\n    },\r\n    \"dbs\": \"users/jon/db_dir_JonData\",\r\n    \"load_db_if_exists\": \"users/jon/db_dir_JonData\"\r\n  }\r\n}\r\n```\r\n\r\nHow do we make it such that when user logs in, their  collection JonData is automatically added? \r\nOr, Any way to simply specify a per user user_path? that would be easiest.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "If you are trying this for shared collection, did you try the CLI options?\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#multiple-embeddings-and-sources\r\n\r\ni.e.\r\n\r\n```\r\npython generate.py --model_lock=\"[{'base_model': 'llama', 'model_path_llama': 'Phi-3-mini-4k-instruct-q4.gguf', 'tokenizer_base_model': 'microsoft/Phi-3-mini-4k-instruct'}]\" --use_auth_token=$HUGGING_FACE_HUB_TOKEN --langchain_modes=\"['UserData', 'MyData', 'UserData2']\"\r\n```\r\n\r\nWould show all users those 2 by default.\r\n\r\nEven if a user logs in that already had a db entry, they will be forced to see those CLI ones.\r\n\r\nIf the system is online, without restarting, there's currently no way to add to all users at once with e.g. some kind of global user added settings.  Is that what you are trying to achieve?"
      },
      {
        "user": "pseudotensor",
        "body": "For personal collections, there's no CLI options for that, it's only in the db/json file.  By default sqlite3 db is used in newer h2oGPT to address speed issues with json, so one would have to edit the db using operations like in the src/db_utils.py.\r\n\r\nI'll think about how to handle this better, probably adding an option to add things via the admin page is best.  Would that work for you?"
      },
      {
        "user": "rxng",
        "body": "thanks for your quick response! Maybe I was confusing in my explanation. I was trying to achieve having a user logging in and then their own collection would be automatically loaded for them.\r\n\r\nHowever, I tried every single parameter and just found a way to do it via the auth.json file, by adding the line \r\n`\"langchain_mode\": \"JonData\",` above the selection_docs_state entry, like so\r\n```\r\n\"langchain_mode\": \"JonData\",\r\n    \"selection_docs_state\": {\r\n```\r\n\r\nThe only question I have is, if we wanted to then add more documents to the collection via make_db.py , would we then have to restart the entire instance of h2ogpt to automatically use the updated collection?\r\n\r\nIt would definitely be great if there was an admin page where these things could easily be managed :)"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1671,
    "title": "Unable to Programmatically Receive Sources with Prompts & Responses",
    "author": "devinrouthuzh",
    "state": "closed",
    "created_at": "2024-06-06T15:23:51Z",
    "updated_at": "2024-06-28T03:59:10Z",
    "labels": [],
    "body": "Firstly, thanks for the aid with my previous question! (#1023) Moreover, I found #685 to be very helpful for my needs—much appreciated for referencing it in my issue.\r\n\r\nOn the note of #685: I'm successfully able to programmatically evaluate a set of prompts in a JSON using:\r\n\r\n```\r\nfrom tests.utils import make_user_path_test\r\n\r\ndef run_eval(cpu=False, bits=None, base_model='h2oai/h2ogpt-oig-oasst1-512-6_9b',eval_filename=None,eval_prompts_only_num=1):\r\n    \r\n    from src.gen import main\r\n    \r\n    user_path = make_user_path_test()\r\n    \r\n    kwargs = dict(\r\n            stream_output=False,\r\n            langchain_mode='UserData',\r\n            langchain_modes=['UserData']\r\n    )\r\n    \r\n    eval_out_filename = main(base_model=base_model,\r\n                             eval=True, gradio=False,\r\n                             eval_filename=eval_filename,\r\n                             eval_prompts_only_num=eval_prompts_only_num,\r\n                             eval_as_output=False,\r\n                             eval_prompts_only_seed=123456,\r\n                             answer_with_sources=True,append_sources_to_answer=True,append_sources_to_chat=False, # !! Added so sources are appended\r\n                             user_path='src/user_path',show_link_in_sources=True, # !! Added so sources are appended\r\n                             **kwargs)\r\n    return eval_out_filename\r\n\r\neval_filename = 'my_prompts.json'\r\nnprompts = 2\r\nbits = 8\r\ncpu = False\r\nbase_model = 'h2oai/h2ogpt-4096-llama2-7b-chat'\r\neval_out_filename = run_eval(cpu=cpu, bits=bits, base_model=base_model,eval_filename=eval_filename,eval_prompts_only_num=nprompts)\r\n```\r\n\r\nwhich is code that was streamlined from the suggested source: [h2ogpt/tests/test_eval.py](https://github.com/h2oai/h2ogpt/blob/9e0e35286d6ae022ae41e46659b3786e95a11f11/tests/test_eval.py#L247-L299).\r\n\r\nWhen running this code, however, I noticed that the model never correctly sets `langchain_mode` to `UserData` (it always stays as `'langchain_mode': None`), and I'm never able to programmatically receive the citations/sources from the database with the prompts.\r\n\r\nNotably, the following code generates a gradio user interface through which `langchain_mode` is correctly set as `UserData` and wherein I receive all citations/sources/tokens correctly (but interactively):\r\n```\r\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --answer_with_sources=True --langchain_mode=UserData\r\n```\r\n\r\nI attempted to use the following call as well, but `langchain_mode` remains set to `None` regardless of my inputs:\r\n```\r\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --answer_with_sources=True --langchain_mode=UserData --eval=True --gradio=False --eval_prompts_only_seed=54321 --eval_filename=my_prompts.json --eval_prompts_only_num=2 --user_path=src/user_path\r\n```\r\n\r\nI need to be able to programmatically receive the responses from my custom prompts along with the paired sources/citations. Am I missing something simple here?\r\n\r\nThanks again!",
    "comments": [
      {
        "user": "devinrouthuzh",
        "body": "Hi there—does anyone else have trouble setting the Langchain mode and receiving source information when interacting programmatically with a model? Am I missing a flag somewhere, or is there something else happening?\r\n\r\nThanks!"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1624,
    "title": "GPU Installation",
    "author": "jaysunl",
    "state": "closed",
    "created_at": "2024-05-16T00:52:38Z",
    "updated_at": "2024-06-28T03:11:54Z",
    "labels": [],
    "body": "May I possibly have specific instructions as for the GPU installation of the tool? I have followed the installation but it still says no GPU detected. I have the following GPU on my system:  NVIDIA Corporation GA102GL [A10G] (rev a1) and my nvcc --version output is \r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Thu_Nov_18_09:45:30_PST_2021\r\nCuda compilation tools, release 11.5, V11.5.119\r\nBuild cuda_11.5.r11.5/compiler.30672275_0\r\n\r\nAnd tried to install Pytorch CUDA. But when I run\r\nimport torch\r\nprint(torch.cuda.is_available())\r\n\r\nit still says False and when I try to run a model, it still says no GPU detected. Any guidance would be appreciated.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's a good start that you checked that torch condition.  We only really support cuda 12.1 and above at this point, so maybe there's some issue with the installation because you have old cuda toolkit.  It's easy to follow our instructions for installing the cuda toolkit 12.1, but you'll need drivers that are also compatible."
      },
      {
        "user": "jaysunl",
        "body": "Ok I was actually able to follow the GPU version of PyTorch. But the launching of the actual interface takes forever. I am running this command:\r\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --load_8bit=True --langchain_mode=UserData --user_path=/some/path\r\n\r\nbut the output gets stuck at this:\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model h2oai/h2ogpt-oig-oasst1-512-6_9b\r\n\r\nwith nothing after it. It has been like that for at least an hour. Any tips?"
      },
      {
        "user": "pseudotensor",
        "body": "I recommend not using h2oai/h2ogpt-oig-oasst1-512-6_9b as a model, but instead a GGUF model at first like `https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF`.  Also, ensure to -pass `--verbose=True` to get more info.\r\n\r\n```\r\npython generate.py --base_model=llama --model_path_llama=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf?download=true --tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct --max_seq_len=8192 --verbose=True\r\n```"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1670,
    "title": "Running H2ogpt with Ollama inference Server",
    "author": "rohitnanda1443",
    "state": "closed",
    "created_at": "2024-06-06T12:38:40Z",
    "updated_at": "2024-06-27T22:40:24Z",
    "labels": [],
    "body": "Hi All,\r\n\r\nI am trying to run an inference server on Ollama using the below script:\r\n\r\nollama run mistral:v0.3\r\n\r\nThen running h2o-gpt using the below script:\r\n\r\npython generate.py --guest_name='' --base_model=mistralai/Mistral-7B-Instruct-v0.3   --max_seq_len=8094  --enable_tts=False --enable_stt=False --enable_transcriptions=False --use_gpu_id=False  --inference_server=vllm_chat:http://localhost:11434/v1/ --prompt_type=openai_chat &\r\n\r\nIssue I am facing is that model not found in the H2o UI. What is the correct model name for the Ollama mistral-0.3 model to be passed in the CLI for H20? \r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Did you follow this?\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#running-ollama-vs-h2ogpt-as-inference-server"
      },
      {
        "user": "pseudotensor",
        "body": "As in the instructions, the \"base_model\" has to be the same name as ollama was set to.\r\n\r\ni.e.\r\n```\r\npython generate.py --guest_name='' --base_model=mistral:v0.3 --max_seq_len=8094 --enable_tts=False --enable_stt=False --enable_transcriptions=False --use_gpu_id=False --inference_server=vllm_chat:http://localhost:11434/v1/ --prompt_type=openai_chat\r\n```\r\nand ignore errors about not finding the tokenizer etc.\r\n\r\nFor more accurate tokenization specify the tokenizer and hf token (because mistralai is gated on HF):\r\n```\r\npython generate.py --guest_name='' --base_model=mistral:v0.3 --tokenizer_base_model=mistralai/Mistral-7B-Instruct-v0.3 --max_seq_len=8094 --enable_tts=False --enable_stt=False --enable_transcriptions=False --use_gpu_id=False --inference_server=vllm_chat:http://localhost:11434/v1/ --prompt_type=openai_chat --use_auth_token=<token>\r\n```\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/792691e0-8bb5-4a72-b1f0-72a50b7f5bed)\r\n\r\n"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1668,
    "title": "Option to place relavent documents chunks in system prompt instead of user prompt",
    "author": "ChiNoel-osu",
    "state": "open",
    "created_at": "2024-06-06T03:23:32Z",
    "updated_at": "2024-06-27T22:08:12Z",
    "labels": [
      "type/feature"
    ],
    "body": "I found out that for models that can handle system prompt, placing the document chunks in the system prompt would give better results. It also makes chat history work well.\r\nFor example: (llama3 format)\r\n```\r\n<|start_header_id|>system<|end_header_id|>\r\nYou are a helpful artificial intelligence assistant.\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\nHello!\r\n<|eot_id|>\r\n<|start_header_id|>assistant<|end_header_id|>\r\nHello, how can I help you today?\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\nRelevent context is below:\r\n\"\"\"\r\n1+1=3\r\n\"\"\"\r\nPrepare a concise answer to the question:\r\nWhat is 1+1?\r\n<|eot_id|>\r\n<|start_header_id|>assistant<|end_header_id|>\r\n......\r\n```\r\nwill have the option to change into this:\r\n```\r\n<|start_header_id|>system<|end_header_id|>\r\nYou are a helpful artificial intelligence assistant.\r\nRelevent context is below:\r\n\"\"\"\r\n1+1=3\r\n\"\"\"\r\nYou job is to prepare a concise answer to user's question.\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\nHello!\r\n<|eot_id|>\r\n<|start_header_id|>assistant<|end_header_id|>\r\nHello, how can I help you today?\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\nWhat is 1+1?\r\n<|eot_id|>\r\n<|start_header_id|>assistant<|end_header_id|>\r\n......\r\n```\r\n\r\nI actually don't know how prompt template works in h2oGPT, I'm having a hard time getting the system prompt thing working as I don't know how it interacts with the custom prompt template. Right now I have to resort to `--context` to handle my system prompt:\r\n```\r\n --prompt_type=custom --prompt_dict=\"{'PreInstruct': '<|start_header_id|>user<|end_header_id|>\\n', 'PreInput': None, 'PreResponse': '<|start_header_id|>assistant<|end_header_id|>\\n', 'terminate_response': ['<|eot_id|>'], 'chat_sep': '\\n<|eot_id|>\\n', 'chat_turn_sep': '\\n<|eot_id|>\\n', 'humanstr': '<|start_header_id|>user<|end_header_id|>\\n', 'botstr': '<|start_header_id|>assistant<|end_header_id|>\\n', 'generates_leading_space': False, 'can_handle_system_prompt': True}\" --context=\"<|start_header_id|>system<|end_header_id|>\r\nYou are a helpful artificial intelligence assistant.\r\n<|eot_id|>\r\n\"\r\n```\r\n\r\nAny help would be appreciated.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi sorry for long wait.  It's a good idea as an option, but within langchain framework there is no easy automatic way to do this.  For inference classes I have overridden, I could do it, but for rest it's not some option that I can control."
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1669,
    "title": "AutoGPT issue running on Local LLM ",
    "author": "rohitnanda1443",
    "state": "closed",
    "created_at": "2024-06-06T10:14:48Z",
    "updated_at": "2024-06-27T08:57:18Z",
    "labels": [],
    "body": "Hi, trying to use the AutoGPT agent. The config is as under:\r\n\r\n1. H2o-GPT running locally \r\n2. The LLM model is running locally / via an vLLM inference server.\r\n3. Correct parameters being passsed to h20-gpt: \r\n\r\npython generate.py --guest_name='' --base_model=mistralai/Mistral-7B-Instruct-v0.2   --max_seq_len=8094  --enable_tts=False --enable_stt=False --enable_transcriptions=False --use_gpu_id=False  --inference_server=\"vllm:0.0.0.0:5002\" &\r\n\r\n\r\n\r\n**Issue**: AutoGPT unable to complete tasks as it goes into an enless loop. The reason is that the response it is getting from the local llm is not is the Json format it expected and hence gives an error and restarts the process. \r\n\r\nHow does one resolve this issue (get the respone in a correct Json format). \r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I agree, it used to work.  Langchain must have changed some things to break it.\r\n\r\nFor me it just fails with handling the first step."
      },
      {
        "user": "rohitnanda1443",
        "body": "Yes that is correct and the reason it fails in the first step for me also is due to the response not received in the Json format expected (which is OpenAI format)"
      },
      {
        "user": "pseudotensor",
        "body": "It probably works now if using vllm, but else only with openai.\r\n\r\nd24272f9121eb4cfb5b0c89e3e1cdade49667796"
      }
    ],
    "repository": "h2oai/h2ogpt"
  },
  {
    "issue_number": 1714,
    "title": ".clone() fix both parent and clone for session hash",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2024-06-27T01:08:47Z",
    "updated_at": "2024-06-27T02:53:32Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "with threading.Lock():\r\n\r\nwhen clone"
      },
      {
        "user": "pseudotensor",
        "body": "client = self.clone()"
      }
    ],
    "repository": "h2oai/h2ogpt"
  }
]